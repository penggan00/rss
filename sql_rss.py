import asyncio
import aiohttp
import logging
import re
import os
import hashlib
import pytz
import fcntl
import sqlite3
import time
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from supabase.client import Client, create_client

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# Supabase é…ç½®
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
USE_SUPABASE = SUPABASE_URL and SUPABASE_KEY
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")
MAX_CONCURRENT_REQUESTS = 2      #å¹¶å‘æ§åˆ¶
semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

# é…ç½®ç»å¯¹è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent

# åˆ›å»ºé”æ–‡ä»¶
LOCK_FILE = BASE_DIR / "rss.lock"

# SQLite æ•°æ®åº“åˆå§‹åŒ–
DATABASE_FILE = BASE_DIR / "rss_status.db"

# å¢å¼ºæ—¥å¿—é…ç½®
logging.basicConfig(
    filename=BASE_DIR / "rss.log",
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–å…¨å±€å®¢æˆ·ç«¯
supabase: Client = None

def init_supabase():
    global supabase
    if not USE_SUPABASE:
        return
    
    try:
        # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥åˆå§‹åŒ–æ–¹å¼
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        
        # éªŒè¯è¿æ¥
        test = supabase.table('rss_status').select("count", count="estimated").execute()
        logger.info(f"âœ… Supabaseè¿æ¥æˆåŠŸ | æµ‹è¯•å“åº”: {test}")
    except Exception as e:
        logger.critical(f"â€¼ï¸ Supabaseè¿æ¥å¤±è´¥: {str(e)}")
        exit(1)

# æ¸…ç†è¶…è¿‡7å¤©çš„æ—¥å¿—æ–‡ä»¶
def clean_old_logs():
    log_file = BASE_DIR / "rss.log"
    if log_file.exists():
        log_modified_time = datetime.fromtimestamp(log_file.stat().st_mtime)
        if datetime.now() - log_modified_time > timedelta(days=3):
            try:
                log_file.unlink()
             #   logger.info("å·²æ¸…ç†è¶…è¿‡7å¤©çš„æ—¥å¿—æ–‡ä»¶")
            except Exception as e:
                logger.error(f"æ¸…ç†æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")

# åœ¨ç¨‹åºå¯åŠ¨æ—¶æ‰§è¡Œæ—¥å¿—æ¸…ç†
clean_old_logs()

# å®šä¹‰æ—¶é—´é—´éš” (ç§’)  600ç§’ = 10åˆ†é’Ÿ    1200ç§’ = 20åˆ†é’Ÿ   1800ç§’ = 30åˆ†é’Ÿ  3600ç§’ = 1å°æ—¶   7200ç§’ = 2å°æ—¶   10800ç§’ = 3å°æ—¶
RSS_GROUPS = [
    # ================== å›½é™…æ–°é—»ç»„ (åŸRSS_FEEDS) ==================
    {
        "name": "å›½é™…æ–°é—»",
        "urls": [
            'https://feeds.bbci.co.uk/news/world/rss.xml',  # BBC
            'https://www3.nhk.or.jp/rss/news/cat6.xml',     # NHK
       #     'https://www.cnbc.com/id/100003114/device/rss/rss.html',  # CNBC
       #     'https://feeds.a.dj.com/rss/RSSWorldNews.xml',  # åå°”è¡—æ—¥æŠ¥
            'https://www.aljazeera.com/xml/rss/all.xml',    # åŠå²›ç”µè§†å°
       #     'https://www.ft.com/?format=rss',                 # é‡‘èæ—¶æŠ¥
       #     'https://www3.nhk.or.jp/rss/news/cat5.xml',  # NHK å•†ä¸š
       #     'http://rss.cnn.com/rss/cnn_topstories.rss',   # cnn
       #     'https://www.theguardian.com/world/rss',     # å«æŠ¥
      #      'https://www.theverge.com/rss/index.xml',   # The Verge:
        ],
        "group_key": "RSS_FEEDS",
        "interval": 3300,      # 55åˆ†é’Ÿ 
        "bot_token": os.getenv("RSS_TWO"), 
        "processor": {
            "translate": True,       #ç¿»è¯‘å¼€
            "template": "*{subject}*\n[{source}]({url})",
            "preview": False,         # ç¦æ­¢é¢„è§ˆ
            "show_count": False        # âœ…æ–°å¢
        }
    },

    # ================== å¿«è®¯ç»„ (åŸFOURTH_RSS_FEEDS) ==================
    {
        "name": "å¿«è®¯",
        "urls": [
    #        'https://rsshub.app/10jqka/realtimenews',
            'https://36kr.com/feed-newsflash',  # 36æ°ªå¿«è®¯
        ],
        "group_key": "FOURTH_RSS_FEEDS",
        "interval": 700,       # 11åˆ†é’Ÿ 
        "bot_token": os.getenv("RSS_LINDA"),  
        "processor": {
            "translate": False,     #ç¿»è¯‘å¼€å…³
            "template": "*{subject}*\n[{source}]({url})",
            "preview": False,            # é¢„è§ˆ
            "show_count": False          #è®¡æ•°
        }
    },

    # ================== ç¤¾äº¤åª’ä½“ç»„ (åŸFIFTH_RSS_FEEDS) ==================
    {
        "name": "ç¤¾äº¤åª’ä½“",
        "urls": [
        #    'https://rsshub.app/twitter/media/clawcloud43609', # claw.cloud
            'https://rsshub.app/twitter/media/ElonMuskAOC',   # Elon Musk
        #    'https://rsshub.app/twitter/media/elonmusk',   # Elon Musk
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog',  # Asmongold
        ],
        "group_key": "FIFTH_RSS_FEEDS",
        "interval": 7000,      # 1å°æ—¶56åˆ†é’Ÿ
        "bot_token": os.getenv("YOUTUBE_RSS"), 
        "processor": {
            "translate": True,
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\nğŸ”— {url}",
            "preview": True,        # é¢„è§ˆ
            "show_count": False     #è®¡æ•°
        }
    },

    # ================== æŠ€æœ¯è®ºå›ç»„ (åŸFIFTH_RSS_RSS_SAN) ==================
    {
        "name": "æŠ€æœ¯è®ºå›",
        "urls": [
            'https://rss.nodeseek.com/',  # Nodeseek
        ],
        "group_key": "FIFTH_RSS_RSS_SAN",
        "interval": 240,       # 4åˆ†é’Ÿ 
        "bot_token": os.getenv("RSS_SAN"),
        "processor": {
            "translate": False,                  #ç¿»è¯‘å¼€å…³
            "template": "*{subject}*\n[{source}]({url})",
            "filter": {
                "enable": False,  # è¿‡æ»¤å¼€å…³     False: å…³é—­ / True: å¼€å¯
                "mode": "allow",  # allowæ¨¡å¼ï¼šåŒ…å«å…³é”®è¯æ‰å‘é€ / blockæ¨¡å¼ï¼šåŒ…å«å…³é”®è¯ä¸å‘é€
                "keywords": ["å…", "c", "é»‘", "æ´»", "å‡º", "ç¦", "ä½", "é¦™", "æ°¸", "æ”¶", "å°", "å¡", "å¹´", "ä¼˜", "bug", "å€¼", "ç™½","æŠ˜"]  # æœ¬ç»„å…³é”®è¯åˆ—è¡¨
            },
            "preview": False,               # é¢„è§ˆ
            "show_count": False               #è®¡æ•°
        }
    },

    # ================== YouTubeé¢‘é“ç»„ (åŸYOUTUBE_RSSS_FEEDS) ==================
    {
        "name": "YouTubeé¢‘é“",
        "urls": [
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCvijahEyGtvMpmMHBu4FS2w', # é›¶åº¦è§£è¯´
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC96OvMh0Mb_3NmuE8Dpu7Gg', # ææœºé›¶è·ç¦»
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQoagx4VHBw3HkAyzvKEEBA', # ç§‘æŠ€å…±äº«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCbCCUH8S3yhlm7__rhxR2QQ', # ä¸è‰¯æ—
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCMtXiCoKFrc2ovAGc1eywDg', # ä¸€ä¼‘
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCii04BCvYIdQvshrdNDAcww', # æ‚Ÿç©ºçš„æ—¥å¸¸
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCJMEiNh1HvpopPU3n9vJsMQ', # ç†ç§‘ç”·å£«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCYjB6uufPeHSwuHs8wovLjg', # ä¸­æŒ‡é€š
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSs4A6HYKmHA2MG_0z-F0xw', # ææ°¸ä¹è€å¸ˆ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCZDgXi7VpKhBJxsPuZcBpgA', # å¯æ©KeEn
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCxukdnZiXnTFvjF5B5dvJ5w', # ç”¬å“¥ä¾ƒä¾ƒä¾ƒygkkk
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCUfT9BAofYBKUTiEVrgYGZw', # ç§‘æŠ€åˆ†äº«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC51FT5EeNPiiQzatlA2RlRA', # ä¹Œå®¢wuke
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCDD8WJ7Il3zWBgEYBUtc9xQ', # jack stone
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCWurUlxgm7YJPPggDz9YJjw', # ä¸€ç“¶å¥¶æ²¹
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCvENMyIFurJi_SrnbnbyiZw', # é…·å‹ç¤¾
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCmhbF9emhHa-oZPiBfcLFaQ', # WenWeekly
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC3BNSKOaphlEoK4L7QTlpbA', # ä¸­å¤–è§‚å¯Ÿ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCXk0rwHPG9eGV8SaF2p8KUQ', # çƒé´‰ç¬‘ç¬‘
                    # ... å…¶ä»–YouTubeé¢‘é“ï¼ˆå…±18ä¸ªï¼‰
        ],
        "group_key": "YOUTUBE_RSSS_FEEDS",
        "interval": 3300,      # 55åˆ†é’Ÿ
        "bot_token": os.getenv("RSS_TOKEN"),
        "processor": {
            "translate": False,
            "template": "*{subject}*\n[{source}]({url})",
            "preview": True,                # é¢„è§ˆ
            "show_count": False               #è®¡æ•°
        }
    },

    # ================== ä¸­æ–‡YouTubeç»„ (åŸFIFTH_RSS_YOUTUBE) ==================
    {
        "name": "ä¸­æ–‡YouTube",
        "urls": [
            'https://blog.090227.xyz/atom.xml',
        #    'https://www.freedidi.com/feed',
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCUNciDq-y6I6lEQPeoP-R5A', # è‹æ’è§‚å¯Ÿ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCXkOTZJ743JgVhJWmNV8F3Q', # å¯’åœ‹äºº
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC2r2LPbOUssIa02EbOIm7NA', # æ˜Ÿçƒç†±é»
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCF-Q1Zwyn9681F7du8DMAWg', # è¬å®—æ¡“-è€è¬ä¾†äº†
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCNiJNzSkfumLB7bYtXcIEmg', # çœŸçš„å¾ˆåšé€š
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCN0eCImZY6_OiJbo8cy5bLw', # å±ˆæ©ŸTV
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCb3TZ4SD_Ys3j4z0-8o6auA', # BBC News ä¸­æ–‡
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCiwt1aanVMoPYUt_CQYCPQg', # å…¨çƒå¤§è¦–é‡
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC000Jn3HGeQSwBuX_cLDK8Q', # æˆ‘æ˜¯æŸ³å‚‘å…‹
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFEBaHCJrHu2hzDA_69WQg', # å›½æ¼«è¯´
            'https://www.youtube.com/feeds/videos.xml?channel_id=UChJ8YKw6E1rjFHVS9vovrZw', # BNE TV - æ–°è¥¿å…°ä¸­æ–‡å›½é™…é¢‘é“
        # å½±è§†
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC7Xeh7thVIgs_qfTlwC-dag', # Marc TV
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCCD14H7fJQl3UZNWhYMG3Mg', # æ¸©åŸé²¤
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQO2T82PiHCYbqmCQ6QO6lw', # æœˆäº®èªª
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCHW6W9g2TJL2_Lf7GfoI5kg', # ç”µå½±æ”¾æ˜ å…

        ],
        "group_key": "FIFTH_RSS_YOUTUBE",
        "interval": 10400,     # 2å°æ—¶53åˆ†é’Ÿ
        "bot_token": os.getenv("YOUTUBE_RSS"),
        "processor": {
        "translate": False,                    #ç¿»è¯‘å¼€å…³
        "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
        "template": "*{subject}*\nğŸ”— {url}",  # æ¡ç›®æ¨¡æ¿
        "preview": True,                       # é¢„è§ˆ
        "show_count": False                    #è®¡æ•°
    }
    },

    # ================== ä¸­æ–‡åª’ä½“ç»„ (åŸTHIRD_RSS_FEEDS) ==================
    {
        "name": "ä¸­æ–‡åª’ä½“", 
        "urls": [
            'https://rsshub.app/guancha',
            'https://rsshub.app/china',
            'https://rsshub.app/guancha/headline',
        ],
        "group_key": "THIRD_RSS_FEEDS",
        "interval": 7000,      # 1å°æ—¶56åˆ†é’Ÿ (åŸTHIRD_RSS_FEEDS_INTERVAL)
        "bot_token": os.getenv("RSS_LINDA_YOUTUBE"),
        "processor": {
            "translate": False,                        #ç¿»è¯‘å¼€å…³
            "template": "*{subject}*\n[{source}]({url})",
            "preview": False,                              # é¢„è§ˆ
            "show_count": False                       #è®¡æ•°
        }
    }
]

# æ–°å¢é€šç”¨å¤„ç†å‡½æ•°
async def process_group(session, group_config, global_status):
    """ç»Ÿä¸€å¤„ç†RSSç»„ï¼ˆä¼˜åŒ–ç‰ˆï¼šç¡®ä¿å‘é€æˆåŠŸåæ‰ä¿å­˜çŠ¶æ€ï¼‰"""
    group_name = group_config["name"]
    group_key = group_config["group_key"]
    processor = group_config["processor"]
    bot_token = group_config["bot_token"]
    
    try:
        # ========== 0. åˆå§‹å»¶è¿Ÿ ==========
        await asyncio.sleep(1)  # ç»„é—´åˆå§‹å»¶è¿Ÿ1ç§’

        # ========== 1. æ£€æŸ¥æ—¶é—´é—´éš” ==========
        last_run = await load_last_run_time_from_db(group_key)
        now = time.time()
        if (now - last_run) < group_config["interval"]:
            return  # æœªåˆ°é—´éš”æ—¶é—´ï¼Œè·³è¿‡å¤„ç†

  #      logger.info(f"ğŸš€ å¼€å§‹å¤„ç† [{group_name}] æº...")
        bot = Bot(token=bot_token)

        # ========== 2. å¤„ç†æ¯ä¸ªURLæº ==========
        for index, feed_url in enumerate(group_config["urls"]):
            try:
                # ===== 2.0 æºé—´å»¶è¿Ÿ =====
                if index > 0:  # ç¬¬ä¸€ä¸ªæºä¸éœ€è¦å»¶è¿Ÿ
                    await asyncio.sleep(1)  # æºé—´å»¶è¿Ÿ1ç§’

                # ------ 2.1 è·å–Feedæ•°æ® ------
                feed_data = await fetch_feed(session, feed_url)
                if not feed_data or not feed_data.entries:
                    logger.warning(f"âš ï¸ ç©ºæ•°æ®æº [{feed_url}]")
                    continue

                # ------ 2.2 åŠ è½½å¤„ç†çŠ¶æ€ & æ”¶é›†æ–°æ¡ç›® ------
                processed_ids = global_status.get(feed_url, set())
                new_entries = []
                pending_entry_ids = []  # å¾…ä¿å­˜çš„æ¡ç›®IDï¼ˆå‘é€æˆåŠŸåæ‰ä¿å­˜ï¼‰
                seen_in_batch = set()  # ä¸´æ—¶å­˜å‚¨å½“å‰æ‰¹æ¬¡çš„IDï¼Œé¿å…é‡å¤

                for entry in feed_data.entries:
                    entry_id = get_entry_identifier(entry)
                    if entry_id in processed_ids or entry_id in seen_in_batch:  # æ–°å¢æ‰¹æ¬¡å†…å»é‡
                        continue
                    seen_in_batch.add(entry_id)

                    # å…³é”®è¯è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    filter_config = processor.get("filter", {})
                    if filter_config.get("enable", False):
                        raw_title = remove_html_tags(entry.title or "")
                        keywords = filter_config.get("keywords", [])
                        match = any(kw.lower() in raw_title.lower() for kw in keywords)
                        # æ ¹æ®æ¨¡å¼åˆ¤æ–­æ˜¯å¦è·³è¿‡
                        if filter_config.get("mode", "allow") == "allow":
                            if not match:  # å…è®¸æ¨¡å¼ï¼šä¸åŒ…å«å…³é”®è¯åˆ™è·³è¿‡
                                continue
                        else:  # blockæ¨¡å¼
                            if match:     # åŒ…å«å…³é”®è¯åˆ™è·³è¿‡
                                continue

                    new_entries.append(entry)
                    pending_entry_ids.append(entry_id)  # æš‚å­˜ï¼Œä¸ç«‹å³ä¿å­˜

                # ===== 2.3 å‘é€æ¶ˆæ¯ï¼ˆæˆåŠŸåä¿å­˜çŠ¶æ€ï¼‰ =====
                if new_entries:
                    await asyncio.sleep(1)  # å‘é€å‰å»¶è¿Ÿ1ç§’
                    feed_message = await generate_group_message(feed_data, new_entries, processor)
                    if feed_message:
                        try:
                            # å°è¯•å‘é€æ¶ˆæ¯
                            await send_single_message(
                                bot,
                                TELEGRAM_CHAT_ID[0],
                                feed_message,
                                disable_web_page_preview=not processor.get("preview", True)
                            )
                 #           logger.info(f"ğŸ“¤ å·²å‘é€ {len(new_entries)} æ¡å†…å®¹ [{feed_url}]")

                            # å‘é€æˆåŠŸï¼Œä¿å­˜æ‰€æœ‰æ¡ç›®çŠ¶æ€
                            for entry_id in pending_entry_ids:
                                await save_single_status(group_key, feed_url, entry_id)
                                processed_ids.add(entry_id)

                            # æ›´æ–°å†…å­˜çŠ¶æ€
                            global_status[feed_url] = processed_ids

                        except Exception as send_error:
                            logger.error(f"âŒ å‘é€æ¶ˆæ¯å¤±è´¥ [{feed_url}]: {str(send_error)}")
                            raise  # æŠ›å‡ºå¼‚å¸¸ï¼Œé˜»æ­¢åç»­ä¿å­˜æ“ä½œ

            except Exception as e:
                logger.error(f"âŒ å¤„ç†æºå¤±è´¥ [{feed_url}]: {str(e)}", exc_info=True)

        # ========== 3. ä¿å­˜æœ€åè¿è¡Œæ—¶é—´ ==========
        await save_last_run_time_to_db(group_key, now)

        # ========== 4. æœ€ç»ˆå»¶è¿Ÿ ==========
        await asyncio.sleep(1)  # ç»„å¤„ç†å®Œæˆåå»¶è¿Ÿ3ç§’

    except Exception as e:
        logger.critical(f"â€¼ï¸ å¤„ç†ç»„å¤±è´¥ [{group_name}]: {str(e)}", exc_info=True)
   # finally:
    #    logger.info(f"ğŸ å®Œæˆå¤„ç† [{group_name}]")

async def generate_group_message(feed_data, entries, processor):
    """ç”Ÿæˆæ ‡å‡†åŒ–æ¶ˆæ¯å†…å®¹"""
    try:
        # ===== 1. åŸºç¡€ä¿¡æ¯å¤„ç† =====
        source_name = feed_data.feed.get('title', "æœªçŸ¥æ¥æº")
        safe_source = escape_markdown_v2(source_name)
        
        # ===== æ–°å¢ï¼šæ ‡é¢˜å¤„ç† =====
        header = ""
        if "header_template" in processor:
            header = processor["header_template"].format(source=safe_source) + "\n"
        
        messages = []

        # ===== 2. å¤„ç†æ¯ä¸ªæ¡ç›® =====
        for entry in entries:
            # -- 2.1 æ ‡é¢˜å¤„ç† --
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            if processor["translate"]:
                translated_subject = await auto_translate_text(raw_subject)
            else:
                translated_subject = raw_subject
            safe_subject = escape_markdown_v2(translated_subject)

            # -- 2.2 é“¾æ¥å¤„ç† --
            raw_url = entry.link
            safe_url = escape_markdown_v2(raw_url)

            # -- 2.3 æ„å»ºæ¶ˆæ¯ --
            message = processor["template"].format(
                subject=safe_subject,
                source=safe_source,
                url=safe_url
            )
            messages.append(message)

        full_message = header + "\n\n".join(messages)
        
        if processor.get("show_count", True):
            full_message += f"\n\nâœ… æ–°å¢ {len(messages)} æ¡å†…å®¹"
            
        return full_message
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¶ˆæ¯å¤±è´¥: {str(e)}")
        return ""

def create_connection():
    """åˆ›å»º SQLite æ•°æ®åº“è¿æ¥"""
    conn = None
    try:
        conn = sqlite3.connect(DATABASE_FILE)
        return conn
    except sqlite3.Error as e:
        logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
    return conn

# ä¿®æ”¹åçš„æ•°æ®åº“åˆå§‹åŒ–å‡½æ•°
def create_table():
    """ä»…éªŒè¯è¡¨ç»“æ„å­˜åœ¨"""
    global USE_SUPABASE
    
    # å¼ºåˆ¶é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(override=True)
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    USE_SUPABASE = bool(SUPABASE_URL and SUPABASE_KEY)
    
  #  logger.info(f"ğŸ›¢ï¸ å½“å‰ä½¿ç”¨æ•°æ®åº“ç±»å‹: {'Supabase' if USE_SUPABASE else 'SQLite'}")
    
    if USE_SUPABASE:
        init_supabase()  # è°ƒç”¨åˆå§‹åŒ–å‡½æ•°
        try:
            # ç®€å•æŸ¥è¯¢éªŒè¯è¡¨å­˜åœ¨
            supabase.table("rss_status").select("*").limit(1).execute()
            supabase.table("timestamps").select("*").limit(1).execute()
       #     logger.info("âœ… Supabaseè¡¨ç»“æ„éªŒè¯é€šè¿‡")
        except Exception as e:
            logger.critical(f"â€¼ï¸ è¡¨ç»“æ„éªŒè¯å¤±è´¥: {str(e)}")
            logger.critical("è¯·å…ˆåœ¨Supabaseæ§åˆ¶å°æ‰‹åŠ¨åˆ›å»ºè¡¨ç»“æ„")
            exit(1)
    else:
        try:
            conn = sqlite3.connect(DATABASE_FILE)
            cursor = conn.cursor()
            
            # SQLiteè¡¨ç»“æ„
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS rss_status (
                    feed_group TEXT,
                    feed_url TEXT,
                    entry_url TEXT,
                    entry_timestamp REAL,
                    PRIMARY KEY (feed_group, feed_url, entry_url)
                )
            """)
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_run_time REAL
                )
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_entry_timestamp 
                ON rss_status (entry_timestamp)
            """)
            
            conn.commit()
       #     logger.info("âœ… SQLiteè¡¨ç»“æ„éªŒè¯å®Œæˆ")
        except sqlite3.Error as e:
            logger.critical(f"â€¼ï¸ SQLiteåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            exit(1)
        finally:
            conn.close()

async def load_last_run_time_from_db(feed_group):
    """åŠ è½½æœ€åè¿è¡Œæ—¶é—´"""
    if USE_SUPABASE:
        try:
            data = supabase.table('timestamps')\
                .select('last_run_time')\
                .eq('feed_group', feed_group)\
                .execute()
            return data.data[0]['last_run_time'] if data.data else 0
        except Exception as e:
            logger.error(f"Supabaseæ—¶é—´åŠ è½½å¤±è´¥: {e}")
            return 0
    else:
        conn = sqlite3.connect(DATABASE_FILE)
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT last_run_time FROM timestamps WHERE feed_group = ?", (feed_group,))
            result = cursor.fetchone()
            return result[0] if result else 0
        except sqlite3.Error as e:
            logger.error(f"SQLiteæ—¶é—´åŠ è½½å¤±è´¥: {e}")
            return 0
        finally:
            conn.close()

async def save_last_run_time_to_db(feed_group, last_run_time):
    """ä¿å­˜æœ€åè¿è¡Œæ—¶é—´"""
    last_run_time = int(last_run_time)  # è½¬æ¢ä¸ºæ•´æ•°
    if USE_SUPABASE:
        try:
            supabase.table('timestamps').upsert({
                'feed_group': feed_group,
                'last_run_time': last_run_time
            }).execute()
        except Exception as e:
            logger.error(f"Supabaseæ—¶é—´ä¿å­˜å¤±è´¥: {e}")
    else:
        conn = sqlite3.connect(DATABASE_FILE)
        try:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO timestamps 
                VALUES (?, ?)
            """, (feed_group, last_run_time))
            conn.commit()
        except sqlite3.Error as e:
            logger.error(f"SQLiteæ—¶é—´ä¿å­˜å¤±è´¥: {e}")
        finally:
            conn.close()

# å‡½æ•° (ä¿æŒä¸å˜ï¼Œé™¤éå¦æœ‰è¯´æ˜)
def remove_html_tags(text):
    """å½»åº•ç§»é™¤hashtags, @ç¬¦å·, ä»¥åŠ"ã€ ã€‘" æ ·å¼çš„ç¬¦å·"""
    text = re.sub(r'#\w+', '', text)  # ç§»é™¤hashtags
    text = re.sub(r'@[^\s]+', '', text).strip()  # åˆ é™¤@åé¢çš„å­—ç¬¦
    text = re.sub(r'ã€\s*ã€‘', '', text)  # ç§»é™¤"ã€ ã€‘" æ ·å¼çš„ç¬¦å·ï¼ŒåŒ…å«ä¸­é—´çš„ç©ºæ ¼
    return text

def escape_markdown_v2(text, exclude=None):
    """è‡ªå®šä¹‰MarkdownV2è½¬ä¹‰å‡½æ•°"""
    if exclude is None:
        exclude = []
    chars = '_*[]()~`>#+-=|{}.!'
    chars_to_escape = [c for c in chars if c not in exclude]
    pattern = re.compile(f'([{"".join(map(re.escape, chars_to_escape))}])')
    return pattern.sub(r'\\\1', text)

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=5, max=30),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0

        # æŒ‰æ®µè½åˆ†å‰²ä¿æŒç»“æ„
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para_length = len(para.encode('utf-8'))
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2

        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))

        # v20.x çš„æ­£ç¡®å‚æ•°
        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview,
                read_timeout=10,  # è¯»å–è¶…æ—¶
                write_timeout=10  # å†™å…¥è¶…æ—¶
            )
    except BadRequest as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
    except Exception as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {e}")
        raise

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def fetch_feed(session, feed_url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    try:
        async with semaphore:
            async with session.get(feed_url, headers=headers, timeout=30) as response:
                # ç»Ÿä¸€å¤„ç†ä¸´æ—¶æ€§é”™è¯¯ï¼ˆ503/403ï¼‰
                if response.status in (503, 403,404,429):
                    logger.warning(f"RSSæºæš‚æ—¶ä¸å¯ç”¨ï¼ˆ{response.status}ï¼‰: {feed_url}")
                    return None  # è·³è¿‡å½“å‰æºï¼Œä¸‹æ¬¡è¿è¡Œä¼šé‡è¯•
                response.raise_for_status()
                return parse(await response.read())
    except aiohttp.ClientResponseError as e:
        if e.status in (503, 403,404,429):
            logger.warning(f"RSSæºæš‚æ—¶ä¸å¯ç”¨ï¼ˆ{e.status}ï¼‰: {feed_url}")
            return None
        logging.error(f"HTTP é”™è¯¯ {e.status} æŠ“å–å¤±è´¥ {feed_url}: {e}")
        raise
    except Exception as e:
        logging.error(f"æŠ“å–å¤±è´¥ {feed_url}: {e}")
        raise

async def auto_translate_text(text):
    try:
        cred = credential.Credential(TENCENTCLOUD_SECRET_ID, TENCENTCLOUD_SECRET_KEY)
        clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
        client = tmt_client.TmtClient(cred, "na-siliconvalley", clientProfile)

        req = models.TextTranslateRequest()
        req.SourceText = remove_html_tags(text)  # ç¿»è¯‘å‰å…ˆç§»é™¤HTML
        req.Source = "auto"
        req.Target = "zh"
        req.ProjectId = 0

        return client.TextTranslate(req).TargetText
    except Exception as e:
        logging.error(f"ç¿»è¯‘é”™è¯¯: {e}")
        return text

async def load_status():
    """åŠ è½½å¤„ç†çŠ¶æ€"""
    status = {}
    if USE_SUPABASE:
        try:
            data = supabase.table('rss_status')\
                .select('feed_url, entry_url')\
                .execute()
            for item in data.data:
                if item['feed_url'] not in status:
                    status[item['feed_url']] = set()
                status[item['feed_url']].add(item['entry_url'])
        except Exception as e:
            logger.error(f"SupabaseçŠ¶æ€åŠ è½½å¤±è´¥: {e}")
    else:
        conn = sqlite3.connect(DATABASE_FILE)
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT feed_url, entry_url FROM rss_status")
            for feed_url, entry_url in cursor.fetchall():
                if feed_url not in status:
                    status[feed_url] = set()
                status[feed_url].add(entry_url)
        except sqlite3.Error as e:
            logger.error(f"SQLiteçŠ¶æ€åŠ è½½å¤±è´¥: {e}")
        finally:
            conn.close()
    return status

async def save_single_status(feed_group, feed_url, entry_url):
    """ä¿å­˜å•æ¡çŠ¶æ€"""
    timestamp = int(time.time())  # å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°
    if USE_SUPABASE:
        try:
            supabase.table('rss_status').upsert({
                'feed_group': feed_group,
                'feed_url': feed_url,
                'entry_url': entry_url,
                'entry_timestamp': timestamp  # ç¡®ä¿æ˜¯æ•´æ•°
            }).execute()
        except Exception as e:
            logger.error(f"SupabaseçŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
    else:
        conn = sqlite3.connect(DATABASE_FILE)
        try:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR IGNORE INTO rss_status 
                VALUES (?, ?, ?, ?)
            """, (feed_group, feed_url, entry_url, timestamp))
            conn.commit()
        except sqlite3.Error as e:
            logger.error(f"SQLiteçŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
        finally:
            conn.close()

async def clean_old_entries(feed_group, max_age_days=30):
    """æ¸…ç†æ—§è®°å½•"""
    cutoff_time = int(time.time() - max_age_days * 24 * 3600)  # å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°
    if USE_SUPABASE:
        try:
            supabase.table('rss_status')\
                .delete()\
                .lt('entry_timestamp', cutoff_time)\
                .eq('feed_group', feed_group)\
                .execute()
        except Exception as e:
            logger.error(f"Supabaseæ¸…ç†å¤±è´¥: {e}")
    else:
        conn = sqlite3.connect(DATABASE_FILE)
        try:
            cursor = conn.cursor()
            cursor.execute("DELETE FROM rss_status WHERE feed_group = ? AND entry_timestamp < ?", 
                         (feed_group, cutoff_time))
            conn.commit()
        except sqlite3.Error as e:
            logger.error(f"SQLiteæ¸…ç†å¤±è´¥: {e}")
        finally:
            conn.close()

def get_entry_identifier(entry):
    # ä¼˜å…ˆä½¿ç”¨guid
    if hasattr(entry, 'guid') and entry.guid:
        return hashlib.sha256(entry.guid.encode()).hexdigest()
    
    # æ ‡å‡†åŒ–é“¾æ¥å¤„ç†
    link = getattr(entry, 'link', '')
    if link:
        try:
            parsed = urlparse(link)
            # ç§»é™¤æŸ¥è¯¢å‚æ•°ã€ç‰‡æ®µï¼Œå¹¶ç»Ÿä¸€ä¸ºå°å†™
            clean_link = parsed._replace(query=None, fragment=None).geturl().lower()
            return hashlib.sha256(clean_link.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"URLè§£æå¤±è´¥ {link}: {e}")
    
    # æœ€åä½¿ç”¨æ ‡é¢˜+å‘å¸ƒæ—¶é—´ç»„åˆ
    title = getattr(entry, 'title', '')
    pub_date = get_entry_timestamp(entry).isoformat() if get_entry_timestamp(entry) else ''
    return hashlib.sha256(f"{title}|||{pub_date}".encode()).hexdigest()

def get_entry_timestamp(entry):
    """è¿”å›UTCæ—¶é—´"""
    dt = datetime.now(pytz.UTC)  # é»˜è®¤å€¼
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        dt = datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        dt = datetime(*entry.pubDate_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        dt = datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc)
    return dt

async def process_feed_common(session, feed_group, feed_url, status):
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            return None

        processed_ids = status.get(feed_url, set())
        new_entries = []

        for entry in feed_data.entries:
            entry_id = get_entry_identifier(entry)
            if entry_id in processed_ids:
                continue

            new_entries.append(entry)
            # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
            await save_single_status(feed_group, feed_url, entry_id)
            # æ›´æ–°å†…å­˜ä¸­çš„çŠ¶æ€ï¼Œé˜²æ­¢åŒä¸€æ‰¹æ¬¡å†…é‡å¤
            processed_ids.add(entry_id)

        status[feed_url] = processed_ids  # æ›´æ–°å†…å­˜çŠ¶æ€
        return feed_data, new_entries

    except Exception as e:
        logger.error(f"å¤„ç†æºå¼‚å¸¸ {feed_url}: {e}")
        return None

async def main():
    # åœ¨ç¨‹åºå¼€å§‹æ—¶å¼ºåˆ¶é‡æ–°åŠ è½½.envæ–‡ä»¶
    from dotenv import load_dotenv
    load_dotenv(override=True)  # æ·»åŠ overrideå‚æ•°
    """ä¸»å¤„ç†å‡½æ•°"""
    # ================== 1. æ–‡ä»¶é”å¤„ç† ==================
    lock_file = None
    try:
        lock_file = open(LOCK_FILE, "w")
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
    #    logger.info("ğŸ”’ æˆåŠŸè·å–æ–‡ä»¶é”ï¼Œå¯åŠ¨å¤„ç†æµç¨‹")
    except OSError:
        logger.warning("â›” æ— æ³•è·å–æ–‡ä»¶é”ï¼Œå·²æœ‰å®ä¾‹åœ¨è¿è¡Œï¼Œç¨‹åºé€€å‡º")
        return
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ–‡ä»¶é”å¼‚å¸¸: {str(e)}")
        return

    # ================== 2. æ•°æ®åº“åˆå§‹åŒ– ==================
    global supabase
    if USE_SUPABASE:
        try:
            supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
            # éªŒè¯è¿æ¥æœ‰æ•ˆæ€§
            supabase.table("rss_status").select("count", count="exact").execute()
            logger.info("ğŸ”Œ Supabaseè¿æ¥éªŒè¯æˆåŠŸ")
        except Exception as e:
            logger.critical(f"â€¼ï¸ Supabaseè¿æ¥å¤±è´¥: {str(e)}")
            exit(1)

    # ================== 3. æ—§æ—¥å¿—æ¸…ç† ==================
    try:
        clean_old_logs()
    #    logger.info("ğŸ—‘ï¸ æ—§æ—¥å¿—æ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.error(f"âš ï¸ æ—¥å¿—æ¸…ç†å¤±è´¥: {str(e)}")

    # ================== 4. ä¸»å¤„ç†æµç¨‹ ==================
    async with aiohttp.ClientSession() as session:
        try:
            # ===== 4.1 åŠ è½½å¤„ç†çŠ¶æ€ =====
            status = await load_status()
     #       logger.info("ğŸ“‚ åŠ è½½å†å²çŠ¶æ€å®Œæˆ")

            # ===== 4.2 æ¸…ç†æ—§è®°å½• =====
            retention_config = {
                "RSS_FEEDS": 30,
                "THIRD_RSS_FEEDS": 30,
                "FOURTH_RSS_FEEDS": 7,
                "FIFTH_RSS_FEEDS": 30,
                "FIFTH_RSS_RSS_SAN": 7,
                "YOUTUBE_RSSS_FEEDS": 600,
                "FIFTH_RSS_YOUTUBE": 600
            }
            
            for group in RSS_GROUPS:
                try:
                    await clean_old_entries(
                        group["group_key"], 
                        retention_config.get(group["group_key"], 30)
                    )
                except Exception as e:
                    logger.error(f"âš ï¸ æ¸…ç†æ—§è®°å½•å¤±è´¥ [{group['name']}]: {str(e)}")

            # ===== 4.3 åˆ›å»ºå¤„ç†ä»»åŠ¡ =====
            tasks = []
            for group in RSS_GROUPS:
                try:
                    tasks.append(process_group(session, group, status))
              #      logger.debug(f"ğŸ“¨ å·²åˆ›å»ºå¤„ç†ä»»åŠ¡ [{group['name']}]")
                except Exception as e:
                    logger.error(f"âš ï¸ åˆ›å»ºä»»åŠ¡å¤±è´¥ [{group['name']}]: {str(e)}")

            # ===== 4.4 å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ =====
            if tasks:
                await asyncio.gather(*tasks)
          #      logger.info("ğŸš© æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²å®Œæˆ")
            else:
                logger.warning("â›” æœªåˆ›å»ºä»»ä½•å¤„ç†ä»»åŠ¡")

        except asyncio.CancelledError:
            logger.warning("â¹ï¸ ä»»åŠ¡è¢«å–æ¶ˆ")
        except Exception as e:
            logger.critical(f"â€¼ï¸ ä¸»å¾ªç¯å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            # ===== 4.5 æœ€ç»ˆæ¸…ç† =====
            try:
                await session.close()
     #           logger.info("ğŸ”Œ å·²å…³é—­ç½‘ç»œä¼šè¯")
            except Exception as e:
                logger.error(f"âš ï¸ å…³é—­ä¼šè¯å¤±è´¥: {str(e)}")

    # ================== 5. é‡Šæ”¾æ–‡ä»¶é” ==================
    try:
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
        lock_file.close()
   #     logger.info("ğŸ”“ æ–‡ä»¶é”å·²é‡Šæ”¾")
    except Exception as e:
        logger.error(f"âš ï¸ é‡Šæ”¾æ–‡ä»¶é”å¤±è´¥: {str(e)}")

    # ================== 6. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š ==================
 #   logger.info("ğŸ ç¨‹åºè¿è¡Œç»“æŸ\n" + "="*50 + "\n")

if __name__ == "__main__":
    create_table()
    asyncio.run(main())