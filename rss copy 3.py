import asyncio
import aiohttp
import logging
import re
import os
import json
import hashlib
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models
import pytz
import fcntl
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type, wait_random
import sqlite3
import time

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# é…ç½®ç»å¯¹è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent

# å¢å¼ºæ—¥å¿—é…ç½®
logging.basicConfig(
    filename=BASE_DIR / "rss.log",
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)
logger = logging.getLogger(__name__)

#RSS æºåˆ—è¡¨ (ä¿æŒä¸å˜)
RSS_FEEDS = [
    'https://feeds.bbci.co.uk/news/world/rss.xml', # bbc
    'https://www3.nhk.or.jp/rss/news/cat6.xml',  # nhk
]
#ä¸»é¢˜
THIRD_RSS_FEEDS = [
    'https://36kr.com/feed-newsflash',
    'https://rsshub.215155.xyz/guancha',
    'https://rsshub.215155.xyz/zaobao/znews/china',
    'https://rsshub.215155.xyz/guancha/headline',
    
]
 # ä¸»é¢˜
FOURTH_RSS_FEEDS = [
    'https://rsshub.215155.xyz/10jqka/realtimenews',
]

# ç¿»è¯‘ä¸»é¢˜+é“¾æ¥çš„
FIFTH_RSS_FEEDS = [
    'https://rsshub.app/twitter/media/elonmusk',  #elonmusk
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog', # Asmongold TV

]
# ä¸»é¢˜
FIFTH_RSS_RSS_SAN = [
    'https://rss.nodeseek.com/',  # nodeseek
]
# 10086
YOUTUBE_RSSS_FEEDS = [
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCvijahEyGtvMpmMHBu4FS2w', # é›¶åº¦è§£è¯´
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC96OvMh0Mb_3NmuE8Dpu7Gg', # ææœºé›¶è·ç¦»
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQoagx4VHBw3HkAyzvKEEBA', # ç§‘æŠ€å…±äº«
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCbCCUH8S3yhlm7__rhxR2QQ', # ä¸è‰¯æ—
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCMtXiCoKFrc2ovAGc1eywDg', # ä¸€ä¼‘
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCii04BCvYIdQvshrdNDAcww', # æ‚Ÿç©ºçš„æ—¥å¸¸
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCJMEiNh1HvpopPU3n9vJsMQ', # ç†ç§‘ç”·å£«
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCYjB6uufPeHSwuHs8wovLjg', # ä¸­æŒ‡é€š
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCSs4A6HYKmHA2MG_0z-F0xw', # ææ°¸ä¹è€å¸ˆ
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCZDgXi7VpKhBJxsPuZcBpgA', # å¯æ©KeEn
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCxukdnZiXnTFvjF5B5dvJ5w', # ç”¬å“¥ä¾ƒä¾ƒä¾ƒygkkk
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCUfT9BAofYBKUTiEVrgYGZw', # ç§‘æŠ€åˆ†äº«
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC51FT5EeNPiiQzatlA2RlRA', # ä¹Œå®¢wuke
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCDD8WJ7Il3zWBgEYBUtc9xQ', # jack stone
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCWurUlxgm7YJPPggDz9YJjw', # ä¸€ç“¶å¥¶æ²¹
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCvENMyIFurJi_SrnbnbyiZw', # é…·å‹ç¤¾
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCmhbF9emhHa-oZPiBfcLFaQ', # WenWeekly
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC3BNSKOaphlEoK4L7QTlpbA', # ä¸­å¤–è§‚å¯Ÿ
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCXk0rwHPG9eGV8SaF2p8KUQ', # çƒé´‰ç¬‘ç¬‘
]
# youtube
FIFTH_RSS_YOUTUBE = [
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCUNciDq-y6I6lEQPeoP-R5A', # è‹æ’è§‚å¯Ÿ
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCXkOTZJ743JgVhJWmNV8F3Q', # å¯’åœ‹äºº
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC2r2LPbOUssIa02EbOIm7NA', # æ˜Ÿçƒç†±é»
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCF-Q1Zwyn9681F7du8DMAWg', # è¬å®—æ¡“-è€è¬ä¾†äº†
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCNiJNzSkfumLB7bYtXcIEmg', # çœŸçš„å¾ˆåšé€š
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCN0eCImZY6_OiJbo8cy5bLw', # å±ˆæ©ŸTV
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCb3TZ4SD_Ys3j4z0-8o6auA', # BBC News ä¸­æ–‡
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCiwt1aanVMoPYUt_CQYCPQg', # å…¨çƒå¤§è¦–é‡
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC000Jn3HGeQSwBuX_cLDK8Q', # æˆ‘æ˜¯æŸ³å‚‘å…‹
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFEBaHCJrHu2hzDA_69WQg', # å›½æ¼«è¯´
    'https://www.youtube.com/feeds/videos.xml?channel_id=UChJ8YKw6E1rjFHVS9vovrZw', # BNE TV - æ–°è¥¿å…°ä¸­æ–‡å›½é™…é¢‘é“
# å½±è§†
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC7Xeh7thVIgs_qfTlwC-dag', # Marc TV
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCCD14H7fJQl3UZNWhYMG3Mg', # æ¸©åŸé²¤
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQO2T82PiHCYbqmCQ6QO6lw', # æœˆäº®èªª
    'https://www.youtube.com/feeds/videos.xml?channel_id=UClyVC2wh_2fQhU0hPdXA4rw', # çƒ­é—¨å¤é£
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCHW6W9g2TJL2_Lf7GfoI5kg', # ç”µå½±æ”¾æ˜ å…
]

# Telegramé…ç½® (ä¿æŒä¸å˜)
TELEGRAM_BOT_TOKEN = os.getenv("RSS_TWO")  # 10086 bbc
RSS_TWO = os.getenv("RSS_TWO")
RSS_TOKEN = os.getenv("RSS_LINDA")    # RSS_LINDA
RSSTWO_TOKEN = os.getenv("RSS_TWO")
RSS_SANG = os.getenv("RSS_SAN")
YOUTUBE_RSS_FEEDSS = os.getenv("RSS_LINDA_YOUTUBE")
YOUTUBE_RSSS = os.getenv("YOUTUBE_RSS")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")

MAX_CONCURRENT_REQUESTS = 5
semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

# å®šä¹‰æ—¶é—´é—´éš” (ç§’)
DEFAULT_INTERVAL = 3500  # é»˜è®¤1å°æ—¶
RSSSS_FEEDS_INTERVAL = DEFAULT_INTERVAL     # BBC
THIRD_RSS_FEEDS_INTERVAL = DEFAULT_INTERVAL     #36KR
FOURTH_RSS_FEEDS_INTERVAL = 1700  # 10jqka
FIFTH_RSS_FEEDS_INTERVAL = DEFAULT_INTERVAL    # Asmongold TV
FIFTH_RSS_RSS_SAN_INTERVAL = 1700  # nodeseek
YOUTUBE_RSSS_FEEDS_INTERVAL = DEFAULT_INTERVAL  # 10086 YOUTUBE
FIFTH_RSS_YOUTUBE_INTERVAL = 7300  # FIFTH_RSS_YOUTUBEï¼Œ2 å°æ—¶1800


# åˆ›å»ºé”æ–‡ä»¶
LOCK_FILE = BASE_DIR / "rss.lock"
# SQLite æ•°æ®åº“åˆå§‹åŒ–
DATABASE_FILE = BASE_DIR / "rss_status.db"

def create_connection():
    """åˆ›å»º SQLite æ•°æ®åº“è¿æ¥"""
    conn = None
    try:
        conn = sqlite3.connect(DATABASE_FILE)
        return conn
    except sqlite3.Error as e:
        logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
    return conn

def create_table():
    """åˆ›å»º rss_status å’Œ timestamp è¡¨"""
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS rss_status (
                    feed_url TEXT PRIMARY KEY,
                    identifier TEXT,
                    timestamp TEXT
                )
            """)
            # æ·»åŠ  timestamp è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_run_time REAL
                )
            """)
            conn.commit()
            logger.info("æˆåŠŸåˆ›å»º/è¿æ¥åˆ°æ•°æ®åº“å’Œè¡¨")
        except sqlite3.Error as e:
            logger.error(f"åˆ›å»ºè¡¨å¤±è´¥: {e}")
        finally:
            conn.close()
    else:
        logger.error("æ— æ³•åˆ›å»ºæ•°æ®åº“è¿æ¥")

create_table()

def load_last_run_time_from_db(feed_group):
    """ä»æ•°æ®åº“åŠ è½½ä¸Šæ¬¡è¿è¡Œæ—¶é—´"""
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT last_run_time FROM timestamps WHERE feed_group = ?", (feed_group,))
            result = cursor.fetchone()
            if result:
                return result[0]
            else:
                return 0  # é»˜è®¤å€¼ä¸º 0
        except sqlite3.Error as e:
            logger.error(f"ä»æ•°æ®åº“åŠ è½½ä¸Šæ¬¡è¿è¡Œæ—¶é—´å¤±è´¥: {e}")
            return 0
        finally:
            conn.close()
    else:
        logger.error("æ— æ³•åˆ›å»ºæ•°æ®åº“è¿æ¥")
        return 0

def save_last_run_time_to_db(feed_group, last_run_time):
    """å°†ä¸Šæ¬¡è¿è¡Œæ—¶é—´ä¿å­˜åˆ°æ•°æ®åº“"""
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO timestamps (feed_group, last_run_time)
                VALUES (?, ?)
            """, (feed_group, last_run_time))
            conn.commit()
            logger.info(f"ä¿å­˜ä¸Šæ¬¡è¿è¡Œæ—¶é—´åˆ°æ•°æ®åº“æˆåŠŸ (feed_group: {feed_group})")
        except sqlite3.Error as e:
            logger.error(f"ä¿å­˜ä¸Šæ¬¡è¿è¡Œæ—¶é—´åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        finally:
            conn.close()
    else:
        logger.error("æ— æ³•åˆ›å»ºæ•°æ®åº“è¿æ¥")

def create_connection():
    """åˆ›å»º SQLite æ•°æ®åº“è¿æ¥"""
    conn = None
    try:
        conn = sqlite3.connect(DATABASE_FILE)
        return conn
    except sqlite3.Error as e:
        logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
    return conn

def create_table():
    """åˆ›å»º rss_status å’Œ timestamp è¡¨"""
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS rss_status (
                    feed_url TEXT PRIMARY KEY,
                    identifier TEXT,
                    timestamp TEXT
                )
            """)
            # æ·»åŠ  timestamp è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_run_time REAL
                )
            """)
            conn.commit()
            logger.info("æˆåŠŸåˆ›å»º/è¿æ¥åˆ°æ•°æ®åº“å’Œè¡¨")
        except sqlite3.Error as e:
            logger.error(f"åˆ›å»ºè¡¨å¤±è´¥: {e}")
        finally:
            conn.close()
    else:
        logger.error("æ— æ³•åˆ›å»ºæ•°æ®åº“è¿æ¥")

create_table()


# å‡½æ•° (ä¿æŒä¸å˜ï¼Œé™¤éå¦æœ‰è¯´æ˜)
def remove_html_tags(text):
    """å½»åº•ç§»é™¤hashtags, @ç¬¦å·, ä»¥åŠ"ã€ ã€‘" æ ·å¼çš„ç¬¦å·"""
    text = re.sub(r'#\w+', '', text)  # ç§»é™¤hashtags
    text = re.sub(r'@[^\s]+', '', text).strip()  # åˆ é™¤@åé¢çš„å­—ç¬¦
    text = re.sub(r'ã€\s*ã€‘', '', text)  # ç§»é™¤"ã€ ã€‘" æ ·å¼çš„ç¬¦å·ï¼ŒåŒ…å«ä¸­é—´çš„ç©ºæ ¼
    return text

def escape_markdown_v2(text, exclude=None):
    """è‡ªå®šä¹‰MarkdownV2è½¬ä¹‰å‡½æ•°"""
    if exclude is None:
        exclude = []
    chars = '_*[]()~`>#+-=|{}.!'
    chars_to_escape = [c for c in chars if c not in exclude]
    pattern = re.compile(f'([{"".join(map(re.escape, chars_to_escape))}])')
    return pattern.sub(r'\\\1', text)

async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0

        # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²ä¿æŒæ®µè½ç»“æ„
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para_length = len(para.encode('utf-8'))
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:  # +2 æ˜¯æ¢è¡Œç¬¦
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2

        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))

        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview
            )
    except BadRequest as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
    except Exception as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {e}")

# è‡ªå®šä¹‰é‡è¯•æ¡ä»¶
def retry_if_transient_error(exception):
    """
    å¦‚æœå¼‚å¸¸æ˜¯ç¬æ€é”™è¯¯ï¼ˆå¦‚è¿æ¥é”™è¯¯ã€è¶…æ—¶ï¼‰ï¼Œåˆ™é‡è¯•ã€‚
    ä¸é‡è¯• 4xx é”™è¯¯ã€‚
    """
    if isinstance(exception, aiohttp.ClientError):
        return True
    if isinstance(exception, aiohttp.ClientResponseError) and 400 <= exception.status < 500:
        return False  # ä¸é‡è¯• 4xx é”™è¯¯
    return False

@retry(
    stop=stop_after_attempt(5),  # å¢åŠ é‡è¯•æ¬¡æ•°
    wait=wait_exponential(multiplier=1, min=2, max=15) + wait_random(0, 2),  # å¢åŠ éšæœºæŠ–åŠ¨
    retry=retry_if_exception_type(aiohttp.ClientError),  # ä»…é‡è¯• ClientError
    before_sleep=lambda retry_state: logging.warning(f"é‡è¯•ä¸­... (å°è¯•æ¬¡æ•°: {retry_state.attempt_number})")
)
async def fetch_feed(session, feed_url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    try:
        async with semaphore:
            async with session.get(feed_url, headers=headers, timeout=40) as response:
                response.raise_for_status()
                return parse(await response.read())
    except aiohttp.ClientResponseError as e:
        logging.error(f"HTTP é”™è¯¯ {e.status} æŠ“å–å¤±è´¥ {feed_url}: {e}")
        raise  # é‡æ–°æŠ›å‡ºï¼Œè®© tenacity åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
    except Exception as e:
        logging.error(f"æŠ“å–å¤±è´¥ {feed_url}: {e}")
        raise  # é‡æ–°æŠ›å‡ºï¼Œè®© tenacity åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•

async def auto_translate_text(text):
    try:
        cred = credential.Credential(TENCENTCLOUD_SECRET_ID, TENCENTCLOUD_SECRET_KEY)
        clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
        client = tmt_client.TmtClient(cred, "na-siliconvalley", clientProfile)

        req = models.TextTranslateRequest()
        req.SourceText = remove_html_tags(text)  # ç¿»è¯‘å‰å…ˆç§»é™¤HTML
        req.Source = "auto"
        req.Target = "zh"
        req.ProjectId = 0

        return client.TextTranslate(req).TargetText
    except Exception as e:
        logging.error(f"ç¿»è¯‘é”™è¯¯: {e}")
        return text

async def load_status():
    """ä» SQLite åŠ è½½çŠ¶æ€"""
    status = {}
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT feed_url, identifier, timestamp FROM rss_status")
            rows = cursor.fetchall()
            for row in rows:
                status[row[0]] = {'identifier': row[1], 'timestamp': row[2]}
            logger.info("ä»æ•°æ®åº“åŠ è½½çŠ¶æ€æˆåŠŸ")
        except sqlite3.Error as e:
            logger.error(f"ä»æ•°æ®åº“åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
        finally:
            conn.close()
    return status


async def save_single_status(feed_url, status_data):
    """ä¿å­˜å•ä¸ªfeedçŠ¶æ€åˆ° SQLite"""
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO rss_status (feed_url, identifier, timestamp)
                VALUES (?, ?, ?)
            """, (feed_url, status_data['identifier'], status_data['timestamp']))
            conn.commit()
            logger.info(f"ä¿å­˜çŠ¶æ€ {feed_url} åˆ°æ•°æ®åº“æˆåŠŸ")
        except sqlite3.Error as e:
            logger.error(f"ä¿å­˜çŠ¶æ€ {feed_url} åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        finally:
            conn.close()

def get_entry_identifier(entry):
    """ä½¿ç”¨SHA256å“ˆå¸Œç”Ÿæˆç¨³å®šæ ‡è¯†ç¬¦"""
    identifier_str = ""
    for field in ['guid', 'link', 'id', 'title']:
        if hasattr(entry, field):
            identifier_str += str(getattr(entry, field))
    if not identifier_str:
        entry_time = get_entry_timestamp(entry).isoformat()
        identifier_str = f"{entry_time}-{hash(frozenset(entry.items()))}"
    return hashlib.sha256(identifier_str.encode()).hexdigest()

def get_entry_timestamp(entry):
    """è¿”å›UTCæ—¶é—´"""
    dt = datetime.now(pytz.UTC)  # é»˜è®¤å€¼
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        dt = datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        dt = datetime(*entry.pubDate_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        dt = datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc)
    return dt

async def process_feed(session, feed_url, status, bot, translate=True):
    logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")  # åœ¨å¤„ç†å¼€å§‹æ—¶è®°å½•çŠ¶æ€
    logger.info(f"å½“å‰çŠ¶æ€: {json.dumps(status.get(feed_url, {}), default=str)}")
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        # çŠ¶æ€è·Ÿè¸ªå¢å¼º
        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp = last_status.get('timestamp')
        last_timestamp_dt = datetime.fromisoformat(last_timestamp).astimezone(pytz.utc) if last_timestamp else None

        logger.debug(f"ä¸Šæ¬¡è®°å½•æ ‡è¯†ç¬¦: {last_identifier}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ—¶é—´: {last_timestamp}")

        new_entries = []
        current_latest = None

        # ä¿®æ”¹æ¡ç›®å¤„ç†é¡ºåºä¸ºæ­£å‘æ—¶é—´é¡ºåº
        for entry in feed_data.entries:
            try:
                entry_time = get_entry_timestamp(entry)
                identifier = get_entry_identifier(entry)
                logger.debug(f"æ£€æŸ¥æ¡ç›®: {identifier[:50]}... æ—¶é—´: {entry_time}")

                if last_identifier and identifier == last_identifier:
                    logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                    break

                if last_timestamp_dt and entry_time <= last_timestamp_dt:
                    logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp_dt}ï¼Œåœæ­¢å¤„ç†")
                    break

                new_entries.append(entry)
                if not current_latest or entry_time > get_entry_timestamp(current_latest):
                    current_latest = entry
            except Exception as e:
                logger.error(f"å¤„ç†æ¡ç›®å¤±è´¥: {str(e)}")
                continue

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        # å¤„ç†æ¶ˆæ¯
        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(new_entries, start=1):
            # åŸå§‹å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
       #     raw_summary = remove_html_tags(getattr(entry, 'summary', "æš‚æ— ç®€ä»‹"))
            raw_url = entry.link

            # ç¿»è¯‘å¤„ç†
            if translate:
                translated_subject = await auto_translate_text(raw_subject)
          #     translated_summary = await auto_translate_text(raw_summary)
            else:
                translated_subject = raw_subject
          #     translated_summary = raw_summary

            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(translated_subject)
      #      safe_summary = escape_markdown_v2(translated_summary)
            safe_source = escape_markdown_v2(source_name)
            safe_url = escape_markdown_v2(raw_url)

            # æ„å»ºæ¶ˆæ¯
      #      message = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
            message = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
            merged_message += message + "\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        # æ›´æ–°çŠ¶æ€:
        if current_latest:
            current_latest_identifier = get_entry_identifier(current_latest)
            current_latest_timestamp = get_entry_timestamp(current_latest).isoformat()

            status[feed_url] = {
                "identifier": current_latest_identifier,
                "timestamp": current_latest_timestamp
            }

            await save_single_status(feed_url, status[feed_url])  #  <----  æ·»åŠ è¿™è¡Œä»£ç 
        return merged_message

    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""

async def process_third_feed(session, feed_url, status, bot):
    logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")  # åœ¨å¤„ç†å¼€å§‹æ—¶è®°å½•çŠ¶æ€
    logger.info(f"å½“å‰çŠ¶æ€: {json.dumps(status.get(feed_url, {}), default=str)}")
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp = last_status.get('timestamp')
        last_timestamp_dt = datetime.fromisoformat(last_timestamp).astimezone(pytz.utc) if last_timestamp else None

        new_entries = []
        current_latest = None

        for entry in feed_data.entries:
            entry_time = get_entry_timestamp(entry)
            identifier = get_entry_identifier(entry)

            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                break

            if last_timestamp_dt and entry_time <= last_timestamp_dt:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp_dt}ï¼Œåœæ­¢å¤„ç†")
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(new_entries, start=1):
            # å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
         #   raw_summary = remove_html_tags(getattr(entry, 'summary', "æš‚æ— ç®€ä»‹"))
            raw_url = entry.link

            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(raw_subject)
      #      safe_summary = escape_markdown_v2(raw_summary)
            safe_source = escape_markdown_v2(source_name)
            safe_url = escape_markdown_v2(raw_url)

            # æ¶ˆæ¯æ„å»º
      #      message_content = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
            message_content = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
            message_bytes = message_content.encode('utf-8')

            if len(message_bytes) <= 444:
                merged_message += message_content + "\n\n"
            else:
                title_link = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
                merged_message += title_link + "\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        # æ›´æ–°çŠ¶æ€:
        if current_latest:
            current_latest_identifier = get_entry_identifier(current_latest)
            current_latest_timestamp = get_entry_timestamp(current_latest).isoformat()

            status[feed_url] = {
                "identifier": current_latest_identifier,
                "timestamp": current_latest_timestamp
            }
            await save_single_status(feed_url, status[feed_url])  #  <----  æ·»åŠ è¿™è¡Œä»£ç 
        return merged_message
    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""

async def process_fourth_feed(session, feed_url, status, bot):
    logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")  # åœ¨å¤„ç†å¼€å§‹æ—¶è®°å½•çŠ¶æ€
    logger.info(f"å½“å‰çŠ¶æ€: {json.dumps(status.get(feed_url, {}), default=str)}")
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp = last_status.get('timestamp')
        last_timestamp_dt = datetime.fromisoformat(last_timestamp).astimezone(pytz.utc) if last_timestamp else None

        new_entries = []
        current_latest = None

        for entry in feed_data.entries:
            entry_time = get_entry_timestamp(entry)
            identifier = get_entry_identifier(entry)

            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                break

            if last_timestamp_dt and entry_time <= last_timestamp_dt:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp_dt}ï¼Œåœæ­¢å¤„ç†")
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(new_entries, start=1):
            # å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
         #   raw_summary = remove_html_tags(getattr(entry, 'summary', "æš‚æ— ç®€ä»‹"))
            raw_url = entry.link

            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(raw_subject)
      #      safe_summary = escape_markdown_v2(raw_summary)
            safe_source = escape_markdown_v2(source_name)
            safe_url = escape_markdown_v2(raw_url)

            # æ¶ˆæ¯æ„å»º
      #      message_content = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
            message_content = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
            message_bytes = message_content.encode('utf-8')

            if len(message_bytes) <= 444:
                merged_message += message_content + "\n\n"
            else:
                title_link = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
                merged_message += title_link + "\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        # æ›´æ–°çŠ¶æ€:
        if current_latest:
            current_latest_identifier = get_entry_identifier(current_latest)
            current_latest_timestamp = get_entry_timestamp(current_latest).isoformat()

            status[feed_url] = {
                "identifier": current_latest_identifier,
                "timestamp": current_latest_timestamp
            }
            await save_single_status(feed_url, status[feed_url])  #  <----  æ·»åŠ è¿™è¡Œä»£ç 
        return merged_message
    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""

async def process_fifth_feed(session, feed_url, status, bot, translate=True):
    logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")  # åœ¨å¤„ç†å¼€å§‹æ—¶è®°å½•çŠ¶æ€
    logger.info(f"å½“å‰çŠ¶æ€: {json.dumps(status.get(feed_url, {}), default=str)}")
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        # çŠ¶æ€å¤„ç†
        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp = last_status.get('timestamp')
        last_timestamp_dt = datetime.fromisoformat(last_timestamp).astimezone(pytz.utc) if last_timestamp else None

        new_entries = []
        current_latest = None

        for entry in feed_data.entries:
            entry_time = get_entry_timestamp(entry)
            identifier = get_entry_identifier(entry)

            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                break

            if last_timestamp_dt and entry_time <= last_timestamp_dt:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp_dt}ï¼Œåœæ­¢å¤„ç†")
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        # å¤„ç†æ¶ˆæ¯
        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        feed_title = escape_markdown_v2(source_name)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        merged_message += f"ğŸ“¢ *{feed_title}*\n\n"
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(new_entries, start=1):
            # åŸå§‹å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            raw_url = entry.link

            # ç¿»è¯‘å¤„ç†
            if translate:
                translated_subject = await auto_translate_text(raw_subject)
            else:
                translated_subject = raw_subject
            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(translated_subject)
        #    safe_source = escape_markdown_v2(source_name)
            safe_url = escape_markdown_v2(raw_url)

            # æ„å»ºæ¶ˆæ¯, åªå‘é€ä¸»é¢˜å’Œé“¾æ¥
            message = f"*{safe_subject}*\nğŸ”— {safe_url}"
            merged_message += message + "\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        # æ›´æ–°çŠ¶æ€:
        if current_latest:
            current_latest_identifier = get_entry_identifier(current_latest)
            current_latest_timestamp = get_entry_timestamp(current_latest).isoformat()

            status[feed_url] = {
                "identifier": current_latest_identifier,
                "timestamp": current_latest_timestamp
            }
            await save_single_status(feed_url, status[feed_url])  #  <----  æ·»åŠ è¿™è¡Œä»£ç 
        return merged_message
    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""
    
async def process_san_feed(session, feed_url, status, bot):
    logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")  # åœ¨å¤„ç†å¼€å§‹æ—¶è®°å½•çŠ¶æ€
    logger.info(f"å½“å‰çŠ¶æ€: {json.dumps(status.get(feed_url, {}), default=str)}")
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp = last_status.get('timestamp')
        last_timestamp_dt = datetime.fromisoformat(last_timestamp).astimezone(pytz.utc) if last_timestamp else None

        new_entries = []
        current_latest = None

        for entry in feed_data.entries:
            entry_time = get_entry_timestamp(entry)
            identifier = get_entry_identifier(entry)

            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                break

            if last_timestamp_dt and entry_time <= last_timestamp_dt:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp_dt}ï¼Œåœæ­¢å¤„ç†")
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(new_entries, start=1):
            # å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
         #   raw_summary = remove_html_tags(getattr(entry, 'summary', "æš‚æ— ç®€ä»‹"))
            raw_url = entry.link

            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(raw_subject)
      #      safe_summary = escape_markdown_v2(raw_summary)
            safe_source = escape_markdown_v2(source_name)
            safe_url = escape_markdown_v2(raw_url)

            # æ¶ˆæ¯æ„å»º
      #      message_content = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
            message_content = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
            message_bytes = message_content.encode('utf-8')

            if len(message_bytes) <= 444:
                merged_message += message_content + "\n\n"
            else:
                title_link = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
                merged_message += title_link + "\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        # æ›´æ–°çŠ¶æ€:
        if current_latest:
            current_latest_identifier = get_entry_identifier(current_latest)
            current_latest_timestamp = get_entry_timestamp(current_latest).isoformat()

            status[feed_url] = {
                "identifier": current_latest_identifier,
                "timestamp": current_latest_timestamp
            }
            await save_single_status(feed_url, status[feed_url])  #  <----  æ·»åŠ è¿™è¡Œä»£ç 
        return merged_message
    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""
    
async def process_you_feed(session, feed_url, status, bot):
    logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")  # åœ¨å¤„ç†å¼€å§‹æ—¶è®°å½•çŠ¶æ€
    logger.info(f"å½“å‰çŠ¶æ€: {json.dumps(status.get(feed_url, {}), default=str)}")
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp = last_status.get('timestamp')
        last_timestamp_dt = datetime.fromisoformat(last_timestamp).astimezone(pytz.utc) if last_timestamp else None

        new_entries = []
        current_latest = None

        for entry in feed_data.entries:
            entry_time = get_entry_timestamp(entry)
            identifier = get_entry_identifier(entry)

            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                break

            if last_timestamp_dt and entry_time <= last_timestamp_dt:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp_dt}ï¼Œåœæ­¢å¤„ç†")
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(new_entries, start=1):
            # å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
         #   raw_summary = remove_html_tags(getattr(entry, 'summary', "æš‚æ— ç®€ä»‹"))
            raw_url = entry.link

            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(raw_subject)
      #      safe_summary = escape_markdown_v2(raw_summary)
            safe_source = escape_markdown_v2(source_name)
            safe_url = escape_markdown_v2(raw_url)

            # æ¶ˆæ¯æ„å»º
      #      message_content = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
            message_content = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
            message_bytes = message_content.encode('utf-8')

            if len(message_bytes) <= 444:
                merged_message += message_content + "\n\n"
            else:
                title_link = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
                merged_message += title_link + "\n\n"
    #    merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        # æ›´æ–°çŠ¶æ€:
        if current_latest:
            current_latest_identifier = get_entry_identifier(current_latest)
            current_latest_timestamp = get_entry_timestamp(current_latest).isoformat()

            status[feed_url] = {
                "identifier": current_latest_identifier,
                "timestamp": current_latest_timestamp
            }
            await save_single_status(feed_url, status[feed_url])  #  <----  æ·»åŠ è¿™è¡Œä»£ç 
        return merged_message
    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""
    
async def process_youtube_feed(session, feed_url, status, bot):
    logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")  # åœ¨å¤„ç†å¼€å§‹æ—¶è®°å½•çŠ¶æ€
    logger.info(f"å½“å‰çŠ¶æ€: {json.dumps(status.get(feed_url, {}), default=str)}")
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp = last_status.get('timestamp')
        last_timestamp_dt = datetime.fromisoformat(last_timestamp).astimezone(pytz.utc) if last_timestamp else None

        new_entries = []
        current_latest = None

        for entry in feed_data.entries:
            entry_time = get_entry_timestamp(entry)
            identifier = get_entry_identifier(entry)

            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                break

            if last_timestamp_dt and entry_time <= last_timestamp_dt:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp_dt}ï¼Œåœæ­¢å¤„ç†")
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        feed_title = escape_markdown_v2(source_name)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        merged_message += f"ğŸ“¢ *{feed_title}*\n\n"

        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(new_entries, start=1):
            # å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            raw_url = entry.link

            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(raw_subject)
            safe_url = escape_markdown_v2(raw_url)

            # æ„å»ºæ¶ˆæ¯ï¼Œæ·»åŠ åºå·
            merged_message += f"*{safe_subject}*\nğŸ”— {safe_url}\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        # æ›´æ–°çŠ¶æ€:
        if current_latest:
            current_latest_identifier = get_entry_identifier(current_latest)
            current_latest_timestamp = get_entry_timestamp(current_latest).isoformat()

            status[feed_url] = {
                "identifier": current_latest_identifier,
                "timestamp": current_latest_timestamp
            }
            await save_single_status(feed_url, status[feed_url])  #  <----  æ·»åŠ è¿™è¡Œä»£ç 
        return merged_message
    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""

async def main():
    # å°è¯•è·å–é”
    try:
        lock_file = open(LOCK_FILE, "w")
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)  # Non-blocking lock
        logger.info("æˆåŠŸè·å–æ–‡ä»¶é”ï¼Œç¨‹åºå¼€å§‹è¿è¡Œ...")
    except OSError:
        logger.warning("æ— æ³•è·å–æ–‡ä»¶é”ï¼Œå¦ä¸€ä¸ªå®ä¾‹å¯èƒ½æ­£åœ¨è¿è¡Œã€‚ç¨‹åºé€€å‡ºã€‚")
        return  # ç›´æ¥é€€å‡º

    async with aiohttp.ClientSession() as session:
        bot = Bot(token=TELEGRAM_BOT_TOKEN)
        third_bot = Bot(token=RSS_TWO)
        fourth_bot = Bot(token=RSS_TOKEN)
        fifth_bot = Bot(token=RSSTWO_TOKEN)
        rsssan_bot = Bot(token=RSS_SANG)
        youtube_bot = Bot(token=YOUTUBE_RSSS)
        you_bot = Bot(token=YOUTUBE_RSS_FEEDSS)
        status = await load_status()  # æ”¹ä¸ºå¼‚æ­¥åŠ è½½

        try:
            # å¤„ç† RSS_FEEDS
            last_rss_feeds_run = load_last_run_time_from_db("RSS_FEEDS")
            now = time.time()
            if now - last_rss_feeds_run >= RSSSS_FEEDS_INTERVAL:
                logger.info("å¼€å§‹å¤„ç† RSS_FEEDS æº...")
                for idx, url in enumerate(RSS_FEEDS):
                    if message := await process_feed(session, url, status, bot):
                        await send_single_message(bot, TELEGRAM_CHAT_ID[0], message, True)
                        logger.info(f"æˆåŠŸå¤„ç† RSS_FEEDS æº {idx + 1}/{len(RSS_FEEDS)}")
                save_last_run_time_to_db("RSS_FEEDS", time.time())
                logger.info("RSS_FEEDS æºå¤„ç†å®Œæˆã€‚")
            else:
                logger.info(
                    f"è·ç¦»ä¸Šæ¬¡è¿è¡Œ RSS_FEEDS ä¸è¶³ {RSSSS_FEEDS_INTERVAL / 3600} å°æ—¶ï¼Œè·³è¿‡ RSS_FEEDS å¤„ç†ã€‚")

            # å¤„ç† THIRD_RSS_FEEDS
            last_third_rss_feeds_run = load_last_run_time_from_db("THIRD_RSS_FEEDS")
            now = time.time()
            if now - last_third_rss_feeds_run >= THIRD_RSS_FEEDS_INTERVAL:
                logger.info("å¼€å§‹å¤„ç† THIRD_RSS_FEEDS æº...")
                for idx, url in enumerate(THIRD_RSS_FEEDS):
                    if message := await process_third_feed(session, url, status, third_bot):
                        await send_single_message(third_bot, TELEGRAM_CHAT_ID[0], message, True)
                        logger.info(
                            f"æˆåŠŸå¤„ç† THIRD_RSS_FEEDS æº {idx + 1}/{len(THIRD_RSS_FEEDS)}")
                save_last_run_time_to_db("THIRD_RSS_FEEDS", time.time())
                logger.info("THIRD_RSS_FEEDS æºå¤„ç†å®Œæˆã€‚")
            else:
                logger.info(f"è·ç¦»ä¸Šæ¬¡è¿è¡Œ THIRD_RSS_FEEDS ä¸è¶³ {THIRD_RSS_FEEDS_INTERVAL / 3600} å°æ—¶ï¼Œè·³è¿‡ THIRD_RSS_FEEDS å¤„ç†ã€‚")


            # å¤„ç† FOURTH_RSS_FEEDS
            last_fourth_rss_feeds_run = load_last_run_time_from_db("FOURTH_RSS_FEEDS")
            now = time.time()
            if now - last_fourth_rss_feeds_run >= FOURTH_RSS_FEEDS_INTERVAL:
                logger.info("å¼€å§‹å¤„ç† FOURTH_RSS_FEEDS æº...")
                for idx, url in enumerate(FOURTH_RSS_FEEDS):
                    if message := await process_fourth_feed(session, url, status, fourth_bot):
                        await send_single_message(fourth_bot, TELEGRAM_CHAT_ID[0], message, True)
                        logger.info(
                            f"æˆåŠŸå¤„ç† FOURTH_RSS_FEEDS æº {idx + 1}/{len(FOURTH_RSS_FEEDS)}")
                save_last_run_time_to_db("FOURTH_RSS_FEEDS", time.time())
                logger.info("FOURTH_RSS_FEEDS æºå¤„ç†å®Œæˆã€‚")
            else:
                logger.info(f"è·ç¦»ä¸Šæ¬¡è¿è¡Œ FOURTH_RSS_FEEDS ä¸è¶³ {FOURTH_RSS_FEEDS_INTERVAL / 3600} å°æ—¶ï¼Œè·³è¿‡ FOURTH_RSS_FEEDS å¤„ç†ã€‚")

            # å¤„ç† FIFTH_RSS_FEEDS
            last_fifth_rss_feeds_run = load_last_run_time_from_db("FIFTH_RSS_FEEDS")
            now = time.time()
            if now - last_fifth_rss_feeds_run >= FIFTH_RSS_FEEDS_INTERVAL:
                logger.info("å¼€å§‹å¤„ç† FIFTH_RSS_FEEDS æº...")
                for idx, url in enumerate(FIFTH_RSS_FEEDS):
                    if message := await process_fifth_feed(session, url, status, fifth_bot):
                        await send_single_message(fifth_bot, TELEGRAM_CHAT_ID[0], message, False)  # æ ¹æ®éœ€è¦è°ƒæ•´Trueä¸æµè§ˆ
                        logger.info(
                            f"æˆåŠŸå¤„ç† FIFTH_RSS_FEEDS æº {idx + 1}/{len(FIFTH_RSS_FEEDS)}")
                save_last_run_time_to_db("FIFTH_RSS_FEEDS", time.time())
                logger.info("FIFTH_RSS_FEEDS æºå¤„ç†å®Œæˆã€‚")
            else:
                logger.info(f"è·ç¦»ä¸Šæ¬¡è¿è¡Œ FIFTH_RSS_FEEDS ä¸è¶³ {FIFTH_RSS_FEEDS_INTERVAL / 3600} å°æ—¶ï¼Œè·³è¿‡ FIFTH_RSS_FEEDS å¤„ç†ã€‚")

            # å¤„ç† FIFTH_RSS_RSS_SAN
            last_fifth_rss_rss_san_run = load_last_run_time_from_db("FIFTH_RSS_RSS_SAN")
            now = time.time()
            if now - last_fifth_rss_rss_san_run >= FIFTH_RSS_RSS_SAN_INTERVAL:
                logger.info("å¼€å§‹å¤„ç† FIFTH_RSS_RSS_SAN æº...")
                for idx, url in enumerate(FIFTH_RSS_RSS_SAN):
                    if message := await process_san_feed(session, url, status, rsssan_bot):
                        await send_single_message(rsssan_bot, TELEGRAM_CHAT_ID[0], message, True)  # æ ¹æ®éœ€è¦è°ƒæ•´Trueä¸æµè§ˆ
                        logger.info(
                            f"æˆåŠŸå¤„ç† FIFTH_RSS_RSS_SAN æº {idx + 1}/{len(FIFTH_RSS_RSS_SAN)}")
                save_last_run_time_to_db("FIFTH_RSS_RSS_SAN", time.time())
                logger.info("FIFTH_RSS_RSS_SAN æºå¤„ç†å®Œæˆã€‚")
            else:
                logger.info(f"è·ç¦»ä¸Šæ¬¡è¿è¡Œ FIFTH_RSS_RSS_SAN ä¸è¶³ {FIFTH_RSS_RSS_SAN_INTERVAL / 3600} å°æ—¶ï¼Œè·³è¿‡ FIFTH_RSS_RSS_SAN å¤„ç†ã€‚")

            # å¤„ç† YOUTUBE_RSSS_FEEDS
            last_youtube_rsss_feeds_run = load_last_run_time_from_db("YOUTUBE_RSSS_FEEDS")
            now = time.time()
            if now - last_youtube_rsss_feeds_run >= YOUTUBE_RSSS_FEEDS_INTERVAL:
                logger.info("å¼€å§‹å¤„ç† YOUTUBE_RSSS_FEEDS æº...")
                for idx, url in enumerate(YOUTUBE_RSSS_FEEDS):
                    if message := await process_you_feed(session, url, status, you_bot):
                        await send_single_message(you_bot, TELEGRAM_CHAT_ID[0], message, False)  # æ ¹æ®éœ€è¦è°ƒæ•´Trueä¸æµè§ˆ
                        logger.info(
                            f"æˆåŠŸå¤„ç† YOUTUBE_RSSS_FEEDS æº {idx + 1}/{len(YOUTUBE_RSSS_FEEDS)}")
                save_last_run_time_to_db("YOUTUBE_RSSS_FEEDS", time.time())
                logger.info("YOUTUBE_RSSS_FEEDS æºå¤„ç†å®Œæˆã€‚")
            else:
                logger.info(f"è·ç¦»ä¸Šæ¬¡è¿è¡Œ YOUTUBE_RSSS_FEEDS ä¸è¶³ {YOUTUBE_RSSS_FEEDS_INTERVAL / 3600} å°æ—¶ï¼Œè·³è¿‡ YOUTUBE_RSSS_FEEDS å¤„ç†ã€‚")

            # å¤„ç† FIFTH_RSS_YOUTUBE
            last_fifth_rss_youtube_run = load_last_run_time_from_db("FIFTH_RSS_YOUTUBE")
            now = time.time()
            if now - last_fifth_rss_youtube_run >= FIFTH_RSS_YOUTUBE_INTERVAL:
                logger.info("å¼€å§‹å¤„ç† FIFTH_RSS_YOUTUBE æº...")
                for idx, url in enumerate(FIFTH_RSS_YOUTUBE):
                    message = await process_youtube_feed(session, url, status, youtube_bot)
                    if message:  # åªæœ‰å½“ process_youtube_feed è¿”å›æ¶ˆæ¯æ—¶æ‰å‘é€
                        await send_single_message(youtube_bot, TELEGRAM_CHAT_ID[0], message, False)  # æ ¹æ®éœ€è¦è°ƒæ•´Trueä¸æµè§ˆ
                        logger.info(
                            f"æˆåŠŸå¤„ç† FIFTH_RSS_YOUTUBE æº {idx + 1}/{len(FIFTH_RSS_YOUTUBE)}")
                    else:
                        logger.info(f"FIFTH_RSS_YOUTUBE æº {idx + 1}/{len(FIFTH_RSS_YOUTUBE)} æ²¡æœ‰æ–°å†…å®¹æˆ–å¤„ç†å¤±è´¥")

                save_last_run_time_to_db("FIFTH_RSS_YOUTUBE", time.time())
                logger.info("FIFTH_RSS_YOUTUBE æºå¤„ç†å®Œæˆã€‚")
            else:
                logger.info(f"è·ç¦»ä¸Šæ¬¡è¿è¡Œ FIFTH_RSS_YOUTUBE ä¸è¶³ {FIFTH_RSS_YOUTUBE_INTERVAL / 3600} å°æ—¶ï¼Œè·³è¿‡ FIFTH_RSS_YOUTUBE å¤„ç†ã€‚")

        except Exception as e:
            logger.critical(f"ä¸»å¾ªç¯å‘ç”Ÿè‡´å‘½é”™è¯¯: {str(e)}")
        finally:
            # é‡Šæ”¾é”
            try:
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
                lock_file.close()
                logger.info("é‡Šæ”¾æ–‡ä»¶é”ï¼Œç¨‹åºè¿è¡Œå®Œæˆï¼ŒçŠ¶æ€å·²ä¿å­˜")
            except Exception as e:
                logger.error(f"é‡Šæ”¾æ–‡ä»¶é”å¤±è´¥: {e}")



if __name__ == "__main__":
    asyncio.run(main())
