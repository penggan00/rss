import asyncio
import aiohttp
import logging
import re
import os
import json
from pathlib import Path
from datetime import datetime, timezone
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models
from supabase import create_client, Client

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦åŠ è½½
print(f"Supabase URL: {os.getenv('DB_URL')}")
print(f"Supabase API Key: {os.getenv('DB_API_KEY')}")

# é…ç½®ç»å¯¹è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent

# å¢å¼ºæ—¥å¿—é…ç½®
logging.basicConfig(
    filename=BASE_DIR / "rss.log",
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)
logger = logging.getLogger(__name__)

# RSSé…ç½®

RSS_FEEDS = [
  #  'https://feeds.bbci.co.uk/news/world/rss.xml', # bbc
  #  'https://www3.nhk.or.jp/rss/news/cat6.xml',  # nhk
  #  'https://www.cnbc.com/id/100003114/device/rss/rss.html', # CNBC
  #  'https://feeds.a.dj.com/rss/RSSWorldNews.xml', # åå°”è¡—æ—¥æŠ¥
  #  'https://www.aljazeera.com/xml/rss/all.xml',# åŠå²›ç”µè§†å°
  #  'https://www3.nhk.or.jp/rss/news/cat5.xml',# NHK å•†ä¸š
  #  'https://www.ft.com/?format=rss', # é‡‘èæ—¶æŠ¥
  #  'http://rss.cnn.com/rss/edition.rss', # cnn
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCupvZG-5ko_eiXAupbDfxWw', # cnn
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog', # Asmongold TV
]
#ä¸»é¢˜+å†…å®¹
THIRD_RSS_FEEDS = [
   # 'https://36kr.com/feed-newsflash',
  #  'https://rss.owo.nz/10jqka/realtimenews',
]
 # ä¸»é¢˜+é¢„è§ˆ
FOURTH_RSS_FEEDS = [
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCvijahEyGtvMpmMHBu4FS2w', # é›¶åº¦è§£è¯´
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UC96OvMh0Mb_3NmuE8Dpu7Gg', # ææœºé›¶è·ç¦»
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQoagx4VHBw3HkAyzvKEEBA', # ç§‘æŠ€å…±äº«
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCbCCUH8S3yhlm7__rhxR2QQ', # ä¸è‰¯æ—
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCMtXiCoKFrc2ovAGc1eywDg', # ä¸€ä¼‘
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCii04BCvYIdQvshrdNDAcww', # æ‚Ÿç©ºçš„æ—¥å¸¸
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCJMEiNh1HvpopPU3n9vJsMQ', # ç†ç§‘ç”·å£«
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCYjB6uufPeHSwuHs8wovLjg', # ä¸­æŒ‡é€š
 #   'https://www.youtube.com/feeds/videos.xml?channel_id=UCSs4A6HYKmHA2MG_0z-F0xw', # ææ°¸ä¹è€å¸ˆ
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCZDgXi7VpKhBJxsPuZcBpgA', # å¯æ©KeEn
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCxukdnZiXnTFvjF5B5dvJ5w', # ç”¬å“¥ä¾ƒä¾ƒä¾ƒygkkk
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCUfT9BAofYBKUTiEVrgYGZw', # ç§‘æŠ€åˆ†äº«
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UC51FT5EeNPiiQzatlA2RlRA', # ä¹Œå®¢wuke
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCDD8WJ7Il3zWBgEYBUtc9xQ', # jack stone
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCWurUlxgm7YJPPggDz9YJjw', # ä¸€ç“¶å¥¶æ²¹
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCvENMyIFurJi_SrnbnbyiZw', # é…·å‹ç¤¾
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCmhbF9emhHa-oZPiBfcLFaQ', # WenWeekly
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UC3BNSKOaphlEoK4L7QTlpbA', # ä¸­å¤–è§‚å¯Ÿ
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCXk0rwHPG9eGV8SaF2p8KUQ', # çƒé´‰ç¬‘ç¬‘
]

# ç¿»è¯‘ä¸»é¢˜+é“¾æ¥çš„
FIFTH_RSS_FEEDS = [
   # 'https://rsshub.app/twitter/media/elonmusk',  #elonmusk
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCupvZG-5ko_eiXAupbDfxWw', # cnn
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog', # Asmongold TV

]

# Telegramé…ç½®
TELEGRAM_BOT_TOKEN = os.getenv("RSS_TWO")
RSS_TWO = os.getenv("RSS_TWO")
RSS_TOKEN = os.getenv("RSS_TOKEN")
RSSTWO_TOKEN = os.getenv("RSS_TWO")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")

MAX_CONCURRENT_REQUESTS = 5
semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

# Supabase é…ç½®
SUPABASE_URL = os.getenv("DB_URL")
SUPABASE_KEY = os.getenv("DB_API_KEY")

# Initialize Supabase client
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# è¡¨å
TABLE_NAME = "rss_status"


def remove_html_tags(text):
    """å½»åº•ç§»é™¤HTMLæ ‡ç­¾"""
    return re.sub(r'<[^>]*>', '', text)

def escape_markdown_v2(text, exclude=None):
    """è‡ªå®šä¹‰MarkdownV2è½¬ä¹‰å‡½æ•°"""
    if exclude is None:
        exclude = []
    chars = '_*[]()~`>#+-=|{}.!'
    chars_to_escape = [c for c in chars if c not in exclude]
    pattern = re.compile(f'([{"".join(map(re.escape, chars_to_escape))}])')  # Fixed the syntax error here
    return pattern.sub(r'\\\1', text)

async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0

        # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²ä¿æŒæ®µè½ç»“æ„
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para_length = len(para.encode('utf-8'))
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:  # +2 æ˜¯æ¢è¡Œç¬¦
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2

        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))

        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview
            )
    except BadRequest as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
    except Exception as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {e}")

async def fetch_feed(session, feed_url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    try:
        async with semaphore:
            async with session.get(feed_url, headers=headers, timeout=40) as response:
                response.raise_for_status()
                return parse(await response.read())
    except Exception as e:
        logging.error(f"æŠ“å–å¤±è´¥ {feed_url}: {e}")
        return None

async def auto_translate_text(text):
    try:
        cred = credential.Credential(TENCENTCLOUD_SECRET_ID, TENCENTCLOUD_SECRET_KEY)
        clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
        client = tmt_client.TmtClient(cred, "na-siliconvalley", clientProfile)

        req = models.TextTranslateRequest()
        req.SourceText = remove_html_tags(text)  # ç¿»è¯‘å‰å…ˆç§»é™¤HTML
        req.Source = "auto"
        req.Target = "zh"
        req.ProjectId = 0

        return client.TextTranslate(req).TargetText
    except Exception as e:
        logging.error(f"ç¿»è¯‘é”™è¯¯: {e}")
        return text

async def load_status_from_supabase():
    """ä» Supabase åŠ è½½çŠ¶æ€"""
    try:
        response = await supabase.table(TABLE_NAME).select("*").execute()
        if response.error:
            logger.error(f"ä» Supabase åŠ è½½çŠ¶æ€å¤±è´¥: {response.error}")
            return {}

        # ä» response ä¸­æå–æ•°æ®
        data = response.data
        if not isinstance(data, list):
            logger.error(f"ä» Supabase åŠ è½½çš„çŠ¶æ€ä¸æ˜¯åˆ—è¡¨: {data}")
            return {}

        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ï¼Œä½¿ç”¨ feed_url ä½œä¸ºé”®
        status = {item["feed_url"]: item for item in data}
        logger.debug(f"æˆåŠŸä»supabaseåŠ è½½çŠ¶æ€: {status}")
        return status
    except Exception as e:
        logger.error(f"åŠ è½½ Supabase çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {}

async def save_status_to_supabase(feed_url, identifier, timestamp):
    """å°†çŠ¶æ€ä¿å­˜åˆ° Supabaseï¼Œæ›¿æ¢ç°æœ‰è®°å½•"""
    try:
        # æ„å»ºè¦æ’å…¥æˆ–æ›´æ–°çš„æ•°æ®
        data = {
            "feed_url": feed_url,
            "identifier": identifier,
            "timestamp": timestamp,
        }

        # ä½¿ç”¨ upsert æ›¿æ¢ç°æœ‰è®°å½•
        response = await supabase.table(TABLE_NAME).upsert(data, on_conflict="feed_url").execute()
        if response.error:
            logger.error(f"ä¿å­˜çŠ¶æ€åˆ° Supabase å¤±è´¥: {response.error}")
        else:
            # æ£€æŸ¥ response.data æ˜¯å¦å­˜åœ¨å¹¶æ‰“å°
            if response.data:
                logger.debug(f"Supabase upsert å“åº”æ•°æ®: {response.data}")
            logger.info(f"æˆåŠŸä¿å­˜çŠ¶æ€åˆ° Supabase: {feed_url} - Identifier: {identifier[:50]}... Timestamp: {timestamp}")
    except Exception as e:
        logger.error(f"ä¿å­˜ Supabase çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def get_entry_identifier(entry):
    """å¢å¼ºç‰ˆå”¯ä¸€æ ‡è¯†ç¬¦ç”Ÿæˆ"""
    # ä¼˜å…ˆä½¿ç”¨ç¨³å®šå­—æ®µ
    if hasattr(entry, 'guid') and entry.guid:
        logger.debug(f"ä½¿ç”¨GUIDä½œä¸ºæ ‡è¯†ç¬¦: {entry.guid}")
        return entry.guid
    if hasattr(entry, 'link') and entry.link:
        logger.debug(f"ä½¿ç”¨é“¾æ¥ä½œä¸ºæ ‡è¯†ç¬¦: {entry.link}")
        return entry.link
    if hasattr(entry, 'id') and entry.id:
        logger.debug(f"ä½¿ç”¨IDä½œä¸ºæ ‡è¯†ç¬¦: {entry.id}")
        return entry.id

    # æ—¶é—´ç›¸å…³å­—æ®µ
    time_fields = [
        ('published_parsed', 'published'),
        ('pubDate_parsed', 'pubDate'),
        ('updated_parsed', 'updated')  # ä¼˜å…ˆç”¨ updated æ—¶é—´
    ]
    for parsed_field, raw_field in time_fields:
        if hasattr(entry, parsed_field) and getattr(entry, parsed_field):
            dt = datetime(*getattr(entry, parsed_field)[:6], tzinfo=timezone.utc)
            logger.debug(f"ä½¿ç”¨æ—¶é—´å­—æ®µ {parsed_field} ä½œä¸ºæ ‡è¯†ç¬¦: {dt.isoformat()}")
            return dt.isoformat()
        if hasattr(entry, raw_field) and getattr(entry, raw_field):
            logger.debug(f"ä½¿ç”¨åŸå§‹æ—¶é—´å­—æ®µ {raw_field} ä½œä¸ºæ ‡è¯†ç¬¦: {getattr(entry, raw_field)}")
            return getattr(entry, raw_field)

    # æœ€ç»ˆå›é€€æ–¹æ¡ˆ
    fallback = f"{entry.get('title', '')}-{entry.get('link', '')}"
    logger.warning(f"ä½¿ç”¨å›é€€æ ‡è¯†ç¬¦: {fallback}")
    return fallback

def get_entry_timestamp(entry):
    """è·å–æ ‡å‡†åŒ–æ—¶é—´æˆ³"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
    if hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        return datetime(*entry.pubDate_parsed[:6], tzinfo=timezone.utc)
    return datetime.now(timezone.utc)

async def process_feed(session, feed_url, status, bot, translate=True):
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        # çŠ¶æ€è·Ÿè¸ªå¢å¼º
        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp_str = last_status.get('timestamp')
        last_timestamp = datetime.fromisoformat(last_timestamp_str) if last_timestamp_str else None

        logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ ‡è¯†ç¬¦: {last_identifier}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ—¶é—´: {last_timestamp}")

        # æŒ‰æ—¶é—´æ’åºï¼ˆå¢å¼ºå®¹é”™ï¼‰
        sorted_entries = []
        for entry in feed_data.entries:
            try:
                entry_time = get_entry_timestamp(entry)
                sorted_entries.append((entry_time, entry))
            except Exception as e:
                logger.error(f"å¤„ç†æ¡ç›®æ—¶é—´å¤±è´¥: {str(e)}")
                continue
        sorted_entries.sort(key=lambda x: x[0], reverse=True)

        new_entries = []
        current_latest = None
        found_break = False

        # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
        for entry_time, entry in sorted_entries:
            identifier = get_entry_identifier(entry)
            logger.debug(f"æ£€æŸ¥æ¡ç›®: {identifier[:50]}... æ—¶é—´: {entry_time}")

            # æ·»åŠ æ—¥å¿—æ¥ç¡®è®¤æ ‡è¯†ç¬¦å’Œæ—¶é—´æˆ³çš„å€¼
            logger.debug(f"å½“å‰æ¡ç›®æ ‡è¯†ç¬¦: {identifier}")
            logger.debug(f"å½“å‰æ¡ç›®æ—¶é—´: {entry_time}")

            # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                found_break = True
                break

            # æ—¶é—´æ¯”å¯¹ï¼ˆä»…å½“æ ‡è¯†ç¬¦ä¸å¯ç”¨æ—¶ï¼‰
            if last_timestamp and entry_time <= last_timestamp:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp}ï¼Œåœæ­¢å¤„ç†")
                found_break = True
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not found_break and last_identifier:
            logger.warning("æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œå¯èƒ½å‘ç”Ÿæ•°æ®å›æ»šæˆ–æ ‡è¯†ç¬¦å˜åŒ–")

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        # æ›´æ–°çŠ¶æ€
        if current_latest:
            latest_identifier = get_entry_identifier(current_latest)
            latest_timestamp = get_entry_timestamp(current_latest).isoformat()
            await save_status_to_supabase(feed_url, latest_identifier, latest_timestamp)

            # æ·»åŠ æ—¥å¿—æ¥ç¡®è®¤çŠ¶æ€æ˜¯å¦æ›´æ–°
            logger.info(f"æ›´æ–°çŠ¶æ€: {feed_url} - Identifier: {latest_identifier[:50]}... Timestamp: {latest_timestamp}")

        # å¤„ç†æ¶ˆæ¯
        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(reversed(new_entries), start=1):
            # åŸå§‹å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            raw_summary = remove_html_tags(getattr(entry, 'summary', ""))  # è·å– summaryï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
            raw_url = entry.link

            # ç¿»è¯‘å¤„ç†
            if translate:
                translated_subject = await auto_translate_text(raw_subject)
                translated_summary = await auto_translate_text(raw_summary)
            else:
                translated_subject = raw_subject
                translated_summary = raw_summary

            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(translated_subject, exclude=['*'])
            safe_summary = escape_markdown_v2(translated_summary)
            safe_source = escape_markdown_v2(source_name, exclude=['[', ']'])
            safe_url = escape_markdown_v2(raw_url)

            # æ„å»ºæ¶ˆæ¯
            message = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
            merged_message += message + "\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"
        return merged_message

    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""

async def process_third_feed(session, feed_url, status, bot):
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        # çŠ¶æ€è·Ÿè¸ªå¢å¼º
        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp_str = last_status.get('timestamp')
        last_timestamp = datetime.fromisoformat(last_timestamp_str) if last_timestamp_str else None

        logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ ‡è¯†ç¬¦: {last_identifier}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ—¶é—´: {last_timestamp}")

        # æŒ‰æ—¶é—´æ’åºï¼ˆå¢å¼ºå®¹é”™ï¼‰
        sorted_entries = []
        for entry in feed_data.entries:
            try:
                entry_time = get_entry_timestamp(entry)
                sorted_entries.append((entry_time, entry))
            except Exception as e:
                logger.error(f"å¤„ç†æ¡ç›®æ—¶é—´å¤±è´¥: {str(e)}")
                continue
        sorted_entries.sort(key=lambda x: x[0], reverse=True)

        new_entries = []
        current_latest = None
        found_break = False

        # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
        for entry_time, entry in sorted_entries:
            identifier = get_entry_identifier(entry)
            logger.debug(f"æ£€æŸ¥æ¡ç›®: {identifier[:50]}... æ—¶é—´: {entry_time}")

            # æ·»åŠ æ—¥å¿—æ¥ç¡®è®¤æ ‡è¯†ç¬¦å’Œæ—¶é—´æˆ³çš„å€¼
            logger.debug(f"å½“å‰æ¡ç›®æ ‡è¯†ç¬¦: {identifier}")
            logger.debug(f"å½“å‰æ¡ç›®æ—¶é—´: {entry_time}")

            # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                found_break = True
                break

            # æ—¶é—´æ¯”å¯¹ï¼ˆä»…å½“æ ‡è¯†ç¬¦ä¸å¯ç”¨æ—¶ï¼‰
            if last_timestamp and entry_time <= last_timestamp:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp}ï¼Œåœæ­¢å¤„ç†")
                found_break = True
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not found_break and last_identifier:
            logger.warning("æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œå¯èƒ½å‘ç”Ÿæ•°æ®å›æ»šæˆ–æ ‡è¯†ç¬¦å˜åŒ–")

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        # æ›´æ–°çŠ¶æ€
        if current_latest:
            latest_identifier = get_entry_identifier(current_latest)
            latest_timestamp = get_entry_timestamp(current_latest).isoformat()
            await save_status_to_supabase(feed_url, latest_identifier, latest_timestamp)

            # æ·»åŠ æ—¥å¿—æ¥ç¡®è®¤çŠ¶æ€æ˜¯å¦æ›´æ–°
            logger.info(f"æ›´æ–°çŠ¶æ€: {feed_url} - Identifier: {latest_identifier[:50]}... Timestamp: {latest_timestamp}")

        # å¤„ç†æ¶ˆæ¯
        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for idx, entry in enumerate(reversed(new_entries), start=1):
            # åŸå§‹å†…å®¹å¤„ç†
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            raw_summary = remove_html_tags(getattr(entry, 'summary', ""))  # è·å– summaryï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
            raw_url = entry.link

            # Markdownè½¬ä¹‰
            safe_subject = escape_markdown_v2(translated_subject, exclude=['*'])
            safe_summary = escape_markdown_v2(translated_summary)
            safe_source = escape_markdown_v2(source_name, exclude=['[', ']'])
            safe_url = escape_markdown_v2(raw_url)

            # æ„å»ºæ¶ˆæ¯
            message = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
            merged_message += message + "\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"
        return merged_message

    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""

async def process_fourth_feed(session, feed_url, status, bot):
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        # çŠ¶æ€è·Ÿè¸ªå¢å¼º
        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp_str = last_status.get('timestamp')
        last_timestamp = datetime.fromisoformat(last_timestamp_str) if last_timestamp_str else None

        logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ ‡è¯†ç¬¦: {last_identifier}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ—¶é—´: {last_timestamp}")

        # æŒ‰æ—¶é—´æ’åºï¼ˆå¢å¼ºå®¹é”™ï¼‰
        sorted_entries = []
        for entry in feed_data.entries:
            try:
                entry_time = get_entry_timestamp(entry)
                sorted_entries.append((entry_time, entry))
            except Exception as e:
                logger.error(f"å¤„ç†æ¡ç›®æ—¶é—´å¤±è´¥: {str(e)}")
                continue
        sorted_entries.sort(key=lambda x: x[0], reverse=True)

        new_entries = []
        current_latest = None
        found_break = False

        # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
        for entry_time, entry in sorted_entries:
            identifier = get_entry_identifier(entry)
            logger.debug(f"æ£€æŸ¥æ¡ç›®: {identifier[:50]}... æ—¶é—´: {entry_time}")

            # æ·»åŠ æ—¥å¿—æ¥ç¡®è®¤æ ‡è¯†ç¬¦å’Œæ—¶é—´æˆ³çš„å€¼
            logger.debug(f"å½“å‰æ¡ç›®æ ‡è¯†ç¬¦: {identifier}")
            logger.debug(f"å½“å‰æ¡ç›®æ—¶é—´: {entry_time}")

            # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                found_break = True
                break

            # æ—¶é—´æ¯”å¯¹ï¼ˆä»…å½“æ ‡è¯†ç¬¦ä¸å¯ç”¨æ—¶ï¼‰
            if last_timestamp and entry_time <= last_timestamp:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp}ï¼Œåœæ­¢å¤„ç†")
                found_break = True
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not found_break and last_identifier:
            logger.warning("æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œå¯èƒ½å‘ç”Ÿæ•°æ®å›æ»šæˆ–æ ‡è¯†ç¬¦å˜åŒ–")

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        # æ›´æ–°çŠ¶æ€
        if current_latest:
            latest_identifier = get_entry_identifier(current_latest)
            latest_timestamp = get_entry_timestamp(current_latest).isoformat()
            await save_status_to_supabase(feed_url, latest_identifier, latest_timestamp)

            # æ·»åŠ æ—¥å¿—æ¥ç¡®è®¤çŠ¶æ€æ˜¯å¦æ›´æ–°
            logger.info(f"æ›´æ–°çŠ¶æ€: {feed_url} - Identifier: {latest_identifier[:50]}... Timestamp: {latest_timestamp}")

        # å¤„ç†æ¶ˆæ¯
        merged_message = ""
        #source_name = feed_data.feed.get('title', feed_url)
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        #merged_message += f"ğŸ“¢ *{source_name}*\n\n"
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for entry in reversed(new_entries):
            # åŸå§‹å†…å®¹å¤„ç†
            raw_title = remove_html_tags(entry.get('title', ''))  # ä½¿ç”¨ get æ–¹æ³•
            raw_url = entry.get('link', '')  # ä½¿ç”¨ get æ–¹æ³•

            # Markdownè½¬ä¹‰
            safe_title = escape_markdown_v2(raw_title, exclude=['*'])
            safe_url = escape_markdown_v2(raw_url)

            # æ„å»ºæ¶ˆæ¯
            merged_message += f"*{safe_title}*\n{safe_url}\n\n"
        #merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        return merged_message

    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""

async def process_fifth_feed(session, feed_url, status, bot):
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            logger.info(f"æº {feed_url} æ²¡æœ‰æ–°æ¡ç›®")
            return ""

        # çŠ¶æ€è·Ÿè¸ªå¢å¼º
        last_status = status.get(feed_url, {})
        last_identifier = last_status.get('identifier')
        last_timestamp_str = last_status.get('timestamp')
        last_timestamp = datetime.fromisoformat(last_timestamp_str) if last_timestamp_str else None

        logger.info(f"å¼€å§‹å¤„ç†æº: {feed_url}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ ‡è¯†ç¬¦: {last_identifier}")
        logger.debug(f"ä¸Šæ¬¡è®°å½•æ—¶é—´: {last_timestamp}")

        # æŒ‰æ—¶é—´æ’åºï¼ˆå¢å¼ºå®¹é”™ï¼‰
        sorted_entries = []
        for entry in feed_data.entries:
            try:
                entry_time = get_entry_timestamp(entry)
                sorted_entries.append((entry_time, entry))
            except Exception as e:
                logger.error(f"å¤„ç†æ¡ç›®æ—¶é—´å¤±è´¥: {str(e)}")
                continue
        sorted_entries.sort(key=lambda x: x[0], reverse=True)

        new_entries = []
        current_latest = None
        found_break = False

        # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
        for entry_time, entry in sorted_entries:
            identifier = get_entry_identifier(entry)
            logger.debug(f"æ£€æŸ¥æ¡ç›®: {identifier[:50]}... æ—¶é—´: {entry_time}")

            # æ·»åŠ æ—¥å¿—æ¥ç¡®è®¤æ ‡è¯†ç¬¦å’Œæ—¶é—´æˆ³çš„å€¼
            logger.debug(f"å½“å‰æ¡ç›®æ ‡è¯†ç¬¦: {identifier}")
            logger.debug(f"å½“å‰æ¡ç›®æ—¶é—´: {entry_time}")

            # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
            if last_identifier and identifier == last_identifier:
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œåœæ­¢å¤„ç†")
                found_break = True
                break

            # æ—¶é—´æ¯”å¯¹ï¼ˆä»…å½“æ ‡è¯†ç¬¦ä¸å¯ç”¨æ—¶ï¼‰
            if last_timestamp and entry_time <= last_timestamp:
                logger.info(f"æ—¶é—´ {entry_time} <= ä¸Šæ¬¡æ—¶é—´ {last_timestamp}ï¼Œåœæ­¢å¤„ç†")
                found_break = True
                break

            new_entries.append(entry)
            if not current_latest or entry_time > get_entry_timestamp(current_latest):
                current_latest = entry

        if not found_break and last_identifier:
            logger.warning("æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…æ ‡è¯†ç¬¦ï¼Œå¯èƒ½å‘ç”Ÿæ•°æ®å›æ»šæˆ–æ ‡è¯†ç¬¦å˜åŒ–")

        if not new_entries:
            logger.info(f"æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†: {feed_url}")
            return ""

        # æ›´æ–°çŠ¶æ€
        if current_latest:
            latest_identifier = get_entry_identifier(current_latest)
            latest_timestamp = get_entry_timestamp(current_latest).isoformat()
            await save_status_to_supabase(feed_url, latest_identifier, latest_timestamp)

            # æ·»åŠ æ—¥å¿—æ¥ç¡®è®¤çŠ¶æ€æ˜¯å¦æ›´æ–°
            logger.info(f"æ›´æ–°çŠ¶æ€: {feed_url} - Identifier: {latest_identifier[:50]}... Timestamp: {latest_timestamp}")

        # å¤„ç†æ¶ˆæ¯
        merged_message = ""
        source_name = feed_data.feed.get('title', feed_url)
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        merged_message += f"ğŸ“¢ *{source_name}*\n\n"
        # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
        for entry in reversed(new_entries):
            # åŸå§‹å†…å®¹å¤„ç†
            raw_title = remove_html_tags(entry.get('title', ''))  # ä½¿ç”¨ get æ–¹æ³•
            raw_url = entry.get('link', '')  # ä½¿ç”¨ get æ–¹æ³•

            # Markdownè½¬ä¹‰
            safe_title = escape_markdown_v2(raw_title, exclude=['*'])
            safe_url = escape_markdown_v2(raw_url)

            # æ„å»ºæ¶ˆæ¯
            merged_message += f"*{safe_title}*\n{safe_url}\n\n"
        merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"

        return merged_message

    except Exception as e:
        logger.error(f"å¤„ç†æº {feed_url} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ""

async def main():
    async with aiohttp.ClientSession() as session:
        bot = Bot(token=TELEGRAM_BOT_TOKEN)
        third_bot = Bot(token=RSS_TWO)
        fourth_bot = Bot(token=RSS_TOKEN)
        fifth_bot = Bot(token=RSSTWO_TOKEN)
        status = await load_status_from_supabase() # ä»supabaseåŠ è½½çŠ¶æ€

        try:
            # å¤„ç†ç¬¬ä¸€ç±»æº
            for idx, url in enumerate(RSS_FEEDS):
                if message := await process_feed(session, url, status, bot):
                    await send_single_message(bot, TELEGRAM_CHAT_ID[0], message, True)
                    logger.info(f"æˆåŠŸå¤„ç†ç¬¬ä¸€ç±»æº {idx+1}/{len(RSS_FEEDS)}")

            # å¤„ç†ç¬¬ä¸‰ç±»æº
            for idx, url in enumerate(THIRD_RSS_FEEDS):
                if message := await process_third_feed(session, url, status, third_bot):
                    await send_single_message(third_bot, TELEGRAM_CHAT_ID[0], message, True)
                    logger.info(f"æˆåŠŸå¤„ç†ç¬¬ä¸‰ç±»æº {idx + 1}/{len(THIRD_RSS_FEEDS)}")

            # å¤„ç†ç¬¬å››ç±»æº
            for idx, url in enumerate(FOURTH_RSS_FEEDS):
                if message := await process_fourth_feed(session, url, status, fourth_bot):
                    if message:  # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä¸ºç©º
                        await send_single_message(fourth_bot, TELEGRAM_CHAT_ID[0], message, disable_web_page_preview=True)
                        logger.info(f"æˆåŠŸå¤„ç†ç¬¬å››ç±»æº {idx + 1}/{len(FOURTH_RSS_FEEDS)}")
                else:
                    logger.info(f"ç¬¬å››ç±»æº {idx + 1}/{len(FOURTH_RSS_FEEDS)} æ²¡æœ‰æ–°æ¡ç›®æˆ–å‘ç”Ÿé”™è¯¯")

            # å¤„ç†ç¬¬äº”ç±»æº
            for idx, url in enumerate(FIFTH_RSS_FEEDS):
                if message := await process_fifth_feed(session, url, status, fifth_bot):
                    await send_single_message(fifth_bot, TELEGRAM_CHAT_ID[0], message, False)  # æ ¹æ®éœ€è¦è°ƒæ•´Trueä¸æµè§ˆ
                    logger.info(f"æˆåŠŸå¤„ç†ç¬¬äº”ç±»æº {idx + 1}/{len(FIFTH_RSS_FEEDS)}")

        except Exception as e:
            logger.critical(f"ä¸»å¾ªç¯å‘ç”Ÿè‡´å‘½é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())
