import asyncio
import aiohttp
import logging
import re
import os
import hashlib
import pytz
import fcntl
import sqlite3
import time
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# é…ç½®ç»å¯¹è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent

# åˆ›å»ºé”æ–‡ä»¶
LOCK_FILE = BASE_DIR / "rss.lock"

# SQLite æ•°æ®åº“åˆå§‹åŒ–
DATABASE_FILE = BASE_DIR / "rss_status.db"

# å¢å¼ºæ—¥å¿—é…ç½®
logging.basicConfig(
    filename=BASE_DIR / "rss.log",
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)
logger = logging.getLogger(__name__)

# æ¸…ç†è¶…è¿‡7å¤©çš„æ—¥å¿—æ–‡ä»¶
def clean_old_logs():
    log_file = BASE_DIR / "rss.log"
    if log_file.exists():
        log_modified_time = datetime.fromtimestamp(log_file.stat().st_mtime)
        if datetime.now() - log_modified_time > timedelta(days=2):
            try:
                log_file.unlink()
             #   logger.info("å·²æ¸…ç†è¶…è¿‡7å¤©çš„æ—¥å¿—æ–‡ä»¶")
            except Exception as e:
                logger.error(f"æ¸…ç†æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")

# åœ¨ç¨‹åºå¯åŠ¨æ—¶æ‰§è¡Œæ—¥å¿—æ¸…ç†
clean_old_logs()

TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")

# FIFTH_RSS_RSS_SANæ·»åŠ ï¼šå…³é”®è¯åˆ—è¡¨å’Œå¼€å…³
KEYWORDS = os.getenv("KEYWORDS", "").split(",")  # ä»ç¯å¢ƒå˜é‡è¯»å–å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”
KEYWORD_FILTER_ENABLED = os.getenv("KEYWORD_FILTER_ENABLED", "False").lower() == "true" # ä»ç¯å¢ƒå˜é‡è¯»å–å¼€å…³

MAX_CONCURRENT_REQUESTS = 2      #å¹¶å‘æ§åˆ¶
semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

# å®šä¹‰æ—¶é—´é—´éš” (ç§’)  600ç§’ = 10åˆ†é’Ÿ    1200ç§’ = 20åˆ†é’Ÿ   1800ç§’ = 30åˆ†é’Ÿ  3600ç§’ = 1å°æ—¶   7200ç§’ = 2å°æ—¶   10800ç§’ = 3å°æ—¶
RSS_GROUPS = [
    # ================== å›½é™…æ–°é—»ç»„ (åŸRSS_FEEDS) ==================
    {
        "name": "å›½é™…æ–°é—»",
        "urls": [
            'https://feeds.bbci.co.uk/news/world/rss.xml',  # BBC
            'https://www3.nhk.or.jp/rss/news/cat6.xml',     # NHK
            'https://www.cnbc.com/id/100003114/device/rss/rss.html',  # CNBC
            'https://feeds.a.dj.com/rss/RSSWorldNews.xml',  # åå°”è¡—æ—¥æŠ¥
            'https://www.aljazeera.com/xml/rss/all.xml',    # åŠå²›ç”µè§†å°
            'https://www.ft.com/?format=rss',                 # é‡‘èæ—¶æŠ¥
            'https://www3.nhk.or.jp/rss/news/cat5.xml',  # NHK å•†ä¸š
            'http://rss.cnn.com/rss/cnn_topstories.rss',   # cnn
            'https://www.theguardian.com/world/rss',     # å«æŠ¥
            'https://www.theverge.com/rss/index.xml',   # The Verge:
        ],
        "group_key": "RSS_FEEDS",
        "interval": 3300,      # 55åˆ†é’Ÿ (åŸRSSSS_FEEDS_INTERVAL)
        "bot_token": os.getenv("RSS_TWO"),  # åŸTELEGRAM_BOT_TOKEN
        "processor": {
            "translate": True,       #ç¿»è¯‘å¼€
            "template": "*{subject}*\n[{source}]({url})",
            "preview": False,         # ç¦æ­¢é¢„è§ˆ
            "show_count": False        # âœ…æ–°å¢
        }
    },

    # ================== å¿«è®¯ç»„ (åŸFOURTH_RSS_FEEDS) ==================
    {
        "name": "å¿«è®¯",
        "urls": [
    #        'https://rsshub.app/10jqka/realtimenews',
            'https://36kr.com/feed-newsflash',  # 36æ°ªå¿«è®¯
        ],
        "group_key": "FOURTH_RSS_FEEDS",
        "interval": 700,       # 11åˆ†é’Ÿ (åŸFOURTH_RSS_FEEDS_INTERVAL)
        "bot_token": os.getenv("RSS_LINDA"),  # åŸRSS_RSSSSS
        "processor": {
            "translate": False,
            "template": "*{subject}*\n[{source}]({url})",
            "preview": False,            # é¢„è§ˆ
            "show_count": False          #è®¡æ•°
        }
    },

    # ================== ç¤¾äº¤åª’ä½“ç»„ (åŸFIFTH_RSS_FEEDS) ==================
    {
        "name": "ç¤¾äº¤åª’ä½“",
        "urls": [
        #    'https://rsshub.app/twitter/media/clawcloud43609', # claw.cloud
            'https://rsshub.app/twitter/media/elonmusk',   # Elon Musk
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog',  # Asmongold
        ],
        "group_key": "FIFTH_RSS_FEEDS",
        "interval": 7000,      # 1å°æ—¶56åˆ†é’Ÿ (åŸFIFTH_RSS_FEEDS_INTERVAL)
        "bot_token": os.getenv("YOUTUBE_RSS"),  # åŸRSSTWO_TOKEN
        "processor": {
            "translate": True,
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\nğŸ”— {url}",
            "preview": True,        # é¢„è§ˆ
            "show_count": False     #è®¡æ•°
        }
    },

    # ================== æŠ€æœ¯è®ºå›ç»„ (åŸFIFTH_RSS_RSS_SAN) ==================
    {
        "name": "æŠ€æœ¯è®ºå›",
        "urls": [
            'https://rss.nodeseek.com/',  # Nodeseek
        ],
        "group_key": "FIFTH_RSS_RSS_SAN",
        "interval": 240,       # 4åˆ†é’Ÿ (åŸFIFTH_RSS_RSS_SAN_INTERVAL)
        "bot_token": os.getenv("RSS_SAN"),
        "processor": {
            "translate": False,
            "template": "*{subject}*\n[{source}]({url})",
            "keyword_filter": True,         #è¿‡æ»¤
            "preview": False,               # é¢„è§ˆ
            "show_count": False               #è®¡æ•°
        }
    },

    # ================== YouTubeé¢‘é“ç»„ (åŸYOUTUBE_RSSS_FEEDS) ==================
    {
        "name": "YouTubeé¢‘é“",
        "urls": [
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCvijahEyGtvMpmMHBu4FS2w', # é›¶åº¦è§£è¯´
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UC96OvMh0Mb_3NmuE8Dpu7Gg', # ææœºé›¶è·ç¦»
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCQoagx4VHBw3HkAyzvKEEBA', # ç§‘æŠ€å…±äº«
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCbCCUH8S3yhlm7__rhxR2QQ', # ä¸è‰¯æ—
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCMtXiCoKFrc2ovAGc1eywDg', # ä¸€ä¼‘
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCii04BCvYIdQvshrdNDAcww', # æ‚Ÿç©ºçš„æ—¥å¸¸
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCJMEiNh1HvpopPU3n9vJsMQ', # ç†ç§‘ç”·å£«
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCYjB6uufPeHSwuHs8wovLjg', # ä¸­æŒ‡é€š
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCSs4A6HYKmHA2MG_0z-F0xw', # ææ°¸ä¹è€å¸ˆ
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCZDgXi7VpKhBJxsPuZcBpgA', # å¯æ©KeEn
        #    'https://www.youtube.com/feeds/videos.xml?channel_id=UCxukdnZiXnTFvjF5B5dvJ5w', # ç”¬å“¥ä¾ƒä¾ƒä¾ƒygkkk
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCUfT9BAofYBKUTiEVrgYGZw', # ç§‘æŠ€åˆ†äº«
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UC51FT5EeNPiiQzatlA2RlRA', # ä¹Œå®¢wuke
        #    'https://www.youtube.com/feeds/videos.xml?channel_id=UCDD8WJ7Il3zWBgEYBUtc9xQ', # jack stone
        #    'https://www.youtube.com/feeds/videos.xml?channel_id=UCWurUlxgm7YJPPggDz9YJjw', # ä¸€ç“¶å¥¶æ²¹
        #    'https://www.youtube.com/feeds/videos.xml?channel_id=UCvENMyIFurJi_SrnbnbyiZw', # é…·å‹ç¤¾
        #    'https://www.youtube.com/feeds/videos.xml?channel_id=UCmhbF9emhHa-oZPiBfcLFaQ', # WenWeekly
        #    'https://www.youtube.com/feeds/videos.xml?channel_id=UC3BNSKOaphlEoK4L7QTlpbA', # ä¸­å¤–è§‚å¯Ÿ
        #    'https://www.youtube.com/feeds/videos.xml?channel_id=UCXk0rwHPG9eGV8SaF2p8KUQ', # çƒé´‰ç¬‘ç¬‘
                    # ... å…¶ä»–YouTubeé¢‘é“ï¼ˆå…±18ä¸ªï¼‰
        ],
        "group_key": "YOUTUBE_RSSS_FEEDS",
        "interval": 3300,      # 55åˆ†é’Ÿ (åŸYOUTUBE_RSSS_FEEDS_INTERVAL)
        "bot_token": os.getenv("RSS_TOKEN"),
        "processor": {
            "translate": False,
            "template": "*{subject}*\n[{source}]({url})",
            "preview": True,                # é¢„è§ˆ
            "show_count": False               #è®¡æ•°
        }
    },

    # ================== ä¸­æ–‡YouTubeç»„ (åŸFIFTH_RSS_YOUTUBE) ==================
    {
        "name": "ä¸­æ–‡YouTube",
        "urls": [
          #  'https://blog.090227.xyz/atom.xml',
          #  'https://www.freedidi.com/feed',
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCUNciDq-y6I6lEQPeoP-R5A', # è‹æ’è§‚å¯Ÿ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCXkOTZJ743JgVhJWmNV8F3Q', # å¯’åœ‹äºº
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC2r2LPbOUssIa02EbOIm7NA', # æ˜Ÿçƒç†±é»
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCF-Q1Zwyn9681F7du8DMAWg', # è¬å®—æ¡“-è€è¬ä¾†äº†
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCNiJNzSkfumLB7bYtXcIEmg', # çœŸçš„å¾ˆåšé€š
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCN0eCImZY6_OiJbo8cy5bLw', # å±ˆæ©ŸTV
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCb3TZ4SD_Ys3j4z0-8o6auA', # BBC News ä¸­æ–‡
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCiwt1aanVMoPYUt_CQYCPQg', # å…¨çƒå¤§è¦–é‡
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC000Jn3HGeQSwBuX_cLDK8Q', # æˆ‘æ˜¯æŸ³å‚‘å…‹
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFEBaHCJrHu2hzDA_69WQg', # å›½æ¼«è¯´
            'https://www.youtube.com/feeds/videos.xml?channel_id=UChJ8YKw6E1rjFHVS9vovrZw', # BNE TV - æ–°è¥¿å…°ä¸­æ–‡å›½é™…é¢‘é“
        # å½±è§†
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC7Xeh7thVIgs_qfTlwC-dag', # Marc TV
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCCD14H7fJQl3UZNWhYMG3Mg', # æ¸©åŸé²¤
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQO2T82PiHCYbqmCQ6QO6lw', # æœˆäº®èªª
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCHW6W9g2TJL2_Lf7GfoI5kg', # ç”µå½±æ”¾æ˜ å…

        ],
        "group_key": "FIFTH_RSS_YOUTUBE",
        "interval": 10400,     # 2å°æ—¶53åˆ†é’Ÿ (åŸFIFTH_RSS_YOUTUBE_INTERVAL)
        "bot_token": os.getenv("YOUTUBE_RSS"),
        "processor": {
        "translate": False,
        "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
        "template": "*{subject}*\nğŸ”— {url}",  # æ¡ç›®æ¨¡æ¿
        "preview": True,
        "show_count": False
    }
    },

    # ================== ä¸­æ–‡åª’ä½“ç»„ (åŸTHIRD_RSS_FEEDS) ==================
    {
        "name": "ä¸­æ–‡åª’ä½“", 
        "urls": [
            'https://rsshub.app/guancha',
            'https://rsshub.app/zaobao/znews/china',
            'https://rsshub.app/guancha/headline',
        ],
        "group_key": "THIRD_RSS_FEEDS",
        "interval": 7000,      # 1å°æ—¶56åˆ†é’Ÿ (åŸTHIRD_RSS_FEEDS_INTERVAL)
        "bot_token": os.getenv("RSS_LINDA_YOUTUBE"),  # åŸRSS_STWO
        "processor": {
            "translate": False,
            "template": "*{subject}*\n[{source}]({url})",
            "preview": False,
            "show_count": False
        }
    }
]

# æ–°å¢é€šç”¨å¤„ç†å‡½æ•°
async def process_group(session, group_config, global_status):
    """ç»Ÿä¸€å¤„ç†RSSç»„"""
    group_name = group_config["name"]
    group_key = group_config["group_key"]
    processor = group_config["processor"]
    bot_token = group_config["bot_token"]
    
    try:
        # ========== 0. åˆå§‹å»¶è¿Ÿ ==========
        await asyncio.sleep(1)  # ç»„é—´åˆå§‹å»¶è¿Ÿ1ç§’
        # ========== 1. æ£€æŸ¥æ—¶é—´é—´éš” ==========
        last_run = await load_last_run_time_from_db(group_key)
        now = time.time()
        if (now - last_run) < group_config["interval"]:
        #    remaining = group_config["interval"] - (now - last_run)
        #    logger.info(f"â³ è·³è¿‡ [{group_name}] è¿˜éœ€ç­‰å¾… {remaining:.0f}ç§’")
            return

   #     logger.info(f"ğŸš€ å¼€å§‹å¤„ç† [{group_name}] æº...")
        bot = Bot(token=bot_token)
        all_messages = []

        # ========== 2. å¤„ç†æ¯ä¸ªURLæº ==========
        for index, feed_url in enumerate(group_config["urls"]):
            try:
                # ===== 2.0 æºé—´å»¶è¿Ÿ =====
                if index > 0:  # ç¬¬ä¸€ä¸ªæºä¸éœ€è¦å»¶è¿Ÿ
                    await asyncio.sleep(1)  # æºé—´å»¶è¿Ÿ1ç§’
                # ------ 2.1 è·å–Feedæ•°æ® ------
                feed_data = await fetch_feed(session, feed_url)
                if not feed_data or not feed_data.entries:
                    logger.warning(f"âš ï¸ ç©ºæ•°æ®æº [{feed_url}]")
                    continue

                # ------ 2.2 åŠ è½½å¤„ç†çŠ¶æ€ ------
                processed_ids = global_status.get(feed_url, set())
                new_entries = []

                # ------ 2.3 å¤„ç†æ¯ä¸ªæ¡ç›® ------
                for entry in feed_data.entries:
                    entry_id = get_entry_identifier(entry)
                    if entry_id in processed_ids:
                        continue

                    # å…³é”®è¯è¿‡æ»¤
                    if processor.get("keyword_filter", False) and KEYWORD_FILTER_ENABLED:
                        raw_title = remove_html_tags(entry.title or "")
                        if not any(kw.lower() in raw_title.lower() for kw in KEYWORDS):
                            continue

                    new_entries.append(entry)
                    await save_single_status(group_key, feed_url, entry_id)
                    processed_ids.add(entry_id)

                global_status[feed_url] = processed_ids  # æ›´æ–°å†…å­˜çŠ¶æ€

                # ========== 2.4 ç”Ÿæˆæ¶ˆæ¯å†…å®¹ ==========
                if new_entries:
                    await asyncio.sleep(1)  # å‘é€å‰å»¶è¿Ÿ1ç§’
                    feed_message = await generate_group_message(feed_data, new_entries, processor)
                    if feed_message:  # æ–°å¢ï¼šç«‹å³å‘é€å½“å‰æºçš„æ¶ˆæ¯
                        await send_single_message(
                            bot,
                            TELEGRAM_CHAT_ID[0],
                            feed_message,
                            disable_web_page_preview=not processor.get("preview", True)
                        )
              #          logger.info(f"ğŸ“¤ å·²å‘é€ {len(new_entries)} æ¡å†…å®¹ [{feed_url}]")

            except Exception as e:
                logger.error(f"âŒ å¤„ç†æºå¤±è´¥ [{feed_url}]: {str(e)}", exc_info=True)

        # ========== 3. ä¿å­˜æœ€åè¿è¡Œæ—¶é—´ ==========
        await save_last_run_time_to_db(group_key, now)
        # ========== 4. æœ€ç»ˆå»¶è¿Ÿ ==========
        await asyncio.sleep(1)  # ç»„å¤„ç†å®Œæˆåå»¶è¿Ÿ3ç§’

    except Exception as e:
        logger.critical(f"â€¼ï¸ å¤„ç†ç»„å¤±è´¥ [{group_name}]: {str(e)}", exc_info=True)
 #   finally:
     #   logger.info(f"ğŸ å®Œæˆå¤„ç† [{group_name}]")

async def generate_group_message(feed_data, entries, processor):
    """ç”Ÿæˆæ ‡å‡†åŒ–æ¶ˆæ¯å†…å®¹"""
    try:
        # ===== 1. åŸºç¡€ä¿¡æ¯å¤„ç† =====
        source_name = feed_data.feed.get('title', "æœªçŸ¥æ¥æº")
        safe_source = escape_markdown_v2(source_name)
        
        # ===== æ–°å¢ï¼šæ ‡é¢˜å¤„ç† =====
        header = ""
        if "header_template" in processor:
            header = processor["header_template"].format(source=safe_source) + "\n"
        
        messages = []

        # ===== 2. å¤„ç†æ¯ä¸ªæ¡ç›® =====
        for entry in entries:
            # -- 2.1 æ ‡é¢˜å¤„ç† --
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            if processor["translate"]:
                translated_subject = await auto_translate_text(raw_subject)
            else:
                translated_subject = raw_subject
            safe_subject = escape_markdown_v2(translated_subject)

            # -- 2.2 é“¾æ¥å¤„ç† --
            raw_url = entry.link
            safe_url = escape_markdown_v2(raw_url)

            # -- 2.3 æ„å»ºæ¶ˆæ¯ --
            message = processor["template"].format(
                subject=safe_subject,
                source=safe_source,
                url=safe_url
            )
            messages.append(message)

        full_message = header + "\n\n".join(messages)
        
        if processor.get("show_count", True):
            full_message += f"\n\nâœ… æ–°å¢ {len(messages)} æ¡å†…å®¹"
            
        return full_message
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¶ˆæ¯å¤±è´¥: {str(e)}")
        return ""

def create_connection():
    """åˆ›å»º SQLite æ•°æ®åº“è¿æ¥"""
    conn = None
    try:
        conn = sqlite3.connect(DATABASE_FILE)
        return conn
    except sqlite3.Error as e:
        logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
    return conn

def create_table():
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            # ä»…ä¿ç•™SQLiteå»ºè¡¨è¯­å¥
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS rss_status (
                    feed_group TEXT,
                    feed_url TEXT,
                    entry_url TEXT,
                    entry_timestamp REAL,
                    PRIMARY KEY (feed_group, feed_url, entry_url)
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_run_time REAL
                )
            """)
            conn.commit()
       #     logger.info("æˆåŠŸåˆ›å»º/è¿æ¥åˆ°æœ¬åœ° SQLite æ•°æ®åº“å’Œè¡¨")
        except sqlite3.Error as e:
            logger.error(f"åˆ›å»ºæœ¬åœ°è¡¨å¤±è´¥: {e}")
        finally:
            conn.close()

async def load_last_run_time_from_db(feed_group):
    """ä»…ä½¿ç”¨SQLiteåŠ è½½æ—¶é—´æˆ³"""
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT last_run_time FROM timestamps WHERE feed_group = ?", (feed_group,))
            result = cursor.fetchone()
            return result[0] if result else 0
        except sqlite3.Error as e:
            logger.error(f"ä»æœ¬åœ°æ•°æ®åº“åŠ è½½æ—¶é—´å¤±è´¥: {e}")
            return 0
        finally:
            conn.close()
    return 0


async def save_last_run_time_to_db(feed_group, last_run_time):
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("BEGIN TRANSACTION")
            cursor.execute("""
                INSERT OR REPLACE INTO timestamps (feed_group, last_run_time)
                VALUES (?, ?)
            """, (feed_group, last_run_time))
            conn.commit()
        except sqlite3.Error as e:
            logger.error(f"æ—¶é—´æˆ³ä¿å­˜å¤±è´¥: {e}")
            conn.rollback()
        finally:
            conn.close()

# å‡½æ•° (ä¿æŒä¸å˜ï¼Œé™¤éå¦æœ‰è¯´æ˜)
def remove_html_tags(text):
    """å½»åº•ç§»é™¤hashtags, @ç¬¦å·, ä»¥åŠ"ã€ ã€‘" æ ·å¼çš„ç¬¦å·"""
    text = re.sub(r'#\w+', '', text)  # ç§»é™¤hashtags
    text = re.sub(r'@[^\s]+', '', text).strip()  # åˆ é™¤@åé¢çš„å­—ç¬¦
    text = re.sub(r'ã€\s*ã€‘', '', text)  # ç§»é™¤"ã€ ã€‘" æ ·å¼çš„ç¬¦å·ï¼ŒåŒ…å«ä¸­é—´çš„ç©ºæ ¼
    return text

def escape_markdown_v2(text, exclude=None):
    """è‡ªå®šä¹‰MarkdownV2è½¬ä¹‰å‡½æ•°"""
    if exclude is None:
        exclude = []
    chars = '_*[]()~`>#+-=|{}.!'
    chars_to_escape = [c for c in chars if c not in exclude]
    pattern = re.compile(f'([{"".join(map(re.escape, chars_to_escape))}])')
    return pattern.sub(r'\\\1', text)

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=5, max=30),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0

        # æŒ‰æ®µè½åˆ†å‰²ä¿æŒç»“æ„
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para_length = len(para.encode('utf-8'))
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2

        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))

        # v20.x çš„æ­£ç¡®å‚æ•°
        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview,
                read_timeout=10,  # è¯»å–è¶…æ—¶
                write_timeout=10  # å†™å…¥è¶…æ—¶
            )
    except BadRequest as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
    except Exception as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {e}")
        raise

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def fetch_feed(session, feed_url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    try:
        async with semaphore:
            async with session.get(feed_url, headers=headers, timeout=30) as response:
                # ç»Ÿä¸€å¤„ç†ä¸´æ—¶æ€§é”™è¯¯ï¼ˆ503/403ï¼‰
                if response.status in (503, 403):
                    logger.warning(f"RSSæºæš‚æ—¶ä¸å¯ç”¨ï¼ˆ{response.status}ï¼‰: {feed_url}")
                    return None  # è·³è¿‡å½“å‰æºï¼Œä¸‹æ¬¡è¿è¡Œä¼šé‡è¯•
                response.raise_for_status()
                return parse(await response.read())
    except aiohttp.ClientResponseError as e:
        if e.status in (503, 403):
            logger.warning(f"RSSæºæš‚æ—¶ä¸å¯ç”¨ï¼ˆ{e.status}ï¼‰: {feed_url}")
            return None
        logging.error(f"HTTP é”™è¯¯ {e.status} æŠ“å–å¤±è´¥ {feed_url}: {e}")
        raise
    except Exception as e:
        logging.error(f"æŠ“å–å¤±è´¥ {feed_url}: {e}")
        raise

async def auto_translate_text(text):
    try:
        cred = credential.Credential(TENCENTCLOUD_SECRET_ID, TENCENTCLOUD_SECRET_KEY)
        clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
        client = tmt_client.TmtClient(cred, "na-siliconvalley", clientProfile)

        req = models.TextTranslateRequest()
        req.SourceText = remove_html_tags(text)  # ç¿»è¯‘å‰å…ˆç§»é™¤HTML
        req.Source = "auto"
        req.Target = "zh"
        req.ProjectId = 0

        return client.TextTranslate(req).TargetText
    except Exception as e:
        logging.error(f"ç¿»è¯‘é”™è¯¯: {e}")
        return text

async def load_status():
    """ä»…ä»SQLiteåŠ è½½çŠ¶æ€"""
    status = {}
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT feed_url, entry_url FROM rss_status")
            for feed_url, entry_url in cursor.fetchall():
                if feed_url not in status:
                    status[feed_url] = set()
                status[feed_url].add(entry_url)
          #  logger.info("æœ¬åœ°çŠ¶æ€åŠ è½½æˆåŠŸ")
        except sqlite3.Error as e:
            logger.error(f"æœ¬åœ°çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
        finally:
            conn.close()
    return status

async def save_single_status(feed_group, feed_url, entry_url):
    """ä»…ä¿å­˜åˆ°SQLiteï¼Œä½¿ç”¨äº‹åŠ¡å’Œé‡è¯•"""
    timestamp = time.time()
    max_retries = 3
    for attempt in range(max_retries):
        conn = create_connection()
        if conn:
            try:
                cursor = conn.cursor()
                cursor.execute("BEGIN TRANSACTION")
                cursor.execute("""
                    INSERT OR IGNORE INTO rss_status 
                    VALUES (?, ?, ?, ?)
                """, (feed_group, feed_url, entry_url, timestamp))
                conn.commit()
                return
            except sqlite3.OperationalError as e:
                if "locked" in str(e) and attempt < max_retries - 1:
                    await asyncio.sleep(0.1 * (attempt + 1))
                    continue
                logger.error(f"SQLiteä¿å­˜å¤±è´¥ï¼ˆå°è¯•{attempt+1}æ¬¡ï¼‰: {e}")
            except sqlite3.Error as e:
                logger.error(f"SQLiteé”™è¯¯: {e}")
            finally:
                conn.close()

async def clean_old_entries(feed_group, max_age_days=30):
    """ä»…æ¸…ç†SQLiteæ—§è®°å½•"""
    cutoff_time = time.time() - max_age_days * 24 * 3600
    conn = create_connection()
    if conn:
        try:
            cursor = conn.cursor()
            cursor.execute("DELETE FROM rss_status WHERE feed_group = ? AND entry_timestamp < ?", 
                         (feed_group, cutoff_time))
            conn.commit()
          #  logger.info(f"æœ¬åœ°è®°å½•æ¸…ç†å®Œæˆ: {feed_group}")
        except sqlite3.Error as e:
            logger.error(f"æœ¬åœ°æ¸…ç†å¤±è´¥: {e}")
        finally:
            conn.close()

def get_entry_identifier(entry):
    # ä¼˜å…ˆä½¿ç”¨guid
    if hasattr(entry, 'guid') and entry.guid:
        return hashlib.sha256(entry.guid.encode()).hexdigest()
    
    # æ ‡å‡†åŒ–é“¾æ¥å¤„ç†
    link = getattr(entry, 'link', '')
    if link:
        try:
            parsed = urlparse(link)
            # ç§»é™¤æŸ¥è¯¢å‚æ•°ã€ç‰‡æ®µï¼Œå¹¶ç»Ÿä¸€ä¸ºå°å†™
            clean_link = parsed._replace(query=None, fragment=None).geturl().lower()
            return hashlib.sha256(clean_link.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"URLè§£æå¤±è´¥ {link}: {e}")
    
    # æœ€åä½¿ç”¨æ ‡é¢˜+å‘å¸ƒæ—¶é—´ç»„åˆ
    title = getattr(entry, 'title', '')
    pub_date = get_entry_timestamp(entry).isoformat() if get_entry_timestamp(entry) else ''
    return hashlib.sha256(f"{title}|||{pub_date}".encode()).hexdigest()

def get_entry_timestamp(entry):
    """è¿”å›UTCæ—¶é—´"""
    dt = datetime.now(pytz.UTC)  # é»˜è®¤å€¼
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        dt = datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        dt = datetime(*entry.pubDate_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        dt = datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc)
    return dt

async def process_feed_common(session, feed_group, feed_url, status):
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            return None

        processed_ids = status.get(feed_url, set())
        new_entries = []

        for entry in feed_data.entries:
            entry_id = get_entry_identifier(entry)
            if entry_id in processed_ids:
                continue

            new_entries.append(entry)
            # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
            await save_single_status(feed_group, feed_url, entry_id)
            # æ›´æ–°å†…å­˜ä¸­çš„çŠ¶æ€ï¼Œé˜²æ­¢åŒä¸€æ‰¹æ¬¡å†…é‡å¤
            processed_ids.add(entry_id)

        status[feed_url] = processed_ids  # æ›´æ–°å†…å­˜çŠ¶æ€
        return feed_data, new_entries

    except Exception as e:
        logger.error(f"å¤„ç†æºå¼‚å¸¸ {feed_url}: {e}")
        return None

async def main():
    """ä¸»å¤„ç†å‡½æ•°"""
    # ================== 1. æ–‡ä»¶é”å¤„ç† ==================
    lock_file = None
    try:
        lock_file = open(LOCK_FILE, "w")
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
    #    logger.info("ğŸ”’ æˆåŠŸè·å–æ–‡ä»¶é”ï¼Œå¯åŠ¨å¤„ç†æµç¨‹")
    except OSError:
        logger.warning("â›” æ— æ³•è·å–æ–‡ä»¶é”ï¼Œå·²æœ‰å®ä¾‹åœ¨è¿è¡Œï¼Œç¨‹åºé€€å‡º")
        return
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ–‡ä»¶é”å¼‚å¸¸: {str(e)}")
        return

    # ================== 2. æ•°æ®åº“åˆå§‹åŒ– ==================
    try:
        create_table()
 #       logger.info("ğŸ’¾ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return

    # ================== 3. æ—§æ—¥å¿—æ¸…ç† ==================
    try:
        clean_old_logs()
    #    logger.info("ğŸ—‘ï¸ æ—§æ—¥å¿—æ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.error(f"âš ï¸ æ—¥å¿—æ¸…ç†å¤±è´¥: {str(e)}")

    # ================== 4. ä¸»å¤„ç†æµç¨‹ ==================
    async with aiohttp.ClientSession() as session:
        try:
            # ===== 4.1 åŠ è½½å¤„ç†çŠ¶æ€ =====
            status = await load_status()
     #       logger.info("ğŸ“‚ åŠ è½½å†å²çŠ¶æ€å®Œæˆ")

            # ===== 4.2 æ¸…ç†æ—§è®°å½• =====
            retention_config = {
                "RSS_FEEDS": 30,
                "THIRD_RSS_FEEDS": 30,
                "FOURTH_RSS_FEEDS": 7,
                "FIFTH_RSS_FEEDS": 30,
                "FIFTH_RSS_RSS_SAN": 7,
                "YOUTUBE_RSSS_FEEDS": 30,
                "FIFTH_RSS_YOUTUBE": 30
            }
            
            for group in RSS_GROUPS:
                try:
                    await clean_old_entries(
                        group["group_key"], 
                        retention_config.get(group["group_key"], 30)
                    )
                except Exception as e:
                    logger.error(f"âš ï¸ æ¸…ç†æ—§è®°å½•å¤±è´¥ [{group['name']}]: {str(e)}")

            # ===== 4.3 åˆ›å»ºå¤„ç†ä»»åŠ¡ =====
            tasks = []
            for group in RSS_GROUPS:
                try:
                    tasks.append(process_group(session, group, status))
              #      logger.debug(f"ğŸ“¨ å·²åˆ›å»ºå¤„ç†ä»»åŠ¡ [{group['name']}]")
                except Exception as e:
                    logger.error(f"âš ï¸ åˆ›å»ºä»»åŠ¡å¤±è´¥ [{group['name']}]: {str(e)}")

            # ===== 4.4 å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ =====
            if tasks:
                await asyncio.gather(*tasks)
          #      logger.info("ğŸš© æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²å®Œæˆ")
            else:
                logger.warning("â›” æœªåˆ›å»ºä»»ä½•å¤„ç†ä»»åŠ¡")

        except asyncio.CancelledError:
            logger.warning("â¹ï¸ ä»»åŠ¡è¢«å–æ¶ˆ")
        except Exception as e:
            logger.critical(f"â€¼ï¸ ä¸»å¾ªç¯å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            # ===== 4.5 æœ€ç»ˆæ¸…ç† =====
            try:
                await session.close()
     #           logger.info("ğŸ”Œ å·²å…³é—­ç½‘ç»œä¼šè¯")
            except Exception as e:
                logger.error(f"âš ï¸ å…³é—­ä¼šè¯å¤±è´¥: {str(e)}")

    # ================== 5. é‡Šæ”¾æ–‡ä»¶é” ==================
    try:
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
        lock_file.close()
   #     logger.info("ğŸ”“ æ–‡ä»¶é”å·²é‡Šæ”¾")
    except Exception as e:
        logger.error(f"âš ï¸ é‡Šæ”¾æ–‡ä»¶é”å¤±è´¥: {str(e)}")

    # ================== 6. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š ==================
 #   logger.info("ğŸ ç¨‹åºè¿è¡Œç»“æŸ\n" + "="*50 + "\n")

if __name__ == "__main__":
    # ç¡®ä¿å…ˆåˆ›å»ºæ–°è¡¨ç»“æ„
    create_table()
    asyncio.run(main())