import asyncio
import aiohttp
import logging
import re
import os
import hashlib
import pytz
import fcntl
import time
import signal
import aiosqlite
import sys
import shutil
from pathlib import Path
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from telegram.ext import Application
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from md2tgmd import escape
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models
from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException
from collections import defaultdict
from langdetect import detect, LangDetectException
from datetime import datetime, timezone
import importlib.util

# ========== é¦–å…ˆåŠ è½½ç¯å¢ƒå˜é‡ ==========
load_dotenv()  # åœ¨é¡¶éƒ¨ç«‹å³åŠ è½½

# ========== é…ç½®åŒæ­¥è®¾ç½® ==========
CONFIG_URL = os.getenv("CONFIG_URL")
LOCAL_CONFIG_FILE = Path(__file__).parent / "rss_config.py"
CONFIG_CACHE_TIME = 24 * 3600  # 24å°æ—¶ç¼“å­˜

# å…¨å±€é…ç½®ç®¡ç†å™¨
config_manager = None

# ========== åŸæœ‰çš„å…¨å±€å˜é‡å’Œé…ç½® ==========
SHOULD_EXIT = False

# è®¾ç½®æ—¶åŒºï¼ˆåœ¨cronç¯å¢ƒä¸­å¾ˆé‡è¦ï¼‰
os.environ['TZ'] = 'Asia/Singapore'
try:
    time.tzset()  # Linuxç³»ç»Ÿ
except AttributeError:
    pass  # Windowsç³»ç»Ÿå¿½ç•¥

BASE_DIR = Path(__file__).resolve().parent
LOCK_FILE = BASE_DIR / "rss.lock"
DATABASE_FILE = BASE_DIR / "rss.db"

logging.basicConfig(
    filename=BASE_DIR / "rss.log",
    level=logging.WARNING,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)
logger = logging.getLogger(__name__)

# ç°åœ¨è¿™äº›ç¯å¢ƒå˜é‡ä¼šåœ¨ load_dotenv() ä¹‹åæ­£ç¡®åŠ è½½
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")
TENCENT_REGION = os.getenv("TENCENT_REGION", "na-siliconvalley")
TENCENT_SECRET_ID = os.getenv("TENCENT_SECRET_ID")
TENCENT_SECRET_KEY = os.getenv("TENCENT_SECRET_KEY")
semaphore = asyncio.Semaphore(2)
BACKUP_DOMAINS_STR = os.getenv("BACKUP_DOMAINS", "")
BACKUP_DOMAINS = [domain.strip() for domain in BACKUP_DOMAINS_STR.split(",") if domain.strip()]

# ========== å…¨å±€ RSS_GROUPS å˜é‡ ==========
RSS_GROUPS = []  # å°†åœ¨mainå‡½æ•°ä¸­ä»é…ç½®æ–‡ä»¶åŠ è½½

# ========== æ•°æ®åº“é…ç½® ==========
PG_URL = os.getenv("PG_URL")
USE_PG = PG_URL is not None

# æ—¥å¿—è®°å½•æ•°æ®åº“ç±»å‹
if USE_PG:
    # å®‰å…¨åœ°è®°å½•æ•°æ®åº“ä¿¡æ¯ï¼ˆéšè—å¯†ç ï¼‰
    safe_pg_url = re.sub(r':([^@]+)@', ':****@', PG_URL) if PG_URL else "æœªé…ç½®"
    logger.info(f"ğŸ”§ ä½¿ç”¨ PostgreSQL æ•°æ®åº“: {safe_pg_url}")
    print(f"âœ… PostgreSQL ")
else:
    logger.info(f"ğŸ”§ ä½¿ç”¨ SQLite æ•°æ®åº“: {DATABASE_FILE}")
    print(f"âœ… SQLite : {DATABASE_FILE}")

class ConfigManager:
    def __init__(self, db=None):
        self.last_update = 0
        self.db = db
        
    async def ensure_fresh_config(self):
        """ç¡®ä¿é…ç½®æ˜¯æœ€æ–°çš„"""
        # å¦‚æœæœ¬åœ°é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼ºåˆ¶ä»GitHubä¸‹è½½
        if not LOCAL_CONFIG_FILE.exists():
            logger.info("ğŸ“¥ é¦–æ¬¡è¿è¡Œï¼Œæœ¬åœ°é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»GitHubä¸‹è½½é…ç½®...")
            success = await self._update_config_from_github()
            if success:
                logger.info("âœ… é¦–æ¬¡é…ç½®ä¸‹è½½æˆåŠŸ")
                if self.db:
                    await self.db.save_config_update_time()
                    logger.info("âœ… é…ç½®æ›´æ–°æ—¶é—´å·²ä¿å­˜åˆ°æ•°æ®åº“")
            else:
                logger.error("âŒ é¦–æ¬¡é…ç½®ä¸‹è½½å¤±è´¥ï¼Œç¨‹åºæ— æ³•ç»§ç»­")
                # å¯ä»¥åœ¨è¿™é‡Œåˆ›å»ºé»˜è®¤é…ç½®æˆ–é€€å‡ºç¨‹åº
            return
            
        # æœ¬åœ°é…ç½®æ–‡ä»¶å­˜åœ¨ï¼ŒæŒ‰æ—¶é—´ç¼“å­˜æ£€æŸ¥
        if not await self._is_config_fresh():
            logger.info("ğŸ”„ é…ç½®å·²è¿‡æœŸï¼Œä»GitHubæ›´æ–°...")
            success = await self._update_config_from_github()
            if success and self.db:
                await self.db.save_config_update_time()
                logger.info("âœ… é…ç½®æ›´æ–°æ—¶é—´å·²ä¿å­˜åˆ°æ•°æ®åº“")
        else:
            logger.info("âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜é…ç½®")
    
    async def _is_config_fresh(self):
        """æ£€æŸ¥é…ç½®æ˜¯å¦åœ¨24å°æ—¶å†…æ›´æ–°è¿‡"""
        # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåº”è¯¥è¿”å›Falseå¼ºåˆ¶æ›´æ–°ï¼Œä½†è¿™ç§æƒ…å†µåº”è¯¥åœ¨ä¸Šå±‚å¤„ç†
        if not LOCAL_CONFIG_FILE.exists():
            return False
            
        if not self.db:
            # å¦‚æœæ²¡æœ‰æ•°æ®åº“ï¼Œå›é€€åˆ°æ–‡ä»¶æ—¶é—´æ£€æŸ¥
            return await self._fallback_config_fresh_check()
            
        try:
            last_update = await self.db.get_last_config_update_time()
            if last_update is None:
                logger.info("ğŸ“ é¦–æ¬¡è¿è¡Œæˆ–æœªæ‰¾åˆ°é…ç½®æ›´æ–°æ—¶é—´è®°å½•")
                return False
                
            is_fresh = (time.time() - last_update) < CONFIG_CACHE_TIME
            if is_fresh:
                logger.info(f"ğŸ•’ é…ç½®ä»åœ¨æœ‰æ•ˆæœŸå†…ï¼Œå‰©ä½™ {int((CONFIG_CACHE_TIME - (time.time() - last_update)) / 3600)} å°æ—¶")
            return is_fresh
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®åº“æ£€æŸ¥å¤±è´¥ï¼Œå›é€€åˆ°æ–‡ä»¶æ£€æŸ¥: {e}")
            return await self._fallback_config_fresh_check()
    
    async def _fallback_config_fresh_check(self):
        """å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ£€æŸ¥"""
        if not LOCAL_CONFIG_FILE.exists():
            return False
        
        file_mtime = LOCAL_CONFIG_FILE.stat().st_mtime
        is_fresh = (time.time() - file_mtime) < CONFIG_CACHE_TIME
        
        if is_fresh:
            remaining_hours = int((CONFIG_CACHE_TIME - (time.time() - file_mtime)) / 3600)
            logger.info(f"ğŸ•’ æ–‡ä»¶æ£€æŸ¥ï¼šé…ç½®ä»åœ¨æœ‰æ•ˆæœŸå†…ï¼Œå‰©ä½™ {remaining_hours} å°æ—¶")
        
        return is_fresh
    
    async def _update_config_from_github(self):
        """ä»GitHubæ›´æ–°é…ç½®"""
        if not CONFIG_URL:
            logger.error("âŒ CONFIG_URL æœªè®¾ç½®ï¼Œæ— æ³•ä»GitHubæ›´æ–°é…ç½®")
            return False
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
        print(f"ğŸ”„ æ­£åœ¨ä» GitHub ä¸‹è½½é…ç½®: {CONFIG_URL}")
    
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(CONFIG_URL, headers=headers, timeout=30) as response:
                    print(f"ğŸ“¡ GitHub å“åº”çŠ¶æ€ç : {response.status}")
                
                    if response.status == 200:
                        content = await response.text()
                        print(f"ğŸ“¥ ä¸‹è½½å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    
                    # å®‰å…¨æ£€æŸ¥
                        if not self._is_safe_python_content(content):
                            logger.error("âŒ ä¸‹è½½çš„é…ç½®åŒ…å«ä¸å®‰å…¨ä»£ç ")
                            return False
                    
                    # å¤‡ä»½å½“å‰é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if LOCAL_CONFIG_FILE.exists():
                            await self._backup_current_config()
                    
                    # ä¿å­˜æ–°é…ç½®
                        with open(LOCAL_CONFIG_FILE, 'w', encoding='utf-8') as f:
                            f.write(content)
                    
                        print(f"ğŸ’¾ é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {LOCAL_CONFIG_FILE}")
                        logger.info("âœ… é…ç½®å·²ä»GitHubæ›´æ–°")
                        return True
                    else:
                        logger.error(f"âŒ GitHubè¯·æ±‚å¤±è´¥: HTTP {response.status}")
                        print(f"âŒ GitHubè¯·æ±‚å¤±è´¥: HTTP {response.status}")
                        return False
        except Exception as e:
            logger.error(f"âŒ é…ç½®æ›´æ–°å¤±è´¥: {e}")
            print(f"âŒ é…ç½®æ›´æ–°å¤±è´¥: {e}")
            return False
    
    async def _backup_current_config(self):
        """å¤‡ä»½å½“å‰é…ç½®"""
        if LOCAL_CONFIG_FILE.exists():
            backup_file = LOCAL_CONFIG_FILE.with_suffix(f'.backup.{int(time.time())}.py')
            try:
                shutil.copy2(LOCAL_CONFIG_FILE, backup_file)
                logger.info(f"ğŸ“¦ é…ç½®å·²å¤‡ä»½åˆ°: {backup_file.name}")
            except Exception as e:
                logger.warning(f"âš ï¸ é…ç½®å¤‡ä»½å¤±è´¥: {e}")
    
    def _is_safe_python_content(self, content):
        """å®‰å…¨æ£€æŸ¥é…ç½®å†…å®¹"""
        dangerous_keywords = [
            '__import__', 'eval(', 'exec(', 'compile(',
            'os.system', 'os.popen', 'subprocess.', 'import requests',
            'import urllib', 'import http'
        ]
        
        content_lower = content.lower()
        for keyword in dangerous_keywords:
            if keyword in content_lower:
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸å®‰å…¨ä»£ç : {keyword}")
                return False
        return True
    
    def load_local_config(self):
        """åŠ è½½æœ¬åœ°é…ç½®æ–‡ä»¶"""
        try:
            if not LOCAL_CONFIG_FILE.exists():
                logger.error("âŒ æœ¬åœ°é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
                return []
                
            spec = importlib.util.spec_from_file_location("rss_config", LOCAL_CONFIG_FILE)
            config_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config_module)
            
            if hasattr(config_module, 'RSS_GROUPS'):
                # è·³è¿‡éªŒè¯é…ç½®
                # if hasattr(config_module, 'validate_config'):
                #     config_module.validate_config()
                return config_module.RSS_GROUPS
            else:
                raise Exception("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°RSS_GROUPSå˜é‡")
                
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°é…ç½®åŠ è½½å¤±è´¥: {e}")
            # è¿”å›ç©ºé…ç½®ï¼Œè®©ç¨‹åºä¼˜é›…é€€å‡º
            return []

if USE_PG:
    import asyncpg
class RSSDatabase:
    def __init__(self, loop=None):
        self.loop = loop or asyncio.get_event_loop()
        self.conn = None
        self.pg_pool = None

    async def open(self):
        if USE_PG:
            self.pg_pool = await asyncpg.create_pool(PG_URL)
        else:
            self.conn = await aiosqlite.connect(DATABASE_FILE)

    async def close(self):
        if USE_PG and self.pg_pool:
            await self.pg_pool.close()
        elif self.conn:
            await self.conn.close()

    async def ensure_initialized(self):
        """ç¡®ä¿æ•°æ®åº“è¡¨å·²åˆ›å»º"""
        await self.create_tables()

    async def get_last_config_update_time(self):  # ğŸ”§ è¿™ä¸ªå‡½æ•°éœ€è¦ç¼©è¿›åˆ°ç±»å†…éƒ¨
        """è·å–æœ€åé…ç½®æ›´æ–°æ—¶é—´"""
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                # å…ˆç¡®ä¿è¡¨å­˜åœ¨
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS app_config (
                        key TEXT PRIMARY KEY,
                        value TEXT
                    )
                """)
                row = await conn.fetchrow("SELECT value FROM app_config WHERE key = 'config_last_update'")
                return float(row['value']) if row else None
        else:
            async with self.conn.cursor() as c:
                # å…ˆç¡®ä¿è¡¨å­˜åœ¨
                await c.execute("""
                    CREATE TABLE IF NOT EXISTS app_config (
                        key TEXT PRIMARY KEY,
                        value TEXT
                    )
                """)
                await c.execute("SELECT value FROM app_config WHERE key = 'config_last_update'")
                result = await c.fetchone()
                return float(result[0]) if result else None

    async def save_config_update_time(self):  # ğŸ”§ è¿™ä¸ªå‡½æ•°éœ€è¦ç¼©è¿›åˆ°ç±»å†…éƒ¨
        """ä¿å­˜é…ç½®æ›´æ–°æ—¶é—´"""
        current_time = time.time()
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                # å…ˆç¡®ä¿è¡¨å­˜åœ¨
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS app_config (
                        key TEXT PRIMARY KEY,
                        value TEXT
                    )
                """)
                await conn.execute("""
                    INSERT INTO app_config (key, value) 
                    VALUES ('config_last_update', $1)
                    ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value
                """, str(current_time))
        else:
            async with self.conn.cursor() as c:
                # å…ˆç¡®ä¿è¡¨å­˜åœ¨
                await c.execute("""
                    CREATE TABLE IF NOT EXISTS app_config (
                        key TEXT PRIMARY KEY,
                        value TEXT
                    )
                """)
                await c.execute("""
                    INSERT OR REPLACE INTO app_config (key, value)
                    VALUES ('config_last_update', ?)
                """, (str(current_time),))
                await self.conn.commit()

    async def create_tables(self):  # ğŸ”§ è¿™ä¸ªå‡½æ•°éœ€è¦ç¼©è¿›åˆ°ç±»å†…éƒ¨
        """æ”¹è¿›çš„å»ºè¡¨è¯­å¥ï¼Œç¡®ä¿ PostgreSQL å’Œ SQLite ç´¢å¼•ä¸€è‡´"""
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                # ä¸»è¡¨
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS rss_status (
                        feed_group TEXT,
                        feed_url TEXT,
                        entry_url TEXT,
                        entry_content_hash TEXT,
                        entry_timestamp DOUBLE PRECISION,
                        PRIMARY KEY (feed_group, feed_url, entry_url)
                    );
                """)
                # ç¡®ä¿å†…å®¹å“ˆå¸Œç´¢å¼•å­˜åœ¨
                await conn.execute("""
                    CREATE UNIQUE INDEX IF NOT EXISTS idx_group_content_hash 
                    ON rss_status(feed_group, entry_content_hash);
                """)
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS timestamps (
                        feed_group TEXT PRIMARY KEY,
                        last_run_time DOUBLE PRECISION
                    );
                """)
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS cleanup_timestamps (
                        feed_group TEXT PRIMARY KEY,
                        last_cleanup_time DOUBLE PRECISION
                    );
                """)
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS pending_messages (
                        feed_group TEXT,
                        feed_url TEXT,
                        entry_id TEXT,
                        content_hash TEXT,
                        title TEXT,
                        translated_title TEXT,
                        link TEXT,
                        summary TEXT,
                        entry_timestamp DOUBLE PRECISION,
                        sent INTEGER DEFAULT 0,
                        feed_title TEXT,
                        PRIMARY KEY (feed_group, feed_url, entry_id)
                    );
                """)
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS batch_timestamps (
                        feed_group TEXT PRIMARY KEY,
                        last_batch_sent_time DOUBLE PRECISION
                    );
                """)
                # æ–°å¢ï¼šåº”ç”¨é…ç½®è¡¨
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS app_config (
                        key TEXT PRIMARY KEY,
                        value TEXT
                    );
                """)
        else:
            async with self.conn.cursor() as c:
                await c.execute("""
                    CREATE TABLE IF NOT EXISTS rss_status (
                        feed_group TEXT,
                        feed_url TEXT,
                        entry_url TEXT,
                        entry_content_hash TEXT,
                        entry_timestamp REAL,
                        PRIMARY KEY (feed_group, feed_url, entry_url)
                    )""")
                await c.execute("""
                    CREATE UNIQUE INDEX IF NOT EXISTS idx_group_content_hash 
                    ON rss_status(feed_group, entry_content_hash);
                """)
                await c.execute("""
                    CREATE TABLE IF NOT EXISTS timestamps (
                        feed_group TEXT PRIMARY KEY,
                        last_run_time REAL
                    )""")
                await c.execute("""
                    CREATE TABLE IF NOT EXISTS cleanup_timestamps (
                        feed_group TEXT PRIMARY KEY,
                        last_cleanup_time REAL
                    )""")
                await c.execute("""
                    CREATE TABLE IF NOT EXISTS pending_messages (
                        feed_group TEXT,
                        feed_url TEXT,
                        entry_id TEXT,
                        content_hash TEXT,
                        title TEXT,
                        translated_title TEXT,
                        link TEXT,
                        summary TEXT,
                        entry_timestamp REAL,
                        sent INTEGER DEFAULT 0,
                        feed_title TEXT,
                        PRIMARY KEY (feed_group, feed_url, entry_id)
                    )
                """)
                await c.execute("""
                    CREATE TABLE IF NOT EXISTS batch_timestamps (
                        feed_group TEXT PRIMARY KEY,
                        last_batch_sent_time REAL
                    )
                """)
                # æ–°å¢ï¼šåº”ç”¨é…ç½®è¡¨
                await c.execute("""
                    CREATE TABLE IF NOT EXISTS app_config (
                        key TEXT PRIMARY KEY,
                        value TEXT
                    )
                """)
                await self.conn.commit()

    async def add_pending_message(self, feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, timestamp, feed_title):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.execute("""
                INSERT INTO pending_messages (feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, entry_timestamp, sent, feed_title)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, 0, $10)
                ON CONFLICT DO NOTHING
                """, feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, timestamp, feed_title)
        else:
            async with self.conn.cursor() as c:
                await c.execute("""
                    INSERT OR IGNORE INTO pending_messages
                    (feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, entry_timestamp, sent, feed_title)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 0, ?)
                """, (feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, timestamp, feed_title))
                await self.conn.commit()

    async def get_pending_messages(self, feed_group):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                rows = await conn.fetch("""
                    SELECT * FROM pending_messages
                    WHERE feed_group=$1 AND sent=0
                    ORDER BY entry_timestamp ASC
                """, feed_group)
                return [dict(row) for row in rows]
        else:
            async with self.conn.cursor() as c:
                await c.execute("""
                    SELECT * FROM pending_messages
                    WHERE feed_group=? AND sent=0
                    ORDER BY entry_timestamp ASC
                """, (feed_group,))
                keys = [d[0] for d in c.description]
                rows = await c.fetchall()
                return [dict(zip(keys, row)) for row in rows]

    async def mark_pending_as_sent(self, feed_group, ids):
        if not ids:
            return
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.executemany("""
                    UPDATE pending_messages SET sent=1
                    WHERE feed_group=$1 AND entry_id=$2
                """, [(feed_group, eid) for eid in ids])
        else:
            async with self.conn.cursor() as c:
                await c.executemany("""
                    UPDATE pending_messages SET sent=1
                    WHERE feed_group=? AND entry_id=?
                """, [(feed_group, eid) for eid in ids])
                await self.conn.commit()

    async def get_last_batch_sent_time(self, feed_group):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow("""
                    SELECT last_batch_sent_time FROM batch_timestamps WHERE feed_group=$1
                """, feed_group)
                return row['last_batch_sent_time'] if row else 0
        else:
            async with self.conn.cursor() as c:
                await c.execute("""
                    SELECT last_batch_sent_time FROM batch_timestamps WHERE feed_group=?
                """, (feed_group,))
                result = await c.fetchone()
                return result[0] if result else 0

    async def save_last_batch_sent_time(self, feed_group, ts):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.execute("""
                INSERT INTO batch_timestamps (feed_group, last_batch_sent_time)
                VALUES ($1, $2)
                ON CONFLICT (feed_group) DO UPDATE SET last_batch_sent_time=EXCLUDED.last_batch_sent_time
                """, feed_group, ts)
        else:
            async with self.conn.cursor() as c:
                await c.execute("""
                    INSERT OR REPLACE INTO batch_timestamps (feed_group, last_batch_sent_time)
                    VALUES (?, ?)
                """, (feed_group, ts))
                await self.conn.commit()

    async def save_status(self, feed_group, feed_url, entry_url, entry_content_hash, timestamp):
        """æ”¹è¿›çš„çŠ¶æ€ä¿å­˜ï¼Œç¡®ä¿å»é‡ä¸€è‡´æ€§"""
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                # ä½¿ç”¨ ON CONFLICT ç¡®ä¿å”¯ä¸€æ€§
                await conn.execute("""
                    INSERT INTO rss_status (feed_group, feed_url, entry_url, entry_content_hash, entry_timestamp) 
                    VALUES($1, $2, $3, $4, $5) 
                    ON CONFLICT (feed_group, feed_url, entry_url) 
                    DO UPDATE SET 
                        entry_content_hash = EXCLUDED.entry_content_hash,
                        entry_timestamp = EXCLUDED.entry_timestamp
                """, feed_group, feed_url, entry_url, entry_content_hash, timestamp)
        else:
            async with self.conn.cursor() as c:
                await c.execute(
                    "INSERT OR REPLACE INTO rss_status VALUES (?, ?, ?, ?, ?)",
                    (feed_group, feed_url, entry_url, entry_content_hash, timestamp)
                )
                await self.conn.commit()

    async def has_content_hash(self, feed_group, content_hash):
        """æ”¹è¿›çš„å†…å®¹å“ˆå¸Œæ£€æŸ¥ï¼Œç¡®ä¿ç¼–ç ä¸€è‡´æ€§"""
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                # ä¿®å¤ï¼šPostgreSQL å‚æ•°å ä½ç¬¦é”™è¯¯ï¼Œåº”è¯¥æ˜¯ $1, $2
                row = await conn.fetchrow(
                    "SELECT 1 FROM rss_status WHERE feed_group=$1 AND entry_content_hash=$2 LIMIT 1",
                    feed_group, content_hash
                )
                return row is not None
        else:
            async with self.conn.cursor() as c:
                await c.execute(
                    "SELECT 1 FROM rss_status WHERE feed_group=? AND entry_content_hash=? LIMIT 1",
                    (feed_group, content_hash)
                )
                return await c.fetchone() is not None

    async def load_status(self):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                rows = await conn.fetch("SELECT feed_url, entry_url FROM rss_status")
                status = {}
                for row in rows:
                    feed_url, entry_url = row['feed_url'], row['entry_url']
                    status.setdefault(feed_url, set()).add(entry_url)
                return status
        else:
            async with self.conn.cursor() as c:
                await c.execute("SELECT feed_url, entry_url FROM rss_status")
                rows = await c.fetchall()
                status = {}
                for feed_url, entry_url in rows:
                    status.setdefault(feed_url, set()).add(entry_url)
                return status

    async def load_last_run_time(self, feed_group):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow("SELECT last_run_time FROM timestamps WHERE feed_group=$1", feed_group)
                return row['last_run_time'] if row else 0
        else:
            async with self.conn.cursor() as c:
                await c.execute("SELECT last_run_time FROM timestamps WHERE feed_group = ?", (feed_group,))
                result = await c.fetchone()
                return result[0] if result else 0

    async def save_last_run_time(self, feed_group, last_run_time):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.execute("""
                INSERT INTO timestamps (feed_group, last_run_time)
                VALUES ($1, $2)
                ON CONFLICT (feed_group) DO UPDATE SET last_run_time=EXCLUDED.last_run_time
                """, feed_group, last_run_time)
        else:
            async with self.conn.cursor() as c:
                await c.execute("""
                    INSERT OR REPLACE INTO timestamps (feed_group, last_run_time)
                    VALUES (?, ?)
                """, (feed_group, last_run_time))
                await self.conn.commit()

    async def cleanup_history(self, days, feed_group):
        now = time.time()
        cutoff_ts = now - days * 86400

        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow(
                    "SELECT last_cleanup_time FROM cleanup_timestamps WHERE feed_group=$1", feed_group
                )
                last_cleanup = row['last_cleanup_time'] if row else 0
                if now - last_cleanup < 86400:
                    return
                await conn.execute(
                    "DELETE FROM rss_status WHERE feed_group=$1 AND entry_timestamp<$2",
                    feed_group, cutoff_ts
                )
                await conn.execute("""
                    INSERT INTO cleanup_timestamps (feed_group, last_cleanup_time)
                    VALUES ($1, $2)
                    ON CONFLICT (feed_group) DO UPDATE SET last_cleanup_time=EXCLUDED.last_cleanup_time
                """, feed_group, now)
        else:
            async with self.conn.cursor() as c:
                await c.execute(
                    "SELECT last_cleanup_time FROM cleanup_timestamps WHERE feed_group = ?",
                    (feed_group,)
                )
                result = await c.fetchone()
                last_cleanup = result[0] if result else 0
                if now - last_cleanup < 86400:
                    return
                await c.execute(
                    "DELETE FROM rss_status WHERE feed_group=? AND entry_timestamp < ?",
                    (feed_group, cutoff_ts)
                )
                await c.execute("""
                    INSERT OR REPLACE INTO cleanup_timestamps (feed_group, last_cleanup_time)
                    VALUES (?, ?)
                """, (feed_group, now))
                await self.conn.commit()

# ========== ä¸šåŠ¡é€»è¾‘ ==========

def remove_html_tags(text):
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'#([^#\s]+)#', r'\1', text)
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'@[^\s]+', '', text).strip()
    text = re.sub(r'ã€\s*ã€‘', '', text)
    text = re.sub(r'(?<!\S)#(?!\S)', '', text)
    text = re.sub(r'(?<!\S)ï¼š(?!\S)', '', text)
    text = text.replace('.', '.\u200c')
    return text

def get_entry_identifier(entry):
    if hasattr(entry, 'guid') and entry.guid:
        return hashlib.sha256(entry.guid.encode()).hexdigest()
    link = getattr(entry, 'link', '')
    if link:
        try:
            parsed = urlparse(link)
            clean_link = parsed._replace(query=None, fragment=None).geturl().lower()
            return hashlib.sha256(clean_link.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"URLè§£æå¤±è´¥ {link}: {e}")
    title = getattr(entry, 'title', '')
    pub_date = get_entry_timestamp(entry).isoformat() if get_entry_timestamp(entry) else ''
    return hashlib.sha256(f"{title}|||{pub_date}".encode()).hexdigest()

def get_entry_content_hash(entry):
    """æ”¹è¿›çš„å†…å®¹å“ˆå¸Œè®¡ç®—ï¼Œç¡®ä¿ç¼–ç ä¸€è‡´æ€§"""
    title = getattr(entry, 'title', '') or ''
    summary = getattr(entry, 'summary', '') or ''
    
    # ç»Ÿä¸€å¤„ç†ç¼–ç å’Œç©ºæ ¼
    title = title.strip().encode('utf-8')
    summary = summary.strip().encode('utf-8')
    
    # è·å–å‘å¸ƒæ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
    pub_date = ''
    if hasattr(entry, 'published'):
        pub_date = entry.published
    elif hasattr(entry, 'updated'):
        pub_date = entry.updated
    
    pub_date = pub_date.strip().encode('utf-8')
    
    # åˆ›å»ºç»Ÿä¸€çš„å“ˆå¸Œå­—ç¬¦ä¸²
    raw_text = title + b'|||' + summary + b'|||' + pub_date
    return hashlib.sha256(raw_text).hexdigest()

def signal_handler(signum, frame):
    """æ”¹è¿›çš„ä¿¡å·å¤„ç†"""
    global SHOULD_EXIT
    logger.warning(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
    SHOULD_EXIT = True

def get_entry_timestamp(entry):
    dt = datetime.now(pytz.UTC)
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        dt = datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        dt = datetime(*entry.pubDate_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        dt = datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc)
    return dt

@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=5, max=30),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0
        paragraphs = text.split('\n\n')
        
        for para in paragraphs:
            para_length = len(para)
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2
            
        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))
            
        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview
                # ç§»é™¤ timeout å‚æ•°
            )
            
    except BadRequest as e:
        logger.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
        # å¯ä»¥å°è¯•ä¸ä½¿ç”¨ Markdown é‡æ–°å‘é€
        try:
            for chunk in text_chunks:
                await bot.send_message(
                    chat_id=chat_id,
                    text=chunk,
                    disable_web_page_preview=disable_web_page_preview
                )
        except Exception as fallback_error:
            logger.error(f"çº¯æ–‡æœ¬å›é€€å‘é€ä¹Ÿå¤±è´¥: {fallback_error}")
    except Exception as e:
        logger.error(f"æ¶ˆæ¯å‘é€æœªçŸ¥é”™è¯¯: {e}")
        raise

@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def fetch_feed(session, feed_url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    parsed = urlparse(feed_url)
    is_rsshub = parsed.netloc == "rsshub.app"
    if is_rsshub:
        try_domains = BACKUP_DOMAINS + ["rsshub.app"]
        canonical_url = feed_url.replace(parsed.netloc, "rsshub.app")
    else:
        try_domains = [parsed.netloc]
        canonical_url = feed_url
    for domain in try_domains:
        modified_url = feed_url.replace(parsed.netloc, domain)
        try:
            async with semaphore:
                async with session.get(modified_url, headers=headers, timeout=30) as response:
                    if response.status in (503, 403, 404, 429):
                        continue
                    response.raise_for_status()
                    return parse(await response.read()), canonical_url
        except aiohttp.ClientResponseError as e:
            if e.status in (503, 403, 404, 429):
                continue
        except Exception as e:
        #    logger.error(f"è¯·æ±‚å¤±è´¥: {modified_url}, é”™è¯¯: {e}")
            continue
   # logger.error(f"æ‰€æœ‰åŸŸåå°è¯•å¤±è´¥: {feed_url}")
    return None, canonical_url

async def translate_with_credentials(secret_id, secret_key, text):
    loop = asyncio.get_running_loop()
    text_bytes = text.encode('utf-8')
    if len(text_bytes) > 2000:
        safe_bytes = text_bytes[:2000]
        while safe_bytes[-1] & 0xC0 == 0x80:
            safe_bytes = safe_bytes[:-1]
        text = safe_bytes.decode('utf-8', errors='ignore')
     #   logger.warning(f"æ–‡æœ¬æˆªæ–­è‡³ {len(text)} å­—ç¬¦ ({len(safe_bytes)} å­—èŠ‚)")
    try:
        return await loop.run_in_executor(
            None, 
            lambda: _sync_translate(secret_id, secret_key, text)
        )
    except Exception as e:
    #    logger.error(f"ç¿»è¯‘æ‰§è¡Œå¤±è´¥: {type(e).__name__} - {str(e)}")
        raise

def is_need_translate(text):
    try:
        lang = detect(text)
        # åªå¯¹è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ç­‰éä¸­æ–‡åšç¿»è¯‘
        return lang not in ("zh-cn", "zh-tw", "zh", "yue")
    except LangDetectException:
        return False
    
def is_mostly_symbols(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸»è¦ç”±ç¬¦å·ã€æ•°å­—ç»„æˆ"""
    if not text:
        return True
    
    # è®¡ç®—å­—æ¯æ¯”ä¾‹
    alpha_count = sum(1 for char in text if char.isalpha())
    total_chars = len(text)
    
    # å¦‚æœå­—æ¯æ¯”ä¾‹ä½äº30%ï¼Œè®¤ä¸ºæ˜¯ç¬¦å·/æ•°å­—æ–‡æœ¬
    return alpha_count / total_chars < 0.3 if total_chars > 0 else True

def _sync_translate(secret_id, secret_key, text):
    try:
        cred = credential.Credential(secret_id, secret_key)
        clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
        client = tmt_client.TmtClient(cred, TENCENT_REGION, clientProfile)
        req = models.TextTranslateRequest()
        req.SourceText = remove_html_tags(text)
        req.Source = "auto"
        req.Target = "zh"
        req.ProjectId = 0
        return client.TextTranslate(req).TargetText
    except TencentCloudSDKException as e:
        error_details = {
            "code": getattr(e, "code", ""),
            "message": getattr(e, "message", str(e)),
            "request_id": getattr(e, "request_id", ""),
            "region": TENCENT_REGION
        }
    #    logger.error(f"è…¾è®¯äº‘APIé”™è¯¯è¯¦æƒ…: {error_details}")
        raise
    except Exception as e:
      #  logger.error(f"ç¿»è¯‘è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        raise

async def should_send_entry(entry, processor):
    filter_config = processor.get("filter", {})
    
    # å¦‚æœæ²¡æœ‰å¯ç”¨è¿‡æ»¤ï¼Œç›´æ¥è¿”å› True
    if not filter_config.get("enable", False):
        return True
        
    title = getattr(entry, "title", "") or ""      # è·å–æ ‡é¢˜
    link = getattr(entry, "link", "") or ""        # è·å–é“¾æ¥
    summary = getattr(entry, "summary", "") or ""  # è·å–æ‘˜è¦
    
    # è·å–è¿‡æ»¤èŒƒå›´é…ç½®ï¼Œé»˜è®¤ä¸º "title"
    scope = filter_config.get("scope", "title")
    keywords = [kw.lower() for kw in filter_config.get("keywords", [])]
    mode = filter_config.get("mode", "allow")
    
    # æ ¹æ®èŒƒå›´é…ç½®æ„å»ºè¿‡æ»¤å†…å®¹
    content_parts = []
    
    if scope == "title":
        content_parts = [title]
    elif scope == "link":
        content_parts = [link]
    elif scope == "both":
        content_parts = [title, link]
    elif scope == "all":
        content_parts = [title, link, summary]
    elif scope == "title_summary":
        content_parts = [title, summary]
    elif scope == "link_summary":
        content_parts = [link, summary]
    else:  # é»˜è®¤åªè¿‡æ»¤æ ‡é¢˜
        content_parts = [title]
    
    # åˆå¹¶å†…å®¹å¹¶è¿›è¡Œè¿‡æ»¤æ£€æŸ¥
    content = " ".join(content_parts).lower()
    has_keyword = any(keyword in content for keyword in keywords)
    
    # è®°å½•è¿‡æ»¤è¯¦æƒ…ï¼ˆè°ƒè¯•ç”¨ï¼‰
    logger.debug(f"[å…³é”®è¯è¿‡æ»¤] èŒƒå›´: {scope} | æ ‡é¢˜: {title[:50]} | é“¾æ¥: {link[:50]} | å…³é”®è¯: {keywords} | æ¨¡å¼: {mode} | å‘½ä¸­: {has_keyword}")
    
    # æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦å‘é€
    if not keywords:  # å¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œæ ¹æ®æ¨¡å¼å†³å®š
        return mode != "allow"
    elif mode == "allow":
        return has_keyword
    elif mode == "block":
        return not has_keyword
    else:
        return True
    
@retry(
    stop=stop_after_attempt(2),
    wait=wait_exponential(multiplier=1, min=2, max=10),
)
async def auto_translate_text(text):
    cleaned_text = remove_html_tags(text).strip()
    
    # å¦‚æœæ–‡æœ¬è¿‡çŸ­æˆ–ä¸»è¦æ˜¯ç¬¦å·/æ•°å­—ï¼Œç›´æ¥è¿”å›åŸæ–‡
    if len(cleaned_text) <= 3 or is_mostly_symbols(cleaned_text):
      #  logger.debug(f"è·³è¿‡ç¿»è¯‘ - æ–‡æœ¬è¿‡çŸ­æˆ–ä¸»è¦ä¸ºç¬¦å·: {cleaned_text}")
        return escape(cleaned_text)
    
    try:
        # é¦–å…ˆå°è¯•ä¸»å¯†é’¥
        try:
            return await translate_with_credentials(
                TENCENTCLOUD_SECRET_ID, 
                TENCENTCLOUD_SECRET_KEY,
                cleaned_text
            )
        except TencentCloudSDKException as e:
            if getattr(e, "code", "") == "FailedOperation.LanguageRecognitionErr":
             #   logger.warning(f"è…¾è®¯äº‘è¯­è¨€è¯†åˆ«å¤±è´¥ï¼Œè¿”å›åŸæ–‡: {cleaned_text[:100]}")
                return escape(cleaned_text)
            else:
          #      logger.error(f"ä¸»å¯†é’¥ç¿»è¯‘å¤±è´¥: [Code: {e.code}] {e.message}")
                raise
                
    except Exception as first_error:
        # åªæœ‰åœ¨éè¯­è¨€è¯†åˆ«é”™è¯¯çš„æƒ…å†µä¸‹æ‰å°è¯•å¤‡ç”¨å¯†é’¥
        if TENCENT_SECRET_ID and TENCENT_SECRET_KEY:
        #    logger.warning("ä¸»ç¿»è¯‘å¯†é’¥å¤±è´¥ï¼ˆéè¯­è¨€è¯†åˆ«é”™è¯¯ï¼‰ï¼Œå°è¯•å¤‡ç”¨å¯†é’¥...")
            try:
                return await translate_with_credentials(
                    TENCENT_SECRET_ID,
                    TENCENT_SECRET_KEY,
                    cleaned_text
                )
            except TencentCloudSDKException as e:
                if getattr(e, "code", "") == "FailedOperation.LanguageRecognitionErr":
                 #   logger.warning(f"å¤‡ç”¨å¯†é’¥è¯­è¨€è¯†åˆ«å¤±è´¥ï¼Œè¿”å›åŸæ–‡: {cleaned_text[:100]}")
                    return escape(cleaned_text)
                else:
                #    logger.error(f"å¤‡ç”¨å¯†é’¥ç¿»è¯‘å¤±è´¥: [Code: {e.code}] {e.message}")
                    raise
            except Exception as e:
         #       logger.error(f"å¤‡ç”¨å¯†é’¥ç¿»è¯‘æœªçŸ¥é”™è¯¯: {type(e).__name__} - {str(e)}")
                raise
        else:
      #      logger.error("ä¸»ç¿»è¯‘å¯†é’¥å¤±è´¥ï¼Œä¸”æœªé…ç½®å¤‡ç”¨å¯†é’¥")
            return escape(cleaned_text)

async def generate_group_message(feed_data, entries, processor):
    try:
        source_name = feed_data.feed.get('title', "æœªçŸ¥æ¥æº")
        safe_source = escape(source_name)
        header = ""
        if "header_template" in processor:
            header = processor["header_template"].format(source=safe_source) + "\n"
        
        messages = []
        
        # âœ… æ£€æŸ¥æ¨¡æ¿æ˜¯å¦éœ€è¦æ‘˜è¦
        template_needs_summary = "{summary}" in processor["template"]
        
        # âœ… ç°åœ¨å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨ template_needs_summary
        logger.debug(f"[æ‘˜è¦å¤„ç†] ç»„: {source_name}, éœ€è¦æ‘˜è¦: {template_needs_summary}")
        
        for entry in entries:
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            if processor.get("translate", False):
                translated_subject = await auto_translate_text(raw_subject)
            else:
                translated_subject = raw_subject
            safe_subject = escape(translated_subject)
            raw_url = entry.link
            safe_url = escape(raw_url)
            
            # âœ… åªåœ¨éœ€è¦æ—¶å¤„ç†æ‘˜è¦
            format_kwargs = {
                "subject": safe_subject,
                "source": safe_source,
                "url": safe_url
            }
            
            if template_needs_summary:
                raw_summary = getattr(entry, "summary", "") or ""
                cleaned_summary = remove_html_tags(raw_summary)
                safe_summary = escape(cleaned_summary)
                format_kwargs["summary"] = safe_summary
            
            message = processor["template"].format(**format_kwargs)
            messages.append(message)
        
        full_message = await _format_batch_message(header, messages, processor)
        return full_message
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¶ˆæ¯å¤±è´¥: {str(e)}")
        return ""

async def _format_batch_message(header, messages, processor):
    """æ”¹è¿›çš„æ‰¹é‡æ¶ˆæ¯æ ¼å¼åŒ–ï¼Œç¡®ä¿Markdownæ ¼å¼å®Œæ•´"""
    MAX_MESSAGE_LENGTH = 4096
    
    if not messages:
        return ""
    
    # å°è¯•æ„å»ºå®Œæ•´æ¶ˆæ¯
    full_content = header + "\n\n".join(messages)
    if processor.get("show_count", False):
        full_content += f"\n\nâœ… æ–°å¢ {len(messages)} æ¡å†…å®¹"
    
    # å¦‚æœæ¶ˆæ¯é•¿åº¦åœ¨é™åˆ¶å†…ï¼Œç›´æ¥è¿”å›
    if len(full_content) <= MAX_MESSAGE_LENGTH:
        return full_content
    
    # æ¶ˆæ¯è¿‡é•¿ï¼Œéœ€è¦åˆ†æ®µ
    segments = []
    current_segment = header
    current_length = len(header)
    
    for i, message in enumerate(messages):
        # æ–°æ®µçš„ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸åŠ åˆ†éš”ç¬¦ï¼Œåç»­æ¶ˆæ¯åŠ åˆ†éš”ç¬¦
        if current_segment == header:
            message_with_separator = message
        else:
            message_with_separator = "\n\n" + message
        
        # æ£€æŸ¥æ·»åŠ è¿™æ¡æ¶ˆæ¯æ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶ï¼ˆé¢„ç•™100å­—ç¬¦ç»™è®¡æ•°ä¿¡æ¯ï¼‰
        if current_length + len(message_with_separator) > MAX_MESSAGE_LENGTH - 300:
            # å®Œæˆå½“å‰æ®µ
            if processor.get("show_count", False) and current_segment != header:
                segment_msg_count = current_segment.count("\n\n") + 1
                current_segment += f"\n\nâœ… æœ¬æ®µåŒ…å« {segment_msg_count} æ¡å†…å®¹"
            segments.append(current_segment)
            
            # å¼€å§‹æ–°æ®µï¼Œé‡æ–°æ·»åŠ header
            current_segment = header
            current_length = len(header)
            message_with_separator = message  # æ–°æ®µçš„ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸åŠ åˆ†éš”ç¬¦
        
        current_segment += message_with_separator
        current_length += len(message_with_separator)
    
    # æ·»åŠ æœ€åä¸€æ®µ
    if current_segment.strip() and current_segment != header:
        if processor.get("show_count", False):
            segment_msg_count = current_segment.count("\n\n") + 1
            current_segment += f"\n\nâœ… æœ¬æ®µåŒ…å« {segment_msg_count} æ¡å†…å®¹"
        segments.append(current_segment)
    
    return segments

async def send_batch_messages(bot, chat_id, message_content, disable_web_page_preview=False):
    """å‘é€æ‰¹é‡æ¶ˆæ¯ï¼Œå¤„ç†åˆ†æ®µ"""
    if isinstance(message_content, list):  # åˆ†æ®µæ¶ˆæ¯
        for i, segment in enumerate(message_content):
            if segment.strip():  # ç¡®ä¿æ®µä¸ä¸ºç©º
                try:
                    await send_single_message(
                        bot, chat_id, segment, 
                        disable_web_page_preview=disable_web_page_preview
                    )
                    if i < len(message_content) - 1:  # ä¸æ˜¯æœ€åä¸€æ¡
                        await asyncio.sleep(1)  # é¿å…å‘é€è¿‡å¿«
                except Exception as e:
                    logger.error(f"å‘é€åˆ†æ®µæ¶ˆæ¯å¤±è´¥: {e}")
    else:  # å•æ¡æ¶ˆæ¯
        await send_single_message(
            bot, chat_id, message_content,
            disable_web_page_preview=disable_web_page_preview
        )

# ä¿®æ”¹æ‰¹é‡å‘é€å‡½æ•°ä¸­çš„è°ƒç”¨
async def process_batch_send(group, db: RSSDatabase):
    group_key = group["group_key"]
    bot_token = group["bot_token"]
    processor = group["processor"]
    batch_interval = group.get("batch_send_interval")
    
    if not batch_interval:
        return
        
    now = datetime.now(pytz.utc).timestamp()
    last_batch_sent = await db.get_last_batch_sent_time(group_key)
    if now - last_batch_sent < batch_interval:
        return
        
    pending = await db.get_pending_messages(group_key)
    if not pending:
        await db.save_last_batch_sent_time(group_key, now)
        return

    # æŒ‰ feed_url åˆ†ç»„æ¶ˆæ¯
    feed_url_to_msgs = defaultdict(list)
    for row in pending:
        feed_url_to_msgs[row["feed_url"]].append(row)

    bot = Bot(token=bot_token)
    sent_entry_ids = []
    
    for feed_url, msgs in feed_url_to_msgs.items():
        feed_title = (msgs[0].get("feed_title") or group.get("name") or feed_url)
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„feedå’Œentryå¯¹è±¡
        class DummyFeed:
            feed = {'title': feed_title}
            
        class Entry:
            def __init__(self, row):
                self.title = row["translated_title"] or row["title"]
                self.link = row["link"]
                self.summary = row.get("summary", "") or ""  # âœ… æ–°å¢æ‘˜è¦æ”¯æŒ
        entries = [Entry(row) for row in msgs]
        
        try:
            # ç”Ÿæˆæ¶ˆæ¯å†…å®¹
            feed_message = await generate_group_message(
                DummyFeed, entries, {**processor, "translate": False}
            )
            
            if feed_message:
                # å‘é€æ¶ˆæ¯ï¼ˆæ”¯æŒåˆ†æ®µï¼‰
                await send_batch_messages(
                    bot,
                    TELEGRAM_CHAT_ID[0],
                    feed_message,
                    disable_web_page_preview=not processor.get("preview", True)
                )
                # è®°å½•å·²å‘é€çš„æ¶ˆæ¯ID
                sent_entry_ids.extend([row["entry_id"] for row in msgs])
                
        except Exception as e:
            logger.error(f"æ‰¹é‡æ¨é€å¤±è´¥[{group_key}-{feed_url}]: {e}")
    
    # æ ‡è®°å·²å‘é€çš„æ¶ˆæ¯
    if sent_entry_ids:
        await db.mark_pending_as_sent(group_key, sent_entry_ids)
    
    await db.save_last_batch_sent_time(group_key, now)

# ========== ç»„é‡‡é›†ï¼ˆé‡‡é›†ä½†å¯é€‰æ‹©æ˜¯å¦ç«‹å³æ¨é€ï¼‰ ==========
async def process_group(session, group_config, global_status, db: RSSDatabase):
    """åœ¨ç»„å¤„ç†ä¸­æ·»åŠ é€€å‡ºæ£€æŸ¥"""
    global SHOULD_EXIT
    
    if SHOULD_EXIT:
        logger.info("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œåœæ­¢å¤„ç†ç»„ä»»åŠ¡")
        return
        
    group_name = group_config["name"]
    group_key = group_config["group_key"]
    processor = group_config["processor"]
    bot_token = group_config["bot_token"]
    batch_send_interval = group_config.get("batch_send_interval", None)
    
    try:
        last_run = await db.load_last_run_time(group_key)
        now = datetime.now(pytz.utc).timestamp()
        if (now - last_run) < group_config["interval"]:
            return
            
        bot = Bot(token=bot_token)
        for index, feed_url in enumerate(group_config["urls"]):
            try:
                if index > 0:
                    await asyncio.sleep(1)
                    
                feed_data, canonical_url = await fetch_feed(session, feed_url)
                if not feed_data or not feed_data.entries:
                    continue
                    
                processed_ids = global_status.get(canonical_url, set())
                new_entries = []
                seen_in_batch = set()
                new_hashes_in_batch = set()  # å½“å‰æ‰¹æ¬¡çš„å†…å®¹å“ˆå¸Œå»é‡

                for entry in feed_data.entries:
                    entry_id = get_entry_identifier(entry)
                    content_hash = get_entry_content_hash(entry)
                    
                    # ç»Ÿä¸€ä½¿ç”¨å†…å®¹å“ˆå¸Œå»é‡ï¼ˆä¸»è¦ä¿®å¤ï¼‰
                    if await db.has_content_hash(group_key, content_hash):
                        continue
                        
                    if entry_id in processed_ids or entry_id in seen_in_batch:
                        continue
                        
                    # åœ¨å½“å‰æ‰¹æ¬¡ä¸­ä¹Ÿç”¨å†…å®¹å“ˆå¸Œå»é‡
                    if content_hash in new_hashes_in_batch:
                        continue  
                        
                    # âœ… è¿‡æ»¤æ£€æŸ¥
                    if not await should_send_entry(entry, processor):
                        continue  # è·³è¿‡ä¸ç¬¦åˆè¿‡æ»¤æ¡ä»¶çš„æ¡ç›®

                    seen_in_batch.add(entry_id)
                    new_hashes_in_batch.add(content_hash)
                    new_entries.append((entry, content_hash, entry_id))
                    
                if new_entries:
                    if batch_send_interval:
                        # æ‰¹é‡å‘é€æ¨¡å¼ï¼šå­˜å…¥å¾…å‘é€é˜Ÿåˆ—
                        for entry, content_hash, entry_id in new_entries:
                            raw_subject = remove_html_tags(getattr(entry, "title", "") or "")
                            if processor["translate"] and is_need_translate(raw_subject):
                                translated_subject = await auto_translate_text(raw_subject)
                            else:
                                translated_subject = raw_subject
                                
                            await db.add_pending_message(
                                group_key, 
                                canonical_url, 
                                entry_id, 
                                content_hash,
                                getattr(entry, "title", ""), 
                                translated_subject, 
                                getattr(entry, "link", ""), 
                                getattr(entry, "summary", ""),
                                get_entry_timestamp(entry).timestamp() if get_entry_timestamp(entry) else time.time(),
                                feed_data.feed.get('title', "") 
                            )
                            await db.save_status(group_key, canonical_url, entry_id, content_hash, time.time())
                            processed_ids.add(entry_id)
                            
                        global_status[canonical_url] = processed_ids
                    else:
                        # ç«‹å³å‘é€æ¨¡å¼
                        feed_message = await generate_group_message(feed_data, [e for e,_,_ in new_entries], processor)
                        if feed_message:
                            try:
                                await send_single_message(
                                    bot,
                                    TELEGRAM_CHAT_ID[0],
                                    feed_message,
                                    disable_web_page_preview=not processor.get("preview", True)
                                )
                                for entry, content_hash, entry_id in new_entries:
                                    await db.save_status(group_key, canonical_url, entry_id, content_hash, time.time())
                                    processed_ids.add(entry_id)
                                global_status[canonical_url] = processed_ids
                            except Exception as send_error:
                                logger.error(f"âŒ å‘é€æ¶ˆæ¯å¤±è´¥ [{feed_url}]: {send_error}")
                                raise
                                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ [{feed_url}]: {e}")
                
        await db.save_last_run_time(group_key, now)
        
    except Exception as e:
        logger.critical(f"â€¼ï¸ å¤„ç†ç»„å¤±è´¥ [{group_key}]: {e}")

async def main():
    logger.info("ğŸš€ RSS Bot å¼€å§‹æ‰§è¡Œ")
    
    # æ·»åŠ ç¯å¢ƒå˜é‡éªŒè¯
    print("ğŸ” ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    print(f"  CONFIG_URL: {CONFIG_URL}")
    print(f"  TELEGRAM_CHAT_ID: {TELEGRAM_CHAT_ID}")
    print(f"  TENCENTCLOUD_SECRET_ID: {'å·²è®¾ç½®' if TENCENTCLOUD_SECRET_ID else 'æœªè®¾ç½®'}")
    print(f"  TENCENTCLOUD_SECRET_KEY: {'å·²è®¾ç½®' if TENCENTCLOUD_SECRET_KEY else 'æœªè®¾ç½®'}")
    
    # éªŒè¯å¿…è¦çš„ç¯å¢ƒå˜é‡
    if not CONFIG_URL:
        print("âŒ é”™è¯¯: CONFIG_URL ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        logger.error("CONFIG_URL ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return
    
    if not TELEGRAM_CHAT_ID or not TELEGRAM_CHAT_ID[0]:
        print("âŒ é”™è¯¯: TELEGRAM_CHAT_ID ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        logger.error("TELEGRAM_CHAT_ID ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return
    
    print(f"ğŸ” å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"ğŸ” é…ç½®æ–‡ä»¶è·¯å¾„: {LOCAL_CONFIG_FILE}")
    print(f"ğŸ” é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {LOCAL_CONFIG_FILE.exists()}")
    
    # å…ˆåˆå§‹åŒ–æ•°æ®åº“
    db = RSSDatabase()
    try:
        await db.open()
        await db.ensure_initialized()
        logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # 1. åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨å¹¶ä¼ å…¥æ•°æ®åº“å®ä¾‹
        global config_manager
        config_manager = ConfigManager(db)
        
        # 2. ç¡®ä¿é…ç½®æ˜¯æœ€æ–°çš„ - æ·»åŠ è¯¦ç»†æ—¥å¿—
        print("ğŸ”„ å¼€å§‹æ£€æŸ¥é…ç½®æ›´æ–°...")
        await config_manager.ensure_fresh_config()
        print("âœ… é…ç½®æ£€æŸ¥å®Œæˆ")
        
        # 3. å†æ¬¡æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        print(f"ğŸ” ä¸‹è½½åé…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {LOCAL_CONFIG_FILE.exists()}")
        
        if not LOCAL_CONFIG_FILE.exists():
            logger.error("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç¨‹åºé€€å‡º")
            print("âŒ é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ CONFIG_URL è®¾ç½®")
            return
        
        # 4. åŠ è½½é…ç½®
        global RSS_GROUPS
        RSS_GROUPS = config_manager.load_local_config()
        
        if not RSS_GROUPS:
            logger.error("âŒ æ— æ³•åŠ è½½RSSé…ç½®ï¼Œç¨‹åºé€€å‡º")
            print("âŒ é”™è¯¯ï¼šæ— æ³•åŠ è½½RSSé…ç½®")
            return
        
        logger.info(f"ğŸ“‹ åŠ è½½äº† {len(RSS_GROUPS)} ä¸ªRSSç»„")
        print(f"âœ… æˆåŠŸåŠ è½½ {len(RSS_GROUPS)} ä¸ªRSSç»„é…ç½®")
        
        # 5. åŸæœ‰çš„ä¸»é€»è¾‘
        start_time = time.time()
        max_retries = 3
        retry_delay = 60
        
        for attempt in range(max_retries):
            try:
                await run_main_logic(db)
                logger.info(f"âœ… RSS Bot æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
                break
            except Exception as e:
                logger.error(f"ä¸»ç¨‹åºè¿è¡Œå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    logger.info(f"{retry_delay}ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)
                else:
                    logger.critical("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç¨‹åºé€€å‡º")
                    return
                    
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
    finally:
        if db:
            await db.close()

async def run_main_logic(db):  # ä¿®æ”¹ï¼šæ¥æ”¶dbå‚æ•°
    lock_file = None
    # æ³¨æ„ï¼šè¿™é‡Œä¸å†æ–°å»ºdbå®ä¾‹ï¼Œè€Œæ˜¯ä½¿ç”¨ä¼ å…¥çš„db
    
    try:
        # è·å–æ–‡ä»¶é”
        lock_file = open(LOCK_FILE, "w")
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
        logger.info("ğŸ”’ æˆåŠŸè·å–æ–‡ä»¶é”")
        
        # æ•°æ®åº“è¿æ¥å·²ç»åœ¨mainä¸­å»ºç«‹ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
        
        # æ¸…ç†å†å²è®°å½•
        logger.info("ğŸ§¹ æ­£åœ¨æ¸…ç†å†å²è®°å½•...")
        for group in RSS_GROUPS:
            days = group.get("history_days", 30)
            try:
                await db.cleanup_history(days, group["group_key"])
            except Exception as e:
                logger.error(f"æ¸…ç†å†å²è®°å½•å¼‚å¸¸: ç»„={group['group_key']}, é”™è¯¯={e}")
                
        # ä¸»å¤„ç†é€»è¾‘
        logger.info("ğŸš€ å¼€å§‹å¤„ç† RSS è®¢é˜…...")
        async with aiohttp.ClientSession() as session:
            status = await db.load_status()
            tasks = []
            
            for group in RSS_GROUPS:
                try:
                    task = asyncio.create_task(
                        process_group(session, group, status, db)
                    )
                    tasks.append(task)
                except Exception as e:
                    logger.error(f"âš ï¸ åˆ›å»ºä»»åŠ¡å¤±è´¥ [{group['name']}]: {str(e)}")
            
            if tasks:
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ‰¹é‡å‘é€ä»»åŠ¡
            batch_tasks = [
                process_batch_send(group, db) 
                for group in RSS_GROUPS 
                if group.get("batch_send_interval")
            ]
            if batch_tasks:
                await asyncio.gather(*batch_tasks, return_exceptions=True)
                
    except asyncio.CancelledError:
        logger.warning("â¹ï¸ ä»»åŠ¡è¢«å–æ¶ˆ")
    except Exception as e:
        logger.error(f"ä¸»é€»è¾‘æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        raise
    finally:
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†å…³é—­dbï¼Œå› ä¸ºdbåœ¨mainå‡½æ•°ä¸­ç»Ÿä¸€ç®¡ç†
        await cleanup_resources(None, lock_file)  # ä¿®æ”¹ï¼šä¼ å…¥Noneè€Œä¸æ˜¯db

async def cleanup_resources(db, lock_file):
    """æ¸…ç†èµ„æº"""
    # æ³¨æ„ï¼šdbç°åœ¨åœ¨mainå‡½æ•°ä¸­ç»Ÿä¸€ç®¡ç†ï¼Œè¿™é‡Œåªå¤„ç†é”æ–‡ä»¶
    try:
        if lock_file:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
            lock_file.close()
            if LOCK_FILE.exists():
                LOCK_FILE.unlink()
    except Exception as e:
        logger.error(f"é‡Šæ”¾æ–‡ä»¶é”å¤±è´¥: {e}")

if __name__ == "__main__":
    for s in (signal.SIGINT, signal.SIGTERM):
        signal.signal(s, signal_handler)
    try:
        asyncio.run(main())
    except Exception as e:
        logger.critical(f"â€¼ï¸ ä¸»è¿›ç¨‹æœªæ•è·å¼‚å¸¸: {str(e)}", exc_info=True)
        sys.exit(1)