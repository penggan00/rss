import asyncio
import aiohttp
import logging
import re
import os
import hashlib
import pytz
import fcntl
import sqlite3
import time
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
#from md2tgmd import escape
from cron import RSS_GROUPS

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# é…ç½®ç»å¯¹è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent
# åˆ›å»ºé”æ–‡ä»¶
LOCK_FILE = BASE_DIR / "rss.lock"
# SQLite æ•°æ®åº“åˆå§‹åŒ–
DATABASE_FILE = BASE_DIR / "rss_status.db"

# å¢å¼ºæ—¥å¿—é…ç½®
logging.basicConfig(
    filename=BASE_DIR / "rss.log",
    level=logging.WARNING,  # åªè®°å½• WARNING/ERROR/CRITICAL
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

logger = logging.getLogger(__name__)

TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")
TENCENT_REGION = os.getenv("TENCENT_REGION", "na-siliconvalley")
# åœ¨ç¯å¢ƒå˜é‡åŠ è½½åæ·»åŠ å¤‡ç”¨å¯†é’¥é…ç½®
TENCENT_SECRET_ID = os.getenv("TENCENT_SECRET_ID")
TENCENT_SECRET_KEY = os.getenv("TENCENT_SECRET_KEY")
semaphore = asyncio.Semaphore(2)  # å¹¶å‘æ§åˆ¶ï¼Œé™åˆ¶åŒæ—¶æœ€å¤š2ä¸ªè¯·æ±‚


# æ–°å¢é€šç”¨å¤„ç†å‡½æ•°
async def process_group(session, group_config, global_status):
    """ç»Ÿä¸€å¤„ç†RSSç»„ï¼ˆä¼˜åŒ–ç‰ˆï¼šç¡®ä¿å‘é€æˆåŠŸåæ‰ä¿å­˜çŠ¶æ€ï¼‰"""
    group_name = group_config["name"]
    group_key = group_config["group_key"]
    processor = group_config["processor"]
    bot_token = group_config["bot_token"]
    
    try:
        # ========== 0. åˆå§‹å»¶è¿Ÿ ==========
        await asyncio.sleep(1)  # ç»„é—´åˆå§‹å»¶è¿Ÿ1ç§’

        # ========== 1. æ£€æŸ¥æ—¶é—´é—´éš” ==========
        last_run = await load_last_run_time_from_db(group_key)
        now = datetime.now(pytz.utc).timestamp()
        if (now - last_run) < group_config["interval"]:
            return  # æœªåˆ°é—´éš”æ—¶é—´ï¼Œè·³è¿‡å¤„ç†

        bot = Bot(token=bot_token)

        # ========== 2. å¤„ç†æ¯ä¸ªURLæº ==========
        for index, feed_url in enumerate(group_config["urls"]):
            try:
                # ===== 2.0 æºé—´å»¶è¿Ÿ =====
                if index > 0:  # ç¬¬ä¸€ä¸ªæºä¸éœ€è¦å»¶è¿Ÿ
                    await asyncio.sleep(1)  # æºé—´å»¶è¿Ÿ1ç§’

                # ------ 2.1 è·å–Feedæ•°æ® ------
                feed_data = await fetch_feed(session, feed_url)
                if not feed_data or not feed_data.entries:
                    continue

                # ------ 2.2 åŠ è½½å¤„ç†çŠ¶æ€ & æ”¶é›†æ–°æ¡ç›® ------
                processed_ids = global_status.get(feed_url, set())
                new_entries = []
                pending_entry_ids = []  # å¾…ä¿å­˜çš„æ¡ç›®IDï¼ˆå‘é€æˆåŠŸåæ‰ä¿å­˜ï¼‰
                seen_in_batch = set()  # ä¸´æ—¶å­˜å‚¨å½“å‰æ‰¹æ¬¡çš„IDï¼Œé¿å…é‡å¤

                for entry in feed_data.entries:
                    entry_id = get_entry_identifier(entry)
                    if entry_id in processed_ids or entry_id in seen_in_batch:  # æ–°å¢æ‰¹æ¬¡å†…å»é‡
                        continue
                    seen_in_batch.add(entry_id)

                    # å…³é”®è¯è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    filter_config = processor.get("filter", {})
                    if filter_config.get("enable", False):
                        raw_title = remove_html_tags(entry.title or "")
                        keywords = filter_config.get("keywords", [])
                        match = any(kw.lower() in raw_title.lower() for kw in keywords)
                        # æ ¹æ®æ¨¡å¼åˆ¤æ–­æ˜¯å¦è·³è¿‡
                        if filter_config.get("mode", "allow") == "allow":
                            if not match:  # å…è®¸æ¨¡å¼ï¼šä¸åŒ…å«å…³é”®è¯åˆ™è·³è¿‡
                                continue
                        else:  # blockæ¨¡å¼
                            if match:     # åŒ…å«å…³é”®è¯åˆ™è·³è¿‡
                                continue

                    new_entries.append(entry)
                    pending_entry_ids.append(entry_id)  # æš‚å­˜ï¼Œä¸ç«‹å³ä¿å­˜

                # ===== 2.3 å‘é€æ¶ˆæ¯ï¼ˆæˆåŠŸåä¿å­˜çŠ¶æ€ï¼‰ =====
                if new_entries:
                    await asyncio.sleep(1)  # å‘é€å‰å»¶è¿Ÿ1ç§’
                    feed_message = await generate_group_message(feed_data, new_entries, processor)
                    if feed_message:
                        try:
                            # å°è¯•å‘é€æ¶ˆæ¯
                            await send_single_message(
                                bot,
                                TELEGRAM_CHAT_ID[0],
                                feed_message,
                                disable_web_page_preview=not processor.get("preview", True)
                            )

                            # å‘é€æˆåŠŸï¼Œä¿å­˜æ‰€æœ‰æ¡ç›®çŠ¶æ€
                            for entry_id in pending_entry_ids:
                                await save_single_status(group_key, feed_url, entry_id)
                                processed_ids.add(entry_id)

                            # æ›´æ–°å†…å­˜çŠ¶æ€
                            global_status[feed_url] = processed_ids

                        except Exception as send_error:
                            logger.error(f"âŒ å‘é€æ¶ˆæ¯å¤±è´¥ [{feed_url}]")
                            raise  # æŠ›å‡ºå¼‚å¸¸ï¼Œé˜»æ­¢åç»­ä¿å­˜æ“ä½œ

            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ [{feed_url}]")

        # ========== 3. ä¿å­˜æœ€åè¿è¡Œæ—¶é—´ ==========
        await save_last_run_time_to_db(group_key, now)

        # ========== 4. æœ€ç»ˆå»¶è¿Ÿ ==========
        await asyncio.sleep(1)
    except Exception as e:
        logger.critical(f"â€¼ï¸ å¤„ç†ç»„å¤±è´¥ [{group_key}]")

async def generate_group_message(feed_data, entries, processor):
    """ç”Ÿæˆæ ‡å‡†åŒ–æ¶ˆæ¯å†…å®¹"""
    try:
        # ===== 1. åŸºç¡€ä¿¡æ¯å¤„ç† =====
        source_name = feed_data.feed.get('title', "æœªçŸ¥æ¥æº")
        safe_source = escape_markdown_v2(source_name)
        
        # ===== æ–°å¢ï¼šæ ‡é¢˜å¤„ç† =====
        header = ""
        if "header_template" in processor:
            header = processor["header_template"].format(source=safe_source) + "\n"
        
        messages = []

        # ===== 2. å¤„ç†æ¯ä¸ªæ¡ç›® =====
        for entry in entries:
            # -- 2.1 æ ‡é¢˜å¤„ç† --
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            if processor["translate"]:
                translated_subject = await auto_translate_text(raw_subject)
            else:
                translated_subject = raw_subject
            safe_subject = escape_markdown_v2(translated_subject)

            # -- 2.2 é“¾æ¥å¤„ç† --
            raw_url = entry.link
            safe_url = escape_markdown_v2(raw_url)

            # -- 2.3 æ„å»ºæ¶ˆæ¯ --
            message = processor["template"].format(
                subject=safe_subject,
                source=safe_source,
                url=safe_url
            )
            messages.append(message)

        full_message = header + "\n\n".join(messages)
        
        if processor.get("show_count", True):
            full_message += f"\n\nâœ… æ–°å¢ {len(messages)} æ¡å†…å®¹"
            
        return full_message
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¶ˆæ¯å¤±è´¥: {str(e)}")
        return ""

def create_connection():
    """åˆ›å»º SQLite æ•°æ®åº“è¿æ¥"""
    conn = None
    try:
        conn = sqlite3.connect(DATABASE_FILE)
        return conn
    except sqlite3.Error as e:
        logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
    return conn

def create_table():
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            # ä»…ä¿ç•™SQLiteå»ºè¡¨è¯­å¥
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS rss_status (
                    feed_group TEXT,
                    feed_url TEXT,
                    entry_url TEXT,
                    entry_timestamp REAL,
                    PRIMARY KEY (feed_group, feed_url, entry_url)
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_run_time REAL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS cleanup_timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_cleanup_time REAL
                )
            """)
            conn.commit()
       #     logger.info("æˆåŠŸåˆ›å»º/è¿æ¥åˆ°æœ¬åœ° SQLite æ•°æ®åº“å’Œè¡¨")
        except sqlite3.Error as e:
            logger.error(f"åˆ›å»ºæœ¬åœ°è¡¨å¤±è´¥: {e}")
        finally:
            conn.close()

async def load_last_run_time_from_db(feed_group):
    """ä»…ä½¿ç”¨SQLiteåŠ è½½æ—¶é—´æˆ³"""
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT last_run_time FROM timestamps WHERE feed_group = ?", (feed_group,))
            result = cursor.fetchone()
            return result[0] if result else 0
        except sqlite3.Error as e:
            logger.error(f"ä»æœ¬åœ°æ•°æ®åº“åŠ è½½æ—¶é—´å¤±è´¥: {e}")
            return 0
        finally:
            conn.close()
    return 0

async def save_last_run_time_to_db(feed_group, last_run_time):
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("BEGIN TRANSACTION")
            cursor.execute("""
                INSERT OR REPLACE INTO timestamps (feed_group, last_run_time)
                VALUES (?, ?)
            """, (feed_group, last_run_time))
            conn.commit()
        except sqlite3.Error as e:
            logger.error(f"æ—¶é—´æˆ³ä¿å­˜å¤±è´¥: {e}")
            conn.rollback()
        finally:
            conn.close()

# å‡½æ•° (ä¿æŒä¸å˜ï¼Œé™¤éå¦æœ‰è¯´æ˜)
def remove_html_tags(text):
    text = re.sub(r'#\w+', '', text)    # ç§»é™¤ hashtags
    text = re.sub(r'@[^\s]+', '', text).strip()     # ç§»é™¤ @æåŠ
    text = re.sub(r'ã€\s*ã€‘', '', text)    # ç§»é™¤ ã€ã€‘ç¬¦å·ï¼ˆå«ä¸­é—´ç©ºæ ¼ï¼‰
    # æ–°å¢ï¼šå¦‚æœ # å‰åéƒ½æ˜¯ç©ºæ ¼ï¼ˆæˆ–ä¸å­˜åœ¨å­—ç¬¦ï¼‰ï¼Œå°±åˆ é™¤ #
    text = re.sub(r'(?<!\S)#(?!\S)', '', text)
    text = re.sub(r'(?<!\S):(?!\S)', '', text)
    # ä»…æ›¿æ¢ è‹±æ–‡å•è¯.è‹±æ–‡å•è¯ çš„æƒ…å†µï¼ˆå¦‚ example.com â†’ exampleï¼comï¼‰
 #   text = re.sub(
 #       r'\.([a-zA-Z])',  # åŒ¹é… `.` åæ¥ä¸€ä¸ªå­—æ¯ï¼ˆä¸å…³å¿ƒå‰é¢æ˜¯ä»€ä¹ˆï¼‰
  #      lambda m: f'ï¼{m.group(1)}',  # æ›¿æ¢ `.` ä¸º `ï¼`ï¼Œå¹¶ä¿ç•™åé¢çš„å­—æ¯
  #      text
  #  )
    return text

def escape_markdown_v2(text):
    """ç»Ÿä¸€ä½¿ç”¨æ­¤å‡½æ•°è¿›è¡ŒMarkdownV2è½¬ä¹‰"""
    chars_to_escape = r'_*[]()~`>#+-=|{}.!\\'
    return re.sub(r'([{}])'.format(re.escape(chars_to_escape)), r'\\\1', text)

@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=5, max=30),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0

        # æŒ‰æ®µè½åˆ†å‰²ä¿æŒç»“æ„
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para_length = len(para.encode('utf-8'))
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2

        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))

        # v20.x çš„æ­£ç¡®å‚æ•°
        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview,
                read_timeout=10,  # è¯»å–è¶…æ—¶
                write_timeout=10  # å†™å…¥è¶…æ—¶
            )
    except BadRequest as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
    except Exception as e:
     #   logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {e}")
        raise

@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def fetch_feed(session, feed_url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    try:
        async with semaphore:
            async with session.get(feed_url, headers=headers, timeout=30) as response:
                # ç»Ÿä¸€å¤„ç†ä¸´æ—¶æ€§é”™è¯¯ï¼ˆ503/403ï¼‰
                if response.status in (503, 403,404,429):
                #    logger.warning(f"RSSæºæš‚æ—¶ä¸å¯ç”¨ï¼ˆ{response.status}ï¼‰: {feed_url}")
                    return None  # è·³è¿‡å½“å‰æºï¼Œä¸‹æ¬¡è¿è¡Œä¼šé‡è¯•
                response.raise_for_status()
                return parse(await response.read())
    except aiohttp.ClientResponseError as e:
        if e.status in (503, 403,404,429):
         #   logger.warning(f"RSSæºæš‚æ—¶ä¸å¯ç”¨{feed_url}")
            return None
    #    logging.error(f"HTTP é”™è¯¯ {e.status} æŠ“å–å¤±è´¥ {feed_url}: {e}")
        raise
    except Exception as e:
     #   logging.error(f"æŠ“å–å¤±è´¥ {feed_url}: {e}")
        raise

# ä¿®æ”¹ auto_translate_text å‡½æ•°
@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=2, max=5),
)
async def auto_translate_text(text):
    """ç¿»è¯‘æ–‡æœ¬ï¼Œå¤±è´¥æ—¶è¿”å›æ¸…ç†åçš„åŸå§‹æ–‡æœ¬"""
    try:
        # æ–‡æœ¬é•¿åº¦å¤„ç†
        max_length = 2000
        if len(text) > max_length:
            logger.warning(f"âš ï¸ æ–‡æœ¬è¿‡é•¿({len(text)}å­—ç¬¦)ï¼Œæˆªæ–­å¤„ç†")
            text = text[:max_length]
        
        # ç¬¬ä¸€ç»„å¯†é’¥å°è¯•
        try:
            return await translate_with_credentials(
                TENCENTCLOUD_SECRET_ID, 
                TENCENTCLOUD_SECRET_KEY,
                text
            )
        except Exception as first_error:
            # ç¬¬ä¸€ç»„å¤±è´¥ä¸”å­˜åœ¨å¤‡ç”¨å¯†é’¥æ—¶å°è¯•ç¬¬äºŒç»„
            if TENCENT_SECRET_ID and TENCENT_SECRET_KEY:
            #    logger.warning("âš ï¸ ä¸»ç¿»è¯‘å¯†é’¥å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å¯†é’¥...")
                try:
                    return await translate_with_credentials(
                        TENCENT_SECRET_ID,
                        TENCENT_SECRET_KEY,
                        text
                    )
                except Exception as second_error:
                    logger.error(f"å¤‡ç”¨å¯†é’¥ç¿»è¯‘å¤±è´¥: {second_error}")
            
            # æ‰€æœ‰å°è¯•å¤±è´¥æ—¶è¿”å›æ¸…ç†åçš„åŸå§‹æ–‡æœ¬
            logger.error(f"æ‰€æœ‰ç¿»è¯‘å°è¯•å‡å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬")
            return remove_html_tags(text)
            
    except Exception as e:
        logging.error(f"ç¿»è¯‘è¿‡ç¨‹å¼‚å¸¸: {e}")
        return remove_html_tags(text)  # ç¡®ä¿è¿”å›å¯ç”¨çš„æ–‡æœ¬

# æ–°å¢è¾…åŠ©ç¿»è¯‘å‡½æ•°
async def translate_with_credentials(secret_id, secret_key, text):
    """ä½¿ç”¨æŒ‡å®šå‡­è¯è¿›è¡Œç¿»è¯‘"""
    cred = credential.Credential(secret_id, secret_key)
    clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
    client = tmt_client.TmtClient(cred, TENCENT_REGION, clientProfile)

    req = models.TextTranslateRequest()
    req.SourceText = remove_html_tags(text)  # ç¡®ä¿æ–‡æœ¬å·²æ¸…ç†
    req.Source = "auto"
    req.Target = "zh"
    req.ProjectId = 0

    return client.TextTranslate(req).TargetText

async def load_status():
    """ä»…ä»SQLiteåŠ è½½çŠ¶æ€"""
    status = {}
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT feed_url, entry_url FROM rss_status")
            for feed_url, entry_url in cursor.fetchall():
                if feed_url not in status:
                    status[feed_url] = set()
                status[feed_url].add(entry_url)
          #  logger.info("æœ¬åœ°çŠ¶æ€åŠ è½½æˆåŠŸ")
        except sqlite3.Error as e:
            logger.error(f"æœ¬åœ°çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
        finally:
            conn.close()
    return status

async def save_single_status(feed_group, feed_url, entry_url):
    """ä»…ä¿å­˜åˆ°SQLiteï¼Œä½¿ç”¨äº‹åŠ¡å’Œé‡è¯•"""
    timestamp = time.time()
    max_retries = 3
    for attempt in range(max_retries):
        conn = create_connection()
        if conn:
            try:
                cursor = conn.cursor()
                cursor.execute("BEGIN TRANSACTION")
                cursor.execute("""
                    INSERT OR IGNORE INTO rss_status 
                    VALUES (?, ?, ?, ?)
                """, (feed_group, feed_url, entry_url, timestamp))
                conn.commit()
                return
            except sqlite3.OperationalError as e:
                if "locked" in str(e) and attempt < max_retries - 1:
                    await asyncio.sleep(0.1 * (attempt + 1))
                    continue
                logger.error(f"SQLiteä¿å­˜å¤±è´¥ï¼ˆå°è¯•{attempt+1}æ¬¡ï¼‰: {e}")
            except sqlite3.Error as e:
                logger.error(f"SQLiteé”™è¯¯: {e}")
            finally:
                conn.close()

def get_entry_identifier(entry):
    # ä¼˜å…ˆä½¿ç”¨guid
    if hasattr(entry, 'guid') and entry.guid:
        return hashlib.sha256(entry.guid.encode()).hexdigest()
    
    # æ ‡å‡†åŒ–é“¾æ¥å¤„ç†
    link = getattr(entry, 'link', '')
    if link:
        try:
            parsed = urlparse(link)
            # ç§»é™¤æŸ¥è¯¢å‚æ•°ã€ç‰‡æ®µï¼Œå¹¶ç»Ÿä¸€ä¸ºå°å†™
            clean_link = parsed._replace(query=None, fragment=None).geturl().lower()
            return hashlib.sha256(clean_link.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"URLè§£æå¤±è´¥ {link}: {e}")
    
    # æœ€åä½¿ç”¨æ ‡é¢˜+å‘å¸ƒæ—¶é—´ç»„åˆ
    title = getattr(entry, 'title', '')
    pub_date = get_entry_timestamp(entry).isoformat() if get_entry_timestamp(entry) else ''
    return hashlib.sha256(f"{title}|||{pub_date}".encode()).hexdigest()

def get_entry_timestamp(entry):
    """è¿”å›UTCæ—¶é—´"""
    dt = datetime.now(pytz.UTC)  # é»˜è®¤å€¼
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        dt = datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        dt = datetime(*entry.pubDate_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        dt = datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc)
    return dt

async def process_feed_common(session, feed_group, feed_url, status):
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            return None

        processed_ids = status.get(feed_url, set())
        new_entries = []

        for entry in feed_data.entries:
            entry_id = get_entry_identifier(entry)
            if entry_id in processed_ids:
                continue

            new_entries.append(entry)
            # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
            await save_single_status(feed_group, feed_url, entry_id)
            # æ›´æ–°å†…å­˜ä¸­çš„çŠ¶æ€ï¼Œé˜²æ­¢åŒä¸€æ‰¹æ¬¡å†…é‡å¤
            processed_ids.add(entry_id)

        status[feed_url] = processed_ids  # æ›´æ–°å†…å­˜çŠ¶æ€
        return feed_data, new_entries

    except Exception as e:
      #  logger.error(f"å¤„ç†æºå¼‚å¸¸ {feed_url}")
        return None
    
def cleanup_history(days, feed_group):
    """ä»…åœ¨è¶…è¿‡24å°æ—¶æ—¶æ‰§è¡Œæ¸…ç†"""
    conn = create_connection()
    if conn:
        try:
            # æ£€æŸ¥ä¸Šæ¬¡æ¸…ç†æ—¶é—´
            cursor = conn.cursor()
            cursor.execute(
                "SELECT last_cleanup_time FROM cleanup_timestamps WHERE feed_group = ?", 
                (feed_group,)
            )
            result = cursor.fetchone()
            last_cleanup = result[0] if result else 0
            
            now = time.time()
            # 24å°æ—¶å†…ä¸æ¸…ç† (86400ç§’ = 24å°æ—¶)
            if now - last_cleanup < 86400:
                return
                
            # æ‰§è¡Œæ¸…ç†
            cutoff_ts = now - days * 86400
            cursor.execute(
                "DELETE FROM rss_status WHERE feed_group=? AND entry_timestamp < ?",
                (feed_group, cutoff_ts)
            )
            affected_rows = cursor.rowcount
            
            # æ›´æ–°æ¸…ç†æ—¶é—´
            cursor.execute("""
                INSERT OR REPLACE INTO cleanup_timestamps (feed_group, last_cleanup_time)
                VALUES (?, ?)
            """, (feed_group, now))
            
            conn.commit()
     #       logger.info(f"âœ… æ—¥å¿—æ¸…ç†: ç»„={feed_group}, ä¿ç•™å¤©æ•°={days}, åˆ é™¤æ¡æ•°={affected_rows}")
        except sqlite3.Error as e:
            logger.error(f"âŒ æ—¥å¿—æ¸…ç†å¤±è´¥: ç»„={feed_group}, é”™è¯¯={e}")
        finally:
            conn.close()

async def main():
    """ä¸»å¤„ç†å‡½æ•°"""
    # ================== 1. æ–‡ä»¶é”å¤„ç† ==================
    lock_file = None
    try:
        lock_file = open(LOCK_FILE, "w")
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
    #    logger.info("ğŸ”’ æˆåŠŸè·å–æ–‡ä»¶é”ï¼Œå¯åŠ¨å¤„ç†æµç¨‹")
    except OSError:
        logger.warning("â›” æ— æ³•è·å–æ–‡ä»¶é”ï¼Œå·²æœ‰å®ä¾‹åœ¨è¿è¡Œï¼Œç¨‹åºé€€å‡º")
        return
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ–‡ä»¶é”å¼‚å¸¸: {str(e)}")
        return

    # ================== 2. æ•°æ®åº“åˆå§‹åŒ– ==================
    try:
        create_table()
 #       logger.info("ğŸ’¾ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return
    # ================== 3. æ¸…ç†å†å²è®°å½• ==================
    for group in RSS_GROUPS:
        days = group.get("history_days", 30)  # é»˜è®¤30å¤©
        try:
            cleanup_history(days, group["group_key"])
        except Exception as e:
            logger.error(f"æ¸…ç†å†å²è®°å½•å¼‚å¸¸: ç»„={group['group_key']}, é”™è¯¯={e}")
    # ================== 4. ä¸»å¤„ç†æµç¨‹ ==================
    async with aiohttp.ClientSession() as session:
        try:
            # ===== 4.1 åŠ è½½å¤„ç†çŠ¶æ€ =====
            status = await load_status()
     #       logger.info("ğŸ“‚ åŠ è½½å†å²çŠ¶æ€å®Œæˆ")

            # ===== 4.3 åˆ›å»ºå¤„ç†ä»»åŠ¡ =====
            tasks = []
            for group in RSS_GROUPS:
                try:
                    tasks.append(process_group(session, group, status))
              #      logger.debug(f"ğŸ“¨ å·²åˆ›å»ºå¤„ç†ä»»åŠ¡ [{group['name']}]")
                except Exception as e:
                    logger.error(f"âš ï¸ åˆ›å»ºä»»åŠ¡å¤±è´¥ [{group['name']}]: {str(e)}")

            # ===== 4.4 å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ =====
            if tasks:
                await asyncio.gather(*tasks)
          #      logger.info("ğŸš© æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²å®Œæˆ")
            else:
                logger.warning("â›” æœªåˆ›å»ºä»»ä½•å¤„ç†ä»»åŠ¡")

        except asyncio.CancelledError:
            logger.warning("â¹ï¸ ä»»åŠ¡è¢«å–æ¶ˆ")
        except Exception as e:
            logger.critical(f"â€¼ï¸ ä¸»å¾ªç¯å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            # ===== 4.5 æœ€ç»ˆæ¸…ç† =====
            try:
                await session.close()
     #           logger.info("ğŸ”Œ å·²å…³é—­ç½‘ç»œä¼šè¯")
            except Exception as e:
                logger.error(f"âš ï¸ å…³é—­ä¼šè¯å¤±è´¥: {str(e)}")

    # ================== 5. é‡Šæ”¾æ–‡ä»¶é” ==================
    try:
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
        lock_file.close()
   #     logger.info("ğŸ”“ æ–‡ä»¶é”å·²é‡Šæ”¾")
    except Exception as e:
        logger.error(f"âš ï¸ é‡Šæ”¾æ–‡ä»¶é”å¤±è´¥: {str(e)}")

    # ================== 6. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š ==================
 #   logger.info("ğŸ ç¨‹åºè¿è¡Œç»“æŸ\n" + "="*50 + "\n")

if __name__ == "__main__":
    # ç¡®ä¿å…ˆåˆ›å»ºæ–°è¡¨ç»“æ„
    create_table()
    asyncio.run(main())